
# 词嵌入：让计算机理解语言的神奇"翻译器"


`#2026/01/01` `#ai` 


## 目录
<!-- toc -->
 ## 1. 词嵌入是什么？ 

想象你有一本书，但计算机只能看懂数字。`词嵌入`就是把单词"翻译"成计算机能理解的数学语言。

### 关键特点：

- 将`文字`转换为数字向量
- 每个单词都对应一个独特的`数字"坐标"`
- 相似意思的词在"坐标空间"会`更接近`

由于文本数据是`离散`的或者说是`非结构化`的，因此我们无法直接用它来执行`神经网络训练`所需的`数学运算`。我们需要一种将单词表示为连续值的`向量格式`的方法

将数据转换为向量格式的过程通常称为 **嵌入** （embedding）

> 不同的数据格式需要使用`不同的嵌入模型`。例如，为文本设计的嵌入模型并不适用于嵌入音频数据或视频数据。

![{%}|584](https://www.ituring.com.cn/figures/2025/LargeLanguageModel/011.jpg)

## 2. 为什么需要词嵌入？

- 🔍 例子：给计算机讲"苹果"这个词
- 传统方法：
	- 简单编号 "苹果" = 1
- 词嵌入：创建一个复杂的数字向量，包含更多信息
    - 向量可以表示：红色、圆形、水果等特征
    - 向量长度可以从几十到上千维

## 3. 词嵌入的技术原理

### 3.1 词嵌入生成方法  

- `Word2Vec`：最早的词嵌入技术
- 核心思路：出现在相似语境的词，其向量更相似
- 训练方法：预测上下文或根据上下文预测目标词

![{%}|739](https://www.ituring.com.cn/figures/2025/LargeLanguageModel/012.jpg)

如果词嵌入是二维的，那么就可以将它们绘制在二维散点图中进行可视化。在使用词嵌入技术（如 word2vec）时，表示相似概念的词通常会在嵌入空间中彼此接近。例如，在嵌入空间中，不同类型的鸟类的距离通常比国家和城市之间的距离更近


### 3.2 维度选择

- 低维度（1-10）：
	- 简单特征
- 高维度（768-12288）：
	- 复杂语义信息
- GPT-2模型：
    - 最小模型：768维
    - 最大模型：12,288维

## 4. 可视化理解

假设我们有一个3维的词嵌入空间：

- "狗"的向量：`[0.2, 0.6, 0.8]`
- "猫"的向量：`[0.3, 0.5, 0.7]`
- "电脑"的向量：`[0.9, 0.1, 0.2]`

观察发现：
- "狗"和"猫"的向量更接近
- "电脑"的向量明显不同

## 5. 代码示例（Python风格）

```python
# 简单词嵌入示例  
word_embeddings = {  
    "狗": [0.2, 0.6, 0.8],  
    "猫": [0.3, 0.5, 0.7],  
    "电脑": [0.9, 0.1, 0.2]  
}  

# 计算向量相似度  
def vector_similarity(vec1, vec2):  
    # 这里可以使用余弦相似度等算法  
    pass  
```

## 6. 大语言模型中的词嵌入

在大模型中：

- 词嵌入是模型的输入层
- 训练过程中不断优化
- 可针对特定任务调整

## 7. 局限性

- 高维嵌入难以直接可视化
- 需要`降维技术`（`如PCA`）展示

## 总结

词嵌入就像是语言的"翻译官"，将人类语言转换为机器可理解的数学语言，是人工智能理解文本的关键技术！
