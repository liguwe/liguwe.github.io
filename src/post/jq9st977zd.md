
# LLM之外的词嵌入：简洁篇

`#2025/12/28` `#ai` 

“LLM 之外的词嵌入” 看作是 “`经典分布式表示技术`及其在`推荐系统中的工程实践`”。

虽然现在 LLM（如 GPT）是主流，但它们计算昂贵且复杂。

而在 LLM 爆发之前，以 `word2vec` 为代表的静态嵌入技术（Static Embeddings）已经统治了业界多年，并且至今在`推荐系统、搜索排序`等对延迟敏感的场景中依然是核心组件。

这一节主要揭示了 `word2vec 的训练黑魔法`以及如何将这种思想“移植”到`非文本业务`中。


## 目录
<!-- toc -->
 ## 1. 核心算法：word2vec 的“滑动窗口”与“二分类欺骗” 

word2vec 的目标和 LLM 一样：把 ID 映射为向量。但它的训练逻辑更加简单粗暴，非常适合工程师理解。
- 数据构造：滑动窗口 (Sliding Window) 想象你有一个固定长度的窗口（比如 size=2），在文本（数据流）上滑动。
    - 中心词 (Target): 
        - 窗口中间的词。
    - 上下文词 (Context):
        -  窗口内的其他词。
    - 逻辑：
        -  算法假设“出现在同一个窗口里的词，含义是相关的”。
- 训练任务：
    - 从“预测单词”降维到“二分类” 早期的神经网络语言模型试图预测“下一个词是什么？”。
    - 这在工程上是个噩梦，因为输出层要做一个巨大的 `Softmax`（比如词表有 10 万个词，就是 10 万分类问题，计算量极大）。
    -  word2vec (Skip-gram) 做了一个`极其聪明的工程优化`：
        - 它不预测“下一个词是谁”，而是预测“这两个词是否是邻居？”。
            - 输入：(词 A, 词 B)
            - 输出：0 或 1 (是/否)
            - 效果： 
                - 这瞬间把一个 10万分类问题 变成了一个 `二分类问题`（Logistic Regression），计算效率提升了几个数量级。
- 关键 Trick：
    - 负采样 (Negative Sampling) 如果只给模型看真实的相邻词（正样本），模型只要永远输出 1 就能达到 100% 准确率，学不到任何东西。 
    - 所以，必须引入“噪声”。我们在训练时，人为地随机抽取一些不相关的词作为负样本（Label = 0）。
        - 正样本： (Thou, shalt) -> Label 1 (来自原文)
        - 负样本： (Thou, taco) -> Label 0 (随机凑的)
        - 模型目标： 拉近正样本向量的距离，推远负样本向量的距离。,

## 2. 预训练模型：拿来主义 (Gensim)

对于工程师来说，很多时候不需要自己训练。书中提到了使用 Python 的 `Gensim` 库。
- 类比： 这就像下载一个开源的 `Shared Library` 或者 `Lookup Table`。
- 操作： 你直接下载 Google 或 Stanford 训练好的 `glove-wiki-gigaword-50` 包。
- 能力： 它提供了一个现成的向量数据库。你可以直接进行向量加减运算。
    - 经典的向量算术：`King - Man + Woman = Queen`。这证明了`向量空间中`保留了语义的相对关系。

## 3. 工程实战：万物皆可 Embedding (推荐系统)

这是本节对软件工程师最有价值的部分。word2vec 的思想不仅仅用于 NLP，它适用于`任何“序列数据”`。

书中举了一个音乐推荐系统的例子，展示了如何把 NLP 技术迁移到业务中：
- 数据映射 (Mapping):
    - NLP 中的“句子” -> 业务中的“歌单” (Playlist) (或者是用户的点击流、购物车序列)。
    - NLP 中的“单词” -> 业务中的“歌曲” (Song) (或者是商品 ID、视频 ID)。
- 训练逻辑：
	-  如果两首歌经常出现在`同一个歌单里`（就像两个词经常出现在同一个句子里），那么它们的向量`在数学空间上`就会非常接近。
- 代码实现： 你完全不需要修改算法代码，只需要把你的 `List<List<SongID>>` 喂给 `gensim.models.Word2Vec` 即可。

```python
# 你的业务数据：歌单列表
playlists = ['song_1', 'song_5', ...], ['song_2', 'song_9', ...](/post/jq9st977zd.html#'song_1',-'song_5',-],-['song_2',-'song_9',-)

# 直接训练
model = Word2Vec(playlists, vector_size=32, window=20, ...)

# 业务应用：猜你喜欢
# 找出和 'song_1' 最像的歌
recommendations = model.wv.most_similar('song_1')
```

## 总结：给工程师的“太长不看版”

1. word2vec 本质： 是一个通过`二分类`任务（判断两个 ID 是否相邻）来学习 ID 向量表示的高效算法。
2. 核心技巧： 
	- 负采样（Negative Sampling）是它训练速度极快的秘诀，避免了全词表计算。
3. 跨界应用： 
	- 不要被“Word”这个词限制住。只要你的业务数据有序列特征（用户行为轨迹、日志流、交易记录），你都可以把它当作“句子”，用 word2vec 训练出 Item Embedding，用于推荐系统或相似物品检索。这是目前业界性价比极高的基线方案。
