
# word2vec 核心原理：滑动窗口、二分类欺骗（及其解决）、负采样

`#2025/12/28` `#ai` 


> 你可以把训练 word2vec 想象成训练一个`只会做判断题`的学生。

---


## 目录
<!-- toc -->
 ## 1. 滑动窗口（Sliding Window）：寻找“朋友圈” 

- 核心逻辑：
	- 怎么定义两个词“关系好”？`离得近`就是关系好。
- 想象句子是一排排坐着的学生。
	- word2vec 拿着一个框（窗口），比如这个框能罩住 5 个人（中心词 + 左右各 2 个）。
- 例子：句子是 “我 爱 吃 北京 烤 鸭”。
- 窗口移动：
    - 当框的中心对准 “吃” 时，框里还有 “我”、“爱”（左边）和 “北京”、“烤”（右边）。
- 结论：
	- 在这个瞬间，word2vec 认为 “吃” 和 “烤” 是朋友（正例），“吃” 和 “爱” 也是朋友。

这就叫滑动窗口：通过移动这个框，我们`收集了一堆“朋友关系”的名单`。


## 2. 二分类欺骗（Binary Classification Trickery）：为什么不能只教“对”的？

核心逻辑： 如果考试只有“对”的选项，学生就会全选“对”来骗分。

我们把收集到的“朋友名单”拿给模型训练：
- 题目：“吃”和“烤”是朋友吗？
- 标准答案：是（1）。
- 题目：“吃”和“鸭”是朋友吗？
- 标准答案：是（1）。

问题来了（文档中提到的“投机取巧”）：  
如果你只给模型看这些“是朋友”的例子（正例），模型是个大聪明，它发现：“**只要我永远回答‘是’（输出1），我就能拿 100 分！**”。

这时候，模型根本没学会谁跟谁真正有关系，它只是学会了无脑点赞。所有的词向量都会变得一模一样，完全没有区分度。



## 3. 负采样（Negative Sampling）：必须得有“错题”

核心逻辑： 为了防止模型“无脑全选对”，必须强行塞给它一些“错”的搭配，逼它学会区分。

我们需要制造一些“不是朋友”的例子（负例）。

- 正例（来自滑动窗口）：
    - 题目：“吃” + “烤” -> 答案：1 (是)
- 负例（怎么来？）：
    - 我们就从词典里随机瞎抽几个词！
    - 比如抽到了“坦克”、“哲学”、“马桶”。
    - 题目：“吃” + “坦克” -> 答案：0 (不是)
    - 题目：“吃” + “哲学” -> 答案：0 (不是)

这就是负采样：  
在训练每一个正例（如“吃-烤”）时，我们顺便随机抓几个“陪练”的假词（如“吃-坦克”），告诉模型：“这个才是错的，别瞎选！”。

为什么叫“采样”？  

因为词典里有几万个词，绝大多数都是“不是朋友”。我们不可能把所有不相关的词都拿来训练一遍（计算量太大了），所以只能随机采几个样本代表“错误的群体”。

---

## 4. 总结：这套组合拳是如何炼成“词向量”的？

把这三者结合起来，word2vec 是这样工作的：

1. 滑动窗口说：“我扫描了整本书，发现`‘国王’和‘王后’经常挨在一起`。” -> 这是正例。
2. 负采样说：“但我随便抽了一次，`‘国王’和‘冰箱’从来没在一起过`。” -> 这是负例。
3. 对比学习（训练目标）说：
	- “模型你听好了，你要调整参数（也就是词向量的数值），让‘国王’和‘王后’的向量算出来的分很高（接近1），让‘国王’和‘冰箱’的分很低（接近0）。”

最终结果：  
经过`几亿次`这样的“拉锯战”，模型被迫学会了：
- 把经常一起出现的词（语义相似）的向量拉近。
- 把不相关的词的向量推远。  
- 这就是为什么 word2vec 训练出来的 `king` 和 `queen` 向量距离很近的原因。
