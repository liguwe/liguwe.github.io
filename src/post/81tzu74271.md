
# é¢å‘å‰ç«¯çš„ python æŒ‡å— 05ï¼š å®ç°åˆ†è¯å™¨ç±»ï¼ˆTokenizer Classï¼‰

`#2026/01/04` `#python` 


## ç›®å½•
<!-- toc -->
 ## ğŸ“ åŠŸèƒ½è¯´æ˜ 

åˆ›å»ºå¯å¤ç”¨çš„ `SimpleTokenizerV1` ç±»ï¼Œæä¾› `encode()`ï¼ˆæ–‡æœ¬â†’IDï¼‰å’Œ `decode()`ï¼ˆIDâ†’æ–‡æœ¬ï¼‰æ–¹æ³•ã€‚

```python
"""
æ­¥éª¤ 5: å®ç°åˆ†è¯å™¨ç±» (Tokenizer Class)
åŠŸèƒ½: åˆ›å»ºå¯å¤ç”¨çš„åˆ†è¯å™¨ç±»ï¼Œæ”¯æŒç¼–ç å’Œè§£ç 
"""

import re
from typing import List, Dict

class SimpleTokenizerV1:
    """
    ç®€å•åˆ†è¯å™¨ V1 ç‰ˆæœ¬
    - åŸºäºé¢„å®šä¹‰çš„è¯æ±‡è¡¨ï¼ˆvocabï¼‰è¿›è¡Œç¼–ç å’Œè§£ç 
    - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
    """

    def __init__(self, vocab: Dict[str, int]):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨

        å‚æ•°:
            vocab (dict): è¯æ±‡è¡¨å­—å…¸ï¼Œæ ¼å¼ä¸º {token_string: token_id}
                         ä¾‹å¦‚: {"hello": 0, "world": 1, ",": 2}

        å±æ€§:
            self.str_to_int: å­—ç¬¦ä¸²åˆ°æ•´æ•°çš„æ˜ å°„ï¼ˆç¼–ç ç”¨ï¼‰
            self.int_to_str: æ•´æ•°åˆ°å­—ç¬¦ä¸²çš„æ˜ å°„ï¼ˆè§£ç ç”¨ï¼‰
        """
        # ä¿å­˜åŸå§‹è¯æ±‡è¡¨ï¼šå­—ç¬¦ä¸² -> æ•´æ•° ID
        self.str_to_int = vocab

        # åˆ›å»ºåå‘æ˜ å°„ï¼šæ•´æ•° ID -> å­—ç¬¦ä¸²
        # ä½¿ç”¨å­—å…¸æ¨å¯¼å¼ï¼Œå°† vocab çš„é”®å€¼å¯¹åè½¬
        # ä¾‹å¦‚: {"hello": 0, "world": 1} -> {0: "hello", 1: "world"}
        self.int_to_str = {i: s for s, i in vocab.items()}

        print(f"âœ“ åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  è¯æ±‡è¡¨å¤§å°: {len(vocab)}")

    def encode(self, text: str) -> List[int]:
        """
        ç¼–ç æ–¹æ³•ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•° ID åºåˆ—

        å‚æ•°:
            text (str): å¾…ç¼–ç çš„æ–‡æœ¬å­—ç¬¦ä¸²

        è¿”å›:
            list[int]: æ•´æ•° ID åˆ—è¡¨

        å¤„ç†æµç¨‹:
            1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬
            2. å»é™¤ç©ºç™½é¡¹
            3. å°†æ¯ä¸ª token æ˜ å°„ä¸ºå¯¹åº”çš„æ•´æ•° ID
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ–‡æœ¬
        # æ¨¡å¼è¯´æ˜:
        # - r'(...)': åŸå§‹å­—ç¬¦ä¸² + æ•è·ç»„ï¼Œä¿ç•™åˆ†éš”ç¬¦
        # - [,.:;?_!"()\']:  åŒ¹é…å¸¸è§æ ‡ç‚¹ç¬¦å·
        # - |--:            åŒ¹é…åŒè¿å­—ç¬¦
        # - |\s:            åŒ¹é…ä»»æ„ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)

        # æ¸…ç†åˆ†è¯ç»“æœï¼š
        # - item.strip(): å»é™¤æ¯ä¸ª token ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦
        # - if item.strip(): è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²ï¼ˆåªåŒ…å«ç©ºç™½çš„é¡¹ï¼‰
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ç®€æ´åœ°å®Œæˆè¿‡æ»¤å’Œæ¸…ç†
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]

        # å°†æ¸…ç†åçš„ token è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­çš„æ•´æ•° ID
        # å¦‚æœ token ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè¿™é‡Œä¼šæŠ¥ KeyError (V1 ç‰ˆæœ¬æš‚ä¸å¤„ç†æœªçŸ¥å•è¯)
        ids = [self.str_to_int[token] for token in preprocessed]
        return ids

    def decode(self, ids: List[int]) -> str:
        """
        è§£ç æ–¹æ³•ï¼šå°†æ•´æ•° ID åºåˆ—è¿˜åŸä¸ºæ–‡æœ¬

        å‚æ•°:
            ids (list[int]): æ•´æ•° ID åˆ—è¡¨

        è¿”å›:
            str: è§£ç åçš„æ–‡æœ¬å­—ç¬¦ä¸²

        å¤„ç†æµç¨‹:
            4. å°†æ¯ä¸ªæ•´æ•° ID æ˜ å°„å›å¯¹åº”çš„å­—ç¬¦ä¸² token
            5. ç”¨ç©ºæ ¼è¿æ¥æ‰€æœ‰ token
            6. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ ‡ç‚¹ç¬¦å·å‰çš„å¤šä½™ç©ºæ ¼
        """
        # å°†æ•´æ•° ID åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥
        # ä¾‹å¦‚: [0, 1, 2] -> ["hello", "world", ","] -> "hello world ,"
        # å¯¹æ¯” JavaScript â†’ ["hello", "world", ","].join(" ")
        text = ' '.join([self.int_to_str[i] for i in ids])

        # å»é™¤æ ‡ç‚¹ç¬¦å·å‰çš„å¤šä½™ç©ºæ ¼
        # æ­£åˆ™è¡¨è¾¾å¼è¯´æ˜:
        # - r'\s+([,.:;?_!"()\'])': åŒ¹é…"ä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ + æ ‡ç‚¹ç¬¦å·"
        # - r'\1': æ›¿æ¢ä¸ºç¬¬ä¸€ä¸ªæ•è·ç»„ï¼ˆå³æ ‡ç‚¹ç¬¦å·æœ¬èº«ï¼‰ï¼Œå»æ‰å‰é¢çš„ç©ºæ ¼
        # ä¾‹å¦‚: "hello , world !" -> "hello, world!"
        # å¯¹æ¯” JavaScript â†’ text.replace(/\s+([,.:;?_!"()\'])/g, "$1");
        text = re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text)
        return text

def test_tokenizer(tokenizer: SimpleTokenizerV1) -> None:
    """
    æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½

    å‚æ•°:
        tokenizer: åˆ†è¯å™¨å®ä¾‹
    """
    print("\n" + "=" * 60)
    print("æµ‹è¯•åˆ†è¯å™¨")
    print("=" * 60)

    # æµ‹è¯•æ–‡æœ¬
    test_text = """It's the last he painted, you know," Mrs. Gisburn said with pardonable pride."""

    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}\n")

    # ç¼–ç 
    ids = tokenizer.encode(test_text)
    print(f"âœ“ ç¼–ç å®Œæˆ")
    print(f"  Token IDs: {ids}")
    print(f"  IDs æ•°é‡: {len(ids)}")

    # è§£ç 
    decoded_text = tokenizer.decode(ids)
    print(f"\nâœ“ è§£ç å®Œæˆ")
    print(f"  è§£ç æ–‡æœ¬: {decoded_text}")

    # éªŒè¯
    print(f"\nâœ“ éªŒè¯ç»“æœ:")
    print(f"  åŸæ–‡é•¿åº¦: {len(test_text)}")
    print(f"  è§£ç é•¿åº¦: {len(decoded_text)}")
    print(f"  å®Œå…¨ä¸€è‡´: {test_text == decoded_text}")

    # å±•ç¤ºä¸€äº› token æ˜ å°„
    print(f"\nâœ“ Token æ˜ å°„ç¤ºä¾‹:")
    for i, token_id in enumerate(ids[:10]):
        token = tokenizer.int_to_str[token_id]
        print(f"    {token_id:4d} -> {repr(token)}")

if __name__ == "__main__":
    print("=" * 60)
    print("æ­¥éª¤ 5: å®ç°åˆ†è¯å™¨ç±»")
    print("=" * 60)

    # å¯¼å…¥å‰é¢æ­¥éª¤çš„å‡½æ•°
    from read_file import read_file
    from tokenization import tokenize
    from create_vocab import create_vocab

    # è¯»å–æ–‡ä»¶
    print("\n[1/4] è¯»å–æ–‡ä»¶...")
    raw_text = read_file()

    # è¿›è¡Œåˆ†è¯
    print("\n[2/4] åˆ†è¯...")
    tokens = tokenize(raw_text)

    # åˆ›å»ºè¯æ±‡è¡¨
    print("\n[3/4] åˆ›å»ºè¯æ±‡è¡¨...")
    vocab = create_vocab(tokens)

    # åˆ›å»ºåˆ†è¯å™¨
    print("\n[4/4] åˆ›å»ºåˆ†è¯å™¨...")
    tokenizer = SimpleTokenizerV1(vocab)

    # æµ‹è¯•åˆ†è¯å™¨
    test_tokenizer(tokenizer)

    print("\n" + "=" * 60)
    print("æ­¥éª¤ 5 å®Œæˆï¼")
    print("=" * 60)

```

---

## ğŸ” æ ¸å¿ƒæ¦‚å¿µ

### 1. ç±»ï¼ˆClassï¼‰å®šä¹‰

#### Python å®ç°

```python
class SimpleTokenizerV1:
    """ç®€å•åˆ†è¯å™¨ V1 ç‰ˆæœ¬"""

    def __init__(self, vocab: Dict[str, int]):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}
```

#### JavaScript/TypeScript ç­‰ä»·å®ç°

```typescript
class SimpleTokenizerV1 {
  private strToInt: Map<string, number>;
  private intToStr: Map<number, string>;

  constructor(vocab: Record<string, number>) {
    // ä¿å­˜åŸå§‹è¯æ±‡è¡¨
    this.strToInt = new Map(Object.entries(vocab).map(([k, v]) => [k, v]));

    // åˆ›å»ºåå‘æ˜ å°„
    this.intToStr = new Map(Object.entries(vocab).map(([k, v]) => [v, k]));
  }

  encode(text: string): number[] {
    // ç¼–ç é€»è¾‘
    return [];
  }

  decode(ids: number[]): string {
    // è§£ç é€»è¾‘
    return "";
  }
}
```

**ç±»çš„åŸºæœ¬è¯­æ³•ï¼š**

```python
class MyClass:
    """ç±»æ–‡æ¡£å­—ç¬¦ä¸²"""

    # ç±»å±æ€§ï¼ˆæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
    class_attr = "I am shared"

    def __init__(self, value):
        """æ„é€ æ–¹æ³•"""
        # å®ä¾‹å±æ€§ï¼ˆæ¯ä¸ªå®ä¾‹ç‹¬ç«‹ï¼‰
        self.instance_attr = value

    def method(self):
        """å®ä¾‹æ–¹æ³•"""
        return self.instance_attr

# ä½¿ç”¨
obj = MyClass("hello")
print(obj.method())  # "hello"
```

---

### 2. **init** æ„é€ æ–¹æ³•

#### Python å®ç°

```python
def __init__(self, vocab: Dict[str, int]):
    """åˆå§‹åŒ–åˆ†è¯å™¨"""
    self.str_to_int = vocab
    self.int_to_str = {i: s for s, i in vocab.items()}
```

#### JavaScript ç­‰ä»·å®ç°

```javascript
constructor(vocab) {
  // åˆå§‹åŒ–
  this.strToInt = vocab;
  this.intToStr = Object.fromEntries(
    Object.entries(vocab).map(([k, v]) => [v, k])
  );
}
```

`init` è¯¦è§£ï¼š 

```python
class Person:
    def __init__(self, name, age):
        """åˆ›å»ºå¯¹è±¡æ—¶è‡ªåŠ¨è°ƒç”¨"""
        self.name = name
        self.age = age

# ä½¿ç”¨
person = Person("Alice", 25)
# __init__ è‡ªåŠ¨è¢«è°ƒç”¨
print(person.name)  # "Alice"
print(person.age)   # 25

# JavaScript å¯¹æ¯”
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
}

const person = new Person("Alice", 25);
console.log(person.name);  // "Alice"
```

**å…³é”®åŒºåˆ«ï¼š**
- Python: `__init__(self)` - åˆå§‹åŒ–å·²åˆ›å»ºçš„å¯¹è±¡
- JavaScript: `constructor()` - åˆ›å»ºå¹¶åˆå§‹åŒ–å¯¹è±¡

---

### 3. self å…³é”®å­—

> self - this

#### Python å®ç°

```python
class SimpleTokenizerV1:
    def __init__(self, vocab):
        # self æŒ‡å‘å½“å‰å®ä¾‹
        self.str_to_int = vocab

    def encode(self, text):
        # self è®¿é—®å®ä¾‹å±æ€§
        return [self.str_to_int[token] for token in text.split()]
```

#### JavaScript ç­‰ä»·å®ç°

```javascript
class SimpleTokenizerV1 {
  constructor(vocab) {
    // this æŒ‡å‘å½“å‰å®ä¾‹
    this.strToInt = vocab;
  }

  encode(text) {
    // this è®¿é—®å®ä¾‹å±æ€§
    return text.split().map(token => this.strToInt[token]);
  }
}
```

**self vs thisï¼š**

| ç‰¹æ€§ | Python (self) | JavaScript (this) |
|------|--------------|-----------------|
| æ˜¯å¦å¿…éœ€ | æ˜¯ï¼ˆå¿…é¡»æ˜¾å¼å£°æ˜ï¼‰ | æ˜¯ï¼ˆéšå¼ï¼‰ |
| ä½œä¸ºå‚æ•° | ç¬¬ä¸€å‚æ•° | ä¸éœ€è¦ |
| æŒ‡å‘ | å½“å‰å®ä¾‹ | å–å†³äºè°ƒç”¨æ–¹å¼ |
| ç®­å¤´å‡½æ•° | æ—  | ç»‘å®šå¤–å±‚ this |

**Python çš„ selfï¼š**

```python
class Example:
    def __init__(self, value):
        self.value = value  # self å¿…é¡»æ˜¾å¼ä½¿ç”¨

    def show(self):
        # self å¿…é¡»ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
        print(f"Value: {self.value}")

# è°ƒç”¨æ—¶ä¸éœ€è¦ä¼  self
obj = Example(42)
obj.show()  # self è‡ªåŠ¨ç»‘å®šåˆ° obj
```

**JavaScript çš„ thisï¼š**

```javascript
class Example {
  constructor(value) {
    this.value = value;  // this è‡ªåŠ¨æŒ‡å‘å®ä¾‹
  }

  show() {
    console.log(`Value: ${this.value}`);
  }
}

const obj = new Example(42);
obj.show();  // this è‡ªåŠ¨ç»‘å®šåˆ° obj
```

---

### 4. å®ä¾‹æ–¹æ³•

#### Python å®ç°

```python
class SimpleTokenizerV1:
    def encode(self, text: str) -> List[int]:
        """ç¼–ç æ–¹æ³•"""
        # æ–¹æ³•ä½“
        return ids

    def decode(self, ids: List[int]) -> str:
        """è§£ç æ–¹æ³•"""
        # æ–¹æ³•ä½“
        return text
```

#### JavaScript ç­‰ä»·å®ç°

```javascript
class SimpleTokenizerV1 {
  encode(text) {
    // ç¼–ç æ–¹æ³•
    return [];
  }

  decode(ids) {
    // è§£ç æ–¹æ³•
    return "";
  }
}
```

**æ–¹æ³•ç±»å‹å¯¹æ¯”ï¼š**

```python hl:11,17
class Example:
    class_var = "shared"

    def __init__(self):
        self.instance_var = "unique"

    # å®ä¾‹æ–¹æ³•ï¼ˆæœ€å¸¸è§ï¼‰
    def instance_method(self):
        return self.instance_var

    # ç±»æ–¹æ³•
    @classmethod
    def class_method(cls):
        return cls.class_var

    # é™æ€æ–¹æ³•
    @staticmethod
    def static_method():
        return "static"

# ä½¿ç”¨
obj = Example()
obj.instance_method()  # "unique"
Example.class_method()  # "shared"
Example.static_method()  # "static"
```

```javascript hl:13,8
class Example {
  static classVar = "shared";

  constructor() {
    this.instanceVar = "unique";
  }

  // å®ä¾‹æ–¹æ³•
  instanceMethod() {
    return this.instanceVar;
  }

  // é™æ€æ–¹æ³•
  static staticMethod() {
    return "static";
  }
}

// ä½¿ç”¨
const obj = new Example();
obj.instanceMethod();  // "unique"
Example.staticMethod();  // "static"
```

---

### 5. å­—ç¬¦ä¸²çš„ join() æ–¹æ³•

#### Python å®ç°

```python
# å°†æ•´æ•° ID åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥
text = ' '.join([self.int_to_str[i] for i in ids])
# ä¾‹å¦‚: [0, 1, 2] -> ["hello", "world", ","] -> "hello world ,"
```

#### JavaScript ç­‰ä»·å®ç°

```javascript
// å°†æ•´æ•° ID åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ•°ç»„ï¼Œç„¶åç”¨ç©ºæ ¼è¿æ¥
const text = ids.map(i => this.intToStr.get(i)).join(' ');
// ä¾‹å¦‚: [0, 1, 2] -> ["hello", "world", ","] -> "hello world ,"
```

**join() è¯¦è§£ï¼š**

```python
# Python
words = ["Hello", "world", "!"]

# ç”¨ç©ºæ ¼è¿æ¥
' '.join(words)  # "Hello world !"

# ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥
''.join(words)   # "Helloworld!"

# ç”¨é€—å·è¿æ¥
','.join(words)  # "Hello,world,!"

# ç”¨æ¢è¡Œç¬¦è¿æ¥
'\n'.join(words)  # "Hello\nworld\n!"
```

```javascript
// JavaScript
const words = ["Hello", "world", "!"];

// ç”¨ç©ºæ ¼è¿æ¥
words.join(' ');  // "Hello world !"

// ç”¨ç©ºå­—ç¬¦ä¸²è¿æ¥
words.join('');   // "Helloworld!"

// ç”¨é€—å·è¿æ¥
words.join(',');  // "Hello,world,!"

// ç”¨æ¢è¡Œç¬¦è¿æ¥
words.join('\n');  // "Hello\nworld\n!"
```

---

### 6. æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ï¼ˆre.subï¼‰

#### Python å®ç°

```python
# å»é™¤æ ‡ç‚¹ç¬¦å·å‰çš„å¤šä½™ç©ºæ ¼
text = re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text)
# "hello , world !" -> "hello, world!"
```

#### JavaScript ç­‰ä»·å®ç°

```javascript
// å»é™¤æ ‡ç‚¹ç¬¦å·å‰çš„å¤šä½™ç©ºæ ¼
const text = text.replace(/\s+([,.:;?_!"()\'])/g, '$1');
// "hello , world !" -> "hello, world!"
```

**re.sub() è¯¦è§£ï¼š**

```python
import re

text = "Hello, World!"

# åŸºæœ¬æ›¿æ¢
re.sub('World', 'Python', text)  # "Hello, Python!"

# ä½¿ç”¨æ­£åˆ™
re.sub(r'\b[a-z]+\b', 'word', text)  # "word, word!"

# ä½¿ç”¨æ•è·ç»„
re.sub(r'(Hello), (World)', r'\2 and \1', text)  # "World and Hello!"

# ä½¿ç”¨å‡½æ•°
def repl(match):
    return match.group(0).upper()

re.sub(r'[a-z]+', repl, text)  # "HELLO, WORLD!"
```

**å¯¹æ¯”è¡¨ï¼š**

| æ“ä½œ | Python | JavaScript |
|------|--------|-----------|
| åŸºæœ¬æ›¿æ¢ | `re.sub(pattern, repl, text)` | `text.replace(pattern, repl)` |
| å…¨å±€æ›¿æ¢ | é»˜è®¤å…¨éƒ¨æ›¿æ¢ | éœ€è¦ `/g` æ ‡å¿— |
| æ•è·ç»„ | `\1`, `\2` | `$1`, `$2` |
| ä½¿ç”¨å‡½æ•° | `repl` å‚æ•° | å‡½æ•°ä½œä¸ºç¬¬äºŒä¸ªå‚æ•° |

---

### 7. ç±»å‹æç¤ºï¼ˆç±»æ–¹æ³•ï¼‰

#### Python å®ç°

```python
from typing import List, Dict

class SimpleTokenizerV1:
    def __init__(self, vocab: Dict[str, int]):
        pass

    def encode(self, text: str) -> List[int]:
        pass

    def decode(self, ids: List[int]) -> str:
        pass
```

#### TypeScript ç­‰ä»·å®ç°

```typescript
class SimpleTokenizerV1 {
  constructor(vocab: Record<string, number>) {}

  encode(text: string): number[] {
    return [];
  }

  decode(ids: number[]): string {
    return "";
  }
}
```

---

## ğŸ¯ Python æœ€ä½³å®è·µ

### 1. ä½¿ç”¨å±æ€§ï¼ˆ@propertyï¼‰

```python hl:7,5,13
class Tokenizer:
    def __init__(self, vocab):
        self._vocab = vocab

    @property
    def vocab_size(self):
        """åªè¯»å±æ€§"""
        return len(self._vocab)

# ä½¿ç”¨
tokenizer = Tokenizer(vocab)
print(tokenizer.vocab_size)  # åƒè®¿é—®å±æ€§ä¸€æ ·
# ä¸æ˜¯ tokenizer.vocab_size()
```

### 2. ä½¿ç”¨ **str** å’Œ **repr**

```python hl:15,16
class Tokenizer:
    def __init__(self, vocab):
        self.vocab = vocab

    def __str__(self):
        """é¢å‘ç”¨æˆ·"""
        return f"Tokenizer(vocab_size={len(self.vocab)})"

    def __repr__(self):
        """é¢å‘å¼€å‘è€…"""
        return f"Tokenizer(vocab={self.vocab})"

# ä½¿ç”¨
tokenizer = Tokenizer({"hello": 0})
print(tokenizer)    # Tokenizer(vocab_size=1)
repr(tokenizer)     # Tokenizer(vocab={'hello': 0})
```

---

## ğŸ“š æ·±å…¥ç†è§£ï¼šåˆ†è¯å™¨è®¾è®¡

### ç¼–ç -è§£ç ä¸€è‡´æ€§

```python
# ç†æƒ³æƒ…å†µ
text = "Hello, world!"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)
assert text == decoded  # åº”è¯¥ç›¸ç­‰

# å®é™…æƒ…å†µï¼ˆå¯èƒ½ä¸ä¸€è‡´ï¼‰
text = "Hello, world!"
ids = tokenizer.encode(text)
decoded = tokenizer.decode(ids)
# "hello, world!" (å¤§å°å†™å¯èƒ½æ”¹å˜)
```

### å¤„ç†æœªçŸ¥è¯ï¼ˆOOVï¼‰

```python
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}
        self.unk_id = vocab.get("<UNK>", -1)

    def encode(self, text):
        tokens = text.split()
        ids = []
        for token in tokens:
            if token in self.str_to_int:
                ids.append(self.str_to_int[token])
            elif self.unk_id >= 0:
                ids.append(self.unk_id)
            else:
                raise ValueError(f"Unknown token: {token}")
        return ids
```

---

## ğŸ”„ Python vs JavaScript å®Œæ•´å¯¹æ¯”

### åˆ†è¯å™¨ç±»å®Œæ•´å®ç°

#### Python

```python hl:5
import re
from typing import List, Dict

class SimpleTokenizerV1:
    def __init__(self, vocab: Dict[str, int]):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def encode(self, text: str) -> List[int]:
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        tokens = [t.strip() for t in preprocessed if t.strip()]
        return [self.str_to_int[t] for t in tokens]

    def decode(self, ids: List[int]) -> str:
        text = ' '.join([self.int_to_str[i] for i in ids])
        return re.sub(r'\s+([,.:;?_!"()\'])', r'\1', text)
```

#### JavaScript

```javascript
class SimpleTokenizerV1 {
  constructor(vocab) {
    this.strToInt = vocab;
    this.intToStr = Object.fromEntries(
      Object.entries(vocab).map(([k, v]) => [v, k])
    );
  }

  encode(text) {
    const preprocessed = text.split(/([,.:;?_!"()\']|--|\s)/);
    const tokens = preprocessed
      .map(t => t.trim())
      .filter(t => t.length > 0);
    return tokens.map(t => this.strToInt[t]);
  }

  decode(ids) {
    let text = ids.map(i => this.intToStr[i]).join(' ');
    return text.replace(/\s+([,.:;?_!"()\'])/g, '$1');
  }
}
```

---

## ğŸ“š æ€»ç»“

**å…³é”®è¦ç‚¹ï¼š**

1. âœ… **ç±»å’Œå¯¹è±¡** - å°è£…æ•°æ®å’Œè¡Œä¸º
2. âœ… ****init** æ„é€ æ–¹æ³•** - åˆå§‹åŒ–å®ä¾‹
3. âœ… **self å…³é”®å­—** - è®¿é—®å®ä¾‹å±æ€§å’Œæ–¹æ³•
4. âœ… **å®ä¾‹æ–¹æ³•** - å®šä¹‰å¯¹è±¡è¡Œä¸º
5. âœ… **encode/decode** - åˆ†è¯å™¨çš„æ ¸å¿ƒåŠŸèƒ½

**Python vs JavaScriptï¼š**
- ç±»è¯­æ³•ç›¸ä¼¼
- `self` å¿…é¡»æ˜¾å¼å£°æ˜ vs `this` éšå¼
- `__init__` vs `constructor`
- æ–¹æ³•åå‡ ä¹ç›¸åŒ
