
# 分词器如何处理语言模型的输入

`#2025/12/27` `#ai` 

> 大模型（LLM）的底层核心根本不处理 `String`（字符串），它只处理 `Integer`（整数）数组。


## 目录
<!-- toc -->
 ## 一、核心问题：模型看不懂文字  

作为软件工程师，你肯定知道这个基本事实：**计算机只认数字**。

所以当你给 LLM 发送这样的提示词时：

```
"Write an email apologizing to Sarah"  
```

模型**实际上看不懂这些字母**。它需要一个"`翻译官`"把文字转成数字。这个`翻译官`就是**分词器（Tokenizer）**。

> [!info]  
   即 **翻译官** 就是 **分词器**

## 二、分词器的工作流程：三步转换

### 第 1 步：文本切分 → `词云`

分词器首先把文本切成小块，这些小块叫**词元（Token）**：

```
原始文本：  
"Have the bards who preceded me..."  

切分后：  
["Have", "the", "b", "ards", "who", "preceded", ...]  
```

> **注意**：不是简单地按空格切分！比如 "bards" 被切成了 "b" 和 "ards"。

### 第 2 步：映射为数字 ID

每个词元都对应一个唯一的整数 ID：

```
词元        → ID  
"Have"      → 6975  
"the"       → 278  
"b"         → 278  
"ards"      → 3163  
"who"       → 1058  
```

### 第 3 步：送入模型 → 一串数字

模型接收的不是文本，而是`这串数字`：

```python
# 你看到的  
"Write an email apologizing to Sarah..."  

# 模型实际接收的  
[1, 14350, 385, 4876, 27746, 5281, 304, 19235, ...]  
```

## 三、实际代码示例

让我们看看真实的代码：

```python hl:3,6,9
from transformers import AutoTokenizer  

# 1. 加载分词器  
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")  

# 2. 准备提示词  
prompt = "Write an email apologizing to Sarah"  

# 3. 分词器处理（编码）  
input_ids = tokenizer(prompt, return_tensors="pt").input_ids  
print(input_ids)  
```

**输出结果**：

```
tensor([1, 14350, 385, 4876, 27746, 5281, 304, 19235, ...](/post/rrte352kvn.html#1,-14350,-385,-4876,-27746,-5281,-304,-19235,-))  
```

这就是模型真正"看到"的东西——`一串整数`！

## 四、双向工作：编码与解码 → 分词器可以解码 + 解码

分词器是**双向工具**：

```python
# 方向 1：编码（文本 → 数字）  
input_ids = tokenizer.encode("Hello world")  
# 输出：[1, 15496, 995]  

# 方向 2：解码（数字 → 文本）  
text = tokenizer.decode([1, 15496, 995])  
# 输出："Hello world"  
```

**类比**：就像 `Base64` 编码和解码

- 编码：把数据转成可传输的格式
- 解码：把格式还原回原始数据

## 五、为什么要这样设计？

### 原因 1：模型只能处理数字

神经网络的本质是`矩阵运算`：
- 输入：
	- 数字矩阵
- 输出：
	- 数字矩阵
- 中间计算：全是数字

**不可能直接输入字母**。

### 原因 2：`词表`是`预先固定的`

分词器内部有一个**词元表（词表）**，类似`哈希表`：

```javascript
// 类比：一个 Map 数据结构  
const vocabulary = {  
  "Have": 6975,  
  "the": 278,  
  "b": 278,  
  "ards": 3163,  
  // ... 总共几万个词元  
}  
```

这个词表在**模型训练前就固定了**，不能随意更改。

## 六、关键概念总结 → 类比软件工程里的东西

| 概念        | 含义          | 类比           |
| --------- | ----------- | ------------ |
| **分词器**   | 文本 ↔ 数字的转换器 | 编解码器         |
| **词元**    | 文本的最小单位     | 字节码指令        |
| **词元 ID** | 词元的数字表示     | 内存地址         |
| **词表**    | 词元到 ID 的映射表 | Symbol Table |

## 七、完整的输入输出流程

```
用户输入  
    ↓  
"Write an email..."  ← 人类看到的  
    ↓  
[分词器：编码]  
    ↓  
[1, 14350, 385, ...] ← 模型接收的  
    ↓  
[语言模型处理]  
    ↓  
[3323, 622, 29901, ...] ← 模型输出的  
    ↓  
[分词器：解码]  
    ↓  
"Subject: My Sincere..." ← 人类看到的  
```

## 八、关键要点

1. **模型从不直接看文字**—— 所有文本都必须先转成数字
2. **分词器和模型绑定**—— 换分词器就得重新训练模型
3. **词元不等于单词**—— 可能是词、词的一部分、甚至标点符号
4. **双向转换是必需的**——输入需编码，输出需解码

> [!success]  
>  LLM 的"眼睛"只能看到数字，分词器就是它的"翻译官"。
