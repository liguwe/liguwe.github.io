
# 文本分词：让计算机精准"切割"文本


`#2026/01/01` `#ai` 

![{%}|584](https://www.ituring.com.cn/figures/2025/LargeLanguageModel/013.jpg)

> 图：大语言模型中文本处理步骤概览。在这一步骤图中，我们将输入文本分割成了单独的词元，这些词元既可以是单词，也可以是诸如标点符号之类的特殊字符


## 目录
<!-- toc -->
 ## 1. 什么是文本分词？ 

🔍 想象你有一段长长的文字，现在需要把它切成小块，这就是`分词`！

### 关键特点：

- 将连续的文本拆分成独立的"词元"
- 词元可以是单个单词
- 词元也可以包括标点符号

## 2. 为什么需要分词？

对于计算机来说：

- 需要将文本转换成它能理解的形式
- 每个`词元`都是处理文本的基本单位

## 3. 分词的实现方法

### 3.1 使用正则表达式分词

```python
import re  

# 示例文本  
text = "Hello, do you like tea?"  

# 使用正则表达式分割文本  
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)  

# 去除空白字符  
preprocessed = [item.strip() for item in preprocessed if item.strip()]  

print(preprocessed)  
# 输出：['Hello', ',', 'do', 'you', 'like', 'tea', '?']  
```

![{%}](https://www.ituring.com.cn/figures/2025/LargeLanguageModel/014.jpg)

## 4. 分词原则

• 保留文本的原始结构  
• 区分大小写  
• 保留专有名词的大小写特征

## 5. 实际应用案例

在本书中，作者使用短篇小说"The Verdict"作为文本样本：

- 总字符数：20,479
- 分词后的词元数：4,690

## 6. 分词的挑战

• 需要处理各种`特殊字符`  
• 要正确处理标点符号  
• 保持文本的`语义完整性`

## 7. 代码实现要点

```python
def tokenize(text):  
    # 使用正则表达式分割  
    tokens = re.split(r'([,.:;?_!"()\']|--|\s)', text)  
    
    # 去除空白字符  
    tokens = [token.strip() for token in tokens if token.strip()]  
    
    return tokens  
```

## 8. 为什么这么重要？

对于大语言模型来说：
- 分词是处理文本的第一步
- 将文本转换为可计算的形式
- 为后续的词元嵌入和模型训练做准备

## 结语

文本分词就像是为计算机准备的"文本切割刀"，能够精确地将复杂的文本转换成便于处理的小块！
