
# LLMä¹‹å¤–çš„è¯åµŒå…¥ï¼šä»£ç è¯¦è§£ç¯‡

`#2025/12/28` `#ai` 


## ç›®å½•
<!-- toc -->
 ## æ ¸å¿ƒè§‚ç‚¹ï¼šåµŒå…¥ = é€šç”¨çš„ç‰¹å¾å·¥ç¨‹æ¡†æ¶ 

ä½ å¯èƒ½ä»¥ä¸º"è¯åµŒå…¥"åªèƒ½ç”¨åœ¨ NLP é¢†åŸŸï¼Œä½†å®é™…ä¸ŠåµŒå…¥æ˜¯ä¸€ä¸ªé€šç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å¼â€”â€”`ç»™ä»»ä½•å¯¹è±¡åˆ†é…æœ‰æ„ä¹‰çš„å‘é‡è¡¨ç¤º`ã€‚

```python
# åµŒå…¥çš„æœ¬è´¨ï¼šå¯¹è±¡ â†’ å‘é‡æ˜ å°„  
è¯åµŒå…¥:    "Python"        â†’ [0.1, 0.5, 0.8, ...]  
ç”¨æˆ·åµŒå…¥:   user_12345      â†’ [0.3, 0.7, 0.2, ...]  
å•†å“åµŒå…¥:   product_789     â†’ [0.6, 0.2, 0.9, ...]  
æ­Œæ›²åµŒå…¥:   "Shape of You"  â†’ [0.4, 0.8, 0.1, ...]  
```

ä¸ºä»€ä¹ˆè¿™å¾ˆå¼ºå¤§ï¼Ÿ ä¸€æ—¦ä½ æŠŠå¯¹è±¡å˜æˆå‘é‡ï¼Œå°±å¯ä»¥ç”¨ç®€å•çš„æ•°å­¦è¿ç®—æ¥ï¼š
- è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ¨èç³»ç»Ÿï¼‰
- åšèšç±»åˆ†æï¼ˆç”¨æˆ·åˆ†ç¾¤ï¼‰
- å¯è§†åŒ–åˆ†æï¼ˆé™ç»´å±•ç¤ºï¼‰

## ä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥

### ğŸ¯ åœºæ™¯ï¼šå¿«é€Ÿå¯åŠ¨NLPé¡¹ç›®

å‡è®¾ä½ è¦åšæƒ…æ„Ÿåˆ†æï¼Œä½†æ²¡æœ‰GPUè®­ç»ƒå¤æ‚æ¨¡å‹ã€‚`é¢„è®­ç»ƒè¯åµŒå…¥`å°±æ˜¯ä½ çš„æ•‘æ˜Ÿã€‚

```python
import gensim.downloader as api  

# ä¸‹è½½é¢„è®­ç»ƒåµŒå…¥ï¼ˆ66 MBï¼Œè®­ç»ƒè‡ªç»´åŸºç™¾ç§‘ï¼‰  
model = api.load("glove-wiki-gigaword-50")  

# ç«‹å³ä½¿ç”¨ï¼  
similar_words = model.most_similar([model['king']], topn=5)  
print(similar_words)  
# è¾“å‡ºï¼š  
# [('prince', 0.82), ('queen', 0.78), ('emperor', 0.77), ...]  
```

### ğŸ“¦ å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹

```python
# word2vecï¼ˆGoogleè®­ç»ƒï¼Œæ–°é—»è¯­æ–™ï¼‰  
model = api.load("word2vec-google-news-300")  # 300ç»´ï¼Œ1.7GB  

# GloVeï¼ˆæ–¯å¦ç¦è®­ç»ƒï¼Œç»´åŸºç™¾ç§‘+æ–°é—»ï¼‰  
model = api.load("glove-wiki-gigaword-50")    # 50ç»´ï¼Œ66MB  
model = api.load("glove-wiki-gigaword-100")   # 100ç»´ï¼Œ128MB  
model = api.load("glove-wiki-gigaword-200")   # 200ç»´ï¼Œ252MB  
```

### ğŸ” å®é™…åº”ç”¨ç¤ºä¾‹

```python
# åœºæ™¯1ï¼šè¯è¯­ç›¸ä¼¼åº¦æœç´¢  
query = "programming"  
similar = model.most_similar(query, topn=10)  
# ç»“æœï¼šcoding, software, development, algorithm...  

# åœºæ™¯2ï¼šç±»æ¯”æ¨ç†ï¼ˆç»å…¸æ¡ˆä¾‹ï¼‰  
result = model.most_similar(  
    positive=['king', 'woman'],   
    negative=['man'],   
    topn=1  
)  
# king - man + woman â‰ˆ queen  

# åœºæ™¯3ï¼šè¯è¯­å…³ç³»åˆ¤æ–­  
similarity = model.similarity('cat', 'dog')      # 0.76 é«˜ç›¸ä¼¼  
similarity = model.similarity('cat', 'laptop')   # 0.12 ä½ç›¸ä¼¼  
```

### âš ï¸ é¢„è®­ç»ƒåµŒå…¥çš„å±€é™æ€§

- é—®é¢˜1ï¼šè¯è¡¨å¤–è¯æ±‡ï¼ˆOOVï¼‰  
- é—®é¢˜2ï¼šé™æ€è¡¨ç¤ºï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰ 
- é—®é¢˜3ï¼šè®­ç»ƒæ•°æ®åè§  

```python hl:1,7,12
# é—®é¢˜1ï¼šè¯è¡¨å¤–è¯æ±‡ï¼ˆOOVï¼‰  
try:  
    vec = model['ChatGPT']  # ğŸ’¥ KeyError!  
except KeyError:  
    print("è¯ä¸åœ¨è¯è¡¨ä¸­")  

# é—®é¢˜2ï¼šé™æ€è¡¨ç¤ºï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰  
bank_vec = model['bank']  # æ°¸è¿œç›¸åŒ  
# æ— æ³•åŒºåˆ†ï¼š  
# "river bank"ï¼ˆæ²³å²¸ï¼‰ vs "money bank"ï¼ˆé“¶è¡Œï¼‰  

# é—®é¢˜3ï¼šè®­ç»ƒæ•°æ®åè§  
# å¦‚æœè®­ç»ƒæ•°æ®æœ‰åè§ï¼ŒåµŒå…¥ä¹Ÿä¼šæœ‰åè§  
# ä¾‹å¦‚ï¼šæŸäº›èŒä¸šè¯ä¸æ€§åˆ«çš„ä¸å½“å…³è”  
```

## word2vecç®—æ³•ä¸å¯¹æ¯”è®­ç»ƒ

### ğŸ’¡ æ ¸å¿ƒæ€æƒ³ï¼šYou shall know a word by the company it keeps

Distributional Hypothesisï¼ˆåˆ†å¸ƒå‡è®¾ï¼‰ï¼šæ„æ€ç›¸è¿‘çš„è¯ï¼Œä¸Šä¸‹æ–‡ä¹Ÿç›¸è¿‘ã€‚

```python
# ä¾‹å­ï¼š  
å¥å­A: "The dog chased the cat"  
å¥å­B: "The puppy chased the mouse"  

# "dog" å’Œ "puppy" ä¸Šä¸‹æ–‡ç›¸ä¼¼ â†’ åµŒå…¥å‘é‡åº”è¯¥æ¥è¿‘  
# "cat" å’Œ "mouse" ä¸Šä¸‹æ–‡ç›¸ä¼¼ â†’ åµŒå…¥å‘é‡åº”è¯¥æ¥è¿‘  
```

---

### ğŸ› ï¸ word2vecè®­ç»ƒæµç¨‹è¯¦è§£

#### Step 1ï¼šæ»‘åŠ¨çª—å£ç”Ÿæˆè®­ç»ƒæ ·æœ¬

```python
text = "Thou shalt not make a machine in the likeness"  
window_size = 2  # å·¦å³å„çœ‹2ä¸ªè¯  

# æ»‘åŠ¨çª—å£ç¤ºä¾‹  
# ä¸­å¿ƒè¯: "make"  
# ä¸Šä¸‹æ–‡: ["not", "shalt", "a", "machine"]  

# ç”Ÿæˆæ­£ä¾‹æ ·æœ¬ï¼ˆå®é™…ç›¸é‚»çš„è¯å¯¹ï¼‰  
positive_pairs = [  
    ("make", "not"),      # label = 1  
    ("make", "shalt"),    # label = 1  
    ("make", "a"),        # label = 1  
    ("make", "machine"),  # label = 1  
]  
```

#### Step 2ï¼šè´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰

```python
# é—®é¢˜ï¼šå¦‚æœåªæœ‰æ­£ä¾‹ï¼Œæ¨¡å‹ä¼šä½œå¼Šï¼ˆæ°¸è¿œè¾“å‡º1ï¼‰  

# è§£å†³ï¼šæ·»åŠ è´Ÿä¾‹ï¼ˆéšæœºé€‰æ‹©ä¸ç›¸é‚»çš„è¯ï¼‰  
negative_pairs = [  
    ("make", "playback"),    # label = 0ï¼ˆéšæœºé€‰çš„ï¼‰  
    ("make", "sublime"),     # label = 0  
    ("make", "apothecary"),  # label = 0  
]  

# è®­ç»ƒæ•°æ®é›†  
training_data = [  
    # æ­£ä¾‹  
    ("make", "not", 1),  
    ("make", "shalt", 1),  
    # è´Ÿä¾‹  
    ("make", "playback", 0),  
    ("make", "sublime", 0),  
]  
```

#### Step 3ï¼šç¥ç»ç½‘ç»œè®­ç»ƒ

```python
import torch  
import torch.nn as nn  

class Word2VecModel(nn.Module):  
    def __init__(self, vocab_size, embedding_dim):  
        super().__init__()  
        # æ ¸å¿ƒï¼šåµŒå…¥çŸ©é˜µï¼ˆå°±åƒ2.2èŠ‚è®²çš„ï¼‰  
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  
        # è¾“å‡ºå±‚ï¼šé¢„æµ‹æ˜¯å¦ç›¸é‚»  
        self.output = nn.Linear(embedding_dim * 2, 1)  
        
    def forward(self, word1_id, word2_id):  
        # æŸ¥è¯¢ä¸¤ä¸ªè¯çš„åµŒå…¥  
        vec1 = self.embeddings(word1_id)  
        vec2 = self.embeddings(word2_id)  
        
        # æ‹¼æ¥åé¢„æµ‹  
        combined = torch.cat([vec1, vec2], dim=-1)  
        score = self.output(combined)  
        return torch.sigmoid(score)  # è¾“å‡º0-1ä¹‹é—´çš„æ¦‚ç‡  

# è®­ç»ƒè¿‡ç¨‹ï¼ˆä¼ªä»£ç ï¼‰  
model = Word2VecModel(vocab_size=50000, embedding_dim=300)  
optimizer = torch.optim.Adam(model.parameters())  
loss_fn = nn.BCELoss()  

for word1, word2, label in training_data:  
    prediction = model(word1, word2)  
    loss = loss_fn(prediction, label)  
    loss.backward()  # åå‘ä¼ æ’­æ›´æ–°åµŒå…¥çŸ©é˜µ  
    optimizer.step()  
```

è®­ç»ƒåçš„æ•ˆæœï¼š

- ç»å¸¸ä¸€èµ·å‡ºç°çš„è¯ â†’ åµŒå…¥å‘é‡æ¥è¿‘
- å¾ˆå°‘ä¸€èµ·å‡ºç°çš„è¯ â†’ åµŒå…¥å‘é‡è¿œç¦»

---

### ğŸ¨ å¯è§†åŒ–word2vecçš„è®­ç»ƒè¿‡ç¨‹

```
è®­ç»ƒå‰ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰ï¼š  
"king"  â†’ [0.12, -0.45, 0.89]  
"queen" â†’ [-0.67, 0.23, -0.11]  
ä½™å¼¦ç›¸ä¼¼åº¦ï¼š0.03ï¼ˆå‡ ä¹ä¸ç›¸å…³ï¼‰  

è®­ç»ƒåï¼ˆå­¦ä¹ åˆ°è¯­ä¹‰ï¼‰ï¼š  
"king"  â†’ [0.52, 0.83, 0.21]  
"queen" â†’ [0.48, 0.79, 0.25]  
ä½™å¼¦ç›¸ä¼¼åº¦ï¼š0.95ï¼ˆéå¸¸ç›¸ä¼¼ï¼‰âœ…  
```

---

## ğŸ”— word2vec ä¸ ç°ä»£LLMçš„è”ç³»

### å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰çš„æ™®é€‚æ€§

```python
# word2vecçš„æ€æƒ³ï¼ˆ2013ï¼‰  
ä»»åŠ¡ï¼šåˆ¤æ–­ä¸¤ä¸ªè¯æ˜¯å¦ç›¸é‚»  
è¾“å…¥ï¼š(word1, word2)  
è¾“å‡ºï¼šç›¸é‚»=1ï¼Œä¸ç›¸é‚»=0  

# CLIPçš„æ€æƒ³ï¼ˆ2021ï¼‰  
ä»»åŠ¡ï¼šåˆ¤æ–­å›¾åƒå’Œæ–‡æœ¬æ˜¯å¦åŒ¹é…  
è¾“å…¥ï¼š(image, text)  
è¾“å‡ºï¼šåŒ¹é…=1ï¼Œä¸åŒ¹é…=0  

# å¥å­åµŒå…¥çš„æ€æƒ³ï¼ˆSBERTï¼‰  
ä»»åŠ¡ï¼šåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸ä¼¼  
è¾“å…¥ï¼š(sentence1, sentence2)  
è¾“å‡ºï¼šç›¸ä¼¼=1ï¼Œä¸ç›¸ä¼¼=0  
```

æ ¸å¿ƒæ¨¡å¼ï¼š

1. æ„é€ æ­£ä¾‹å¯¹ï¼ˆç›¸å…³çš„ï¼‰å’Œè´Ÿä¾‹å¯¹ï¼ˆä¸ç›¸å…³çš„ï¼‰
2. è®­ç»ƒæ¨¡å‹æ‹‰è¿‘æ­£ä¾‹ã€æ¨è¿œè´Ÿä¾‹
3. å‰¯äº§å“ï¼šå­¦åˆ°æœ‰æ„ä¹‰çš„åµŒå…¥å‘é‡

---

## å®æˆ˜æ¡ˆä¾‹ï¼šä»è¯åµŒå…¥åˆ°æ¨èç³»ç»Ÿ

### ğŸµ æ­Œæ›²æ¨èç³»ç»Ÿï¼ˆé¢„å‘Š2.5èŠ‚ï¼‰

```python
# æ ¸å¿ƒæ€æƒ³ï¼šç”¨word2vecçš„æ–¹æ³•è®­ç»ƒæ­Œæ›²åµŒå…¥  
# æŠŠæ’­æ”¾åˆ—è¡¨å½“ä½œ"å¥å­"ï¼Œæ­Œæ›²å½“ä½œ"è¯"  

# ç¤ºä¾‹æ’­æ”¾åˆ—è¡¨ï¼ˆç”¨æˆ·çš„å¬æ­Œè®°å½•ï¼‰  
playlists = [  
    ["Shape of You", "Perfect", "Thinking Out Loud"],  # Ed Sheerané£æ ¼  
    ["Bohemian Rhapsody", "We Will Rock You", "Radio Ga Ga"],  # Queen  
    ["Perfect", "Photograph", "Lego House"],  # æŠ’æƒ…æ­Œ  
]  

# è®­ç»ƒword2vecï¼ˆä»£ç å®Œå…¨ä¸€æ ·ï¼ï¼‰  
from gensim.models import Word2Vec  
model = Word2Vec(sentences=playlists, vector_size=100, window=3)  

# æ¨èåŠŸèƒ½  
similar_songs = model.wv.most_similar("Perfect", topn=5)  
# è¾“å‡ºï¼šThinking Out Loud, Photograph, Shape of You...  
```

ä¸ºä»€ä¹ˆè¿™èƒ½å·¥ä½œï¼Ÿ

- ç»å¸¸åœ¨åŒä¸€æ’­æ”¾åˆ—è¡¨å‡ºç°çš„æ­Œ â†’ é£æ ¼ç›¸ä¼¼
- æ»‘åŠ¨çª—å£ = ç”¨æˆ·å¬æ­Œçš„è¿ç»­æ€§
- åµŒå…¥å‘é‡ = æ­Œæ›²çš„"é£æ ¼ç‰¹å¾"

---

## ğŸš€ å·¥ç¨‹å®è·µå»ºè®®

### ä½•æ—¶ä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥ï¼Ÿ

âœ… é€‚åˆåœºæ™¯ï¼š

```python
# 1. å¿«é€ŸåŸå‹éªŒè¯  
# 2. è®¡ç®—èµ„æºæœ‰é™  
# 3. æ•°æ®é‡è¾ƒå°ï¼ˆ<10kæ ·æœ¬ï¼‰  
# 4. ä»»åŠ¡å¯¹ä¸Šä¸‹æ–‡è¦æ±‚ä¸é«˜ï¼ˆå¦‚å…³é”®è¯åŒ¹é…ï¼‰  

from gensim.downloader import api  
model = api.load("glove-wiki-gigaword-50")  # å³æ’å³ç”¨  
```

âŒ ä¸é€‚åˆåœºæ™¯ï¼š

```python
# 1. éœ€è¦ä¸Šä¸‹æ–‡ç†è§£ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰  
# 2. é¢†åŸŸç‰¹å®šè¯æ±‡ï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰  
# 3. å¤šä¹‰è¯å¤„ç†ï¼ˆbankçš„ä¸åŒå«ä¹‰ï¼‰  
# 4. 2013å¹´åçš„æ–°è¯ï¼ˆChatGPTã€Bitcoinç­‰ï¼‰  

# æ­¤æ—¶åº”è¯¥ç”¨ï¼šBERTã€RoBERTaç­‰ä¸Šä¸‹æ–‡åµŒå…¥  
```

### æ€§èƒ½å¯¹æ¯”

|æ–¹æ³•|è®­ç»ƒæˆæœ¬|æ¨ç†é€Ÿåº¦|å‡†ç¡®åº¦|é€‚ç”¨åœºæ™¯|
|---|---|---|---|---|
|word2vec|â­ ä½|ğŸš€ğŸš€ğŸš€ æå¿«|â­â­ ä¸­ç­‰|å…³é”®è¯æœç´¢ã€å¿«é€ŸåŸå‹|
|BERT|â­â­â­ é«˜|ğŸš€ è¾ƒæ…¢|â­â­â­â­ é«˜|è¯­ä¹‰ç†è§£ã€åˆ†ç±»ä»»åŠ¡|
|sentence-transformers|â­â­ ä¸­|ğŸš€ğŸš€ å¿«|â­â­â­â­ é«˜|è¯­ä¹‰æœç´¢ã€RAGç³»ç»Ÿ|

---

## ä¸€å¥è¯æ€»ç»“

`è¯åµŒå…¥`çš„æ€æƒ³è¶…è¶Šäº†NLPé¢†åŸŸâ€”â€”`ä»»ä½•å¯ä»¥è¡¨ç¤ºæˆ"åºåˆ—"çš„æ•°æ®`ï¼ˆæ’­æ”¾åˆ—è¡¨ã€ç”¨æˆ·è¡Œä¸ºã€å•†å“æµè§ˆè®°å½•ï¼‰éƒ½å¯ä»¥ç”¨word2vecçš„æ–¹æ³•è®­ç»ƒåµŒå…¥ï¼Œä»è€Œå®ç°æ¨èã€æœç´¢ã€èšç±»ç­‰åŠŸèƒ½ã€‚

å¼€å‘è€…è®°ä½ä¸‰ç‚¹ï¼š
1. ğŸ¯ é¢„è®­ç»ƒåµŒå…¥ï¼šä¸‹è½½å³ç”¨ï¼Œé€‚åˆå¿«é€ŸéªŒè¯æƒ³æ³•
2. ğŸ§  word2vecåŸç†ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆæ­£è´Ÿä¾‹ï¼‰æ˜¯ç°ä»£AIçš„åŸºçŸ³
3. ğŸ”„ é€šç”¨æ¨¡å¼ï¼šè¯â†’æ­Œæ›²â†’å•†å“â†’ç”¨æˆ·ï¼ŒåŒä¸€å¥—ç®—æ³•è§£å†³ä¸åŒé—®é¢˜
