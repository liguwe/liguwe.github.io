
# æ–‡æœ¬åµŒå…¥ï¼ˆç”¨äºå¥å­å’Œæ•´ç¯‡æ–‡æ¡£ï¼‰ï¼šä»£ç è¯¦è§£ç¯‡

`#2025/12/28` `#ai` 


## ç›®å½•
<!-- toc -->
 ## æ‰¿æ¥å‰æ–‡ï¼šä»è¯å…ƒåˆ°æ–‡æœ¬çš„è·¨è¶Š 

å‰æ–‡ï¼š
- æˆ‘ä»¬å­¦ä¼šäº†"å¦‚ä½•æŠŠ`å•ä¸ªè¯å…ƒ`å˜æˆå‘é‡"  
- ç°åœ¨è¦è§£å†³"å¦‚ä½•æŠŠ`æ•´å¥è¯ã€æ•´ç¯‡æ–‡æ¡£`å˜æˆä¸€ä¸ªå‘é‡"

```python
# é—®é¢˜æ¼”è¿›  
è¯å…ƒ "Python" â†’ [0.1, 0.5, 0.8, ...]  # 2.2èŠ‚ï¼šè¯å…ƒåµŒå…¥ âœ…  

å¥å­ "I love Python" â†’ [0.3, 0.7, 0.2, ...]  # 2.3èŠ‚ï¼šæ–‡æœ¬åµŒå…¥ âœ…  
æ–‡æ¡£ "æ•´ç¯‡åšå®¢æ–‡ç« ..." â†’ [0.5, 0.1, 0.9, ...]  # 2.3èŠ‚ï¼šæ–‡æœ¬åµŒå…¥ âœ…  
```

---

## æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦æ–‡æœ¬åµŒå…¥ï¼Ÿ

### ğŸ¯ å®é™…åº”ç”¨åœºæ™¯é©±åŠ¨

```python hl:1,10,14
# åœºæ™¯1ï¼šæœç´¢å¼•æ“  
ç”¨æˆ·æŸ¥è¯¢ = "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ"  
æ–‡æ¡£åº“ = [  
    "Pythonå…¥é—¨æ•™ç¨‹...",  
    "JavaScriptæ¡†æ¶ä»‹ç»...",   
    "Pythonæ•°æ®åˆ†æå®æˆ˜..."  
]  
# éœ€è¦ï¼šæŠŠæŸ¥è¯¢å’Œæ¯ä¸ªæ–‡æ¡£éƒ½å˜æˆå‘é‡ï¼Œç„¶åè®¡ç®—ç›¸ä¼¼åº¦  

# åœºæ™¯2ï¼šæ–‡æ¡£åˆ†ç±»  
email = "å°Šæ•¬çš„å®¢æˆ·ï¼Œæ‚¨çš„è®¢å•å·²å‘è´§..."  
# éœ€è¦ï¼šæŠŠæ•´å°é‚®ä»¶å˜æˆä¸€ä¸ªå‘é‡ï¼Œç„¶ååˆ¤æ–­æ˜¯"å•†åŠ¡é‚®ä»¶"è¿˜æ˜¯"åƒåœ¾é‚®ä»¶"  

# åœºæ™¯3ï¼šè¯­ä¹‰å»é‡  
è¯„è®º1 = "è¿™ä¸ªäº§å“å¾ˆæ£’ï¼"  
è¯„è®º2 = "è¿™ä¸œè¥¿çœŸä¸é”™ï¼"  
è¯„è®º3 = "è¿™ä¸ªæ‰‹æœºæ€ä¹ˆç”¨ï¼Ÿ"  
# éœ€è¦ï¼šæŠŠæ¯æ¡è¯„è®ºå˜æˆå‘é‡ï¼Œæ‰¾å‡ºæ„æ€ç›¸è¿‘çš„ï¼ˆè¯„è®º1å’Œè¯„è®º2ï¼‰  
```

å¦‚æœ`åªç”¨è¯å…ƒåµŒå…¥`ï¼š

```python
# âŒ é—®é¢˜ï¼šä¸€å¥è¯æœ‰å¤šä¸ªè¯å…ƒ  
"I love Python" â†’ [  
    [0.2, 0.8, ...],  # "I"  
    [0.5, 0.3, ...],  # "love"  
    [0.1, 0.7, ...]   # "Python"  
]  
# è¿™æ˜¯3ä¸ªå‘é‡ï¼Œæ— æ³•ç›´æ¥ç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼  
```

æ–‡æœ¬åµŒå…¥è§£å†³æ–¹æ¡ˆï¼š â†’ `æ•´å¥è¯å˜æˆä¸€ä¸ªå‘é‡`   

```python
# âœ… ç›®æ ‡ï¼šæ•´å¥è¯å˜æˆä¸€ä¸ªå‘é‡  
"I love Python" â†’ [0.4, 0.6, 0.5, ...]  # ä¸€ä¸ª768ç»´å‘é‡  
```

## å®ç°æ–¹æ³•1ï¼šæœ´ç´ å¹³å‡ï¼ˆä¸æ¨èï¼‰

### ğŸ“ ç®€å•ç²—æš´çš„åšæ³•

```python
# æ­¥éª¤1ï¼šè·å–æ¯ä¸ªè¯å…ƒçš„åµŒå…¥  
tokens = ["I", "love", "Python"]  
token_embeddings = [  
    [0.2, 0.8, 0.1],  # "I"  
    [0.5, 0.3, 0.9],  # "love"  
    [0.1, 0.7, 0.2]   # "Python"  
]  

# æ­¥éª¤2ï¼šç›´æ¥æ±‚å¹³å‡  
import numpy as np  
sentence_embedding = np.mean(token_embeddings, axis=0)  
# ç»“æœï¼š[0.27, 0.6, 0.4]  
```

é—®é¢˜ï¼š

- âŒ ä¿¡æ¯ä¸¢å¤±ä¸¥é‡ï¼ˆ"`love`"å’Œ"`hate`"å¹³å‡åå¯èƒ½å·®ä¸å¤šï¼‰
- âŒ å¿½ç•¥`è¯åº`ï¼ˆ"`Python loves me`" å’Œ "`I love Python`" å®Œå…¨ä¸€æ ·ï¼‰
- âŒ æ— æ³•æ•æ‰å¤æ‚è¯­ä¹‰

---

## å®ç°æ–¹æ³•2ï¼šä¸“ç”¨æ¨¡å‹ï¼ˆæ¨èï¼‰

### ğŸš€ Sentence Transformers ç™»åœº

```python hl:10
from sentence_transformers import SentenceTransformer  

# åŠ è½½ä¸“é—¨è®­ç»ƒçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹  
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  

# ä¸€è¡Œä»£ç æå®šï¼  
text = "Best movie ever!"  
vector = model.encode(text)  

# æ‰“å°å‡ºä¸€ä¸ª 768 ç»´å‘é‡
print(vector.shape)  # (768,) â€” ä¸€ä¸ª768ç»´å‘é‡  
```

ä¸`è¯å…ƒåµŒå…¥`çš„å¯¹æ¯”ï¼š

| ç»´åº¦  | è¯å…ƒåµŒå…¥                         | æ–‡æœ¬åµŒå…¥                                |
| --- | ---------------------------- | ----------------------------------- |
| è¾“å…¥  | å•ä¸ªè¯å…ƒ                         | æ•´å¥è¯/æ–‡æ¡£                              |
| è¾“å‡º  | å¤šä¸ªå‘é‡ï¼ˆæ¯ä¸ªè¯å…ƒä¸€ä¸ªï¼‰                 | ä¸€ä¸ªå‘é‡                                |
| ç”¨é€”  | LLMå†…éƒ¨å¤„ç†                      | åº”ç”¨å±‚ä»»åŠ¡                               |
| ä¾‹å­  | "Python" â†’ `[0.1, 0.5, ...]` | "I love Python" â†’ `[0.3, 0.7, ...]` |

---

## ä»£ç å®æˆ˜ï¼šæ–‡æœ¬åµŒå…¥çš„å®Œæ•´æµç¨‹

### ğŸ”§ Step 1ï¼šå®‰è£…ä¾èµ–

```bash
pip install sentence-transformers  
```

### ğŸ”§ Step 2ï¼šç”ŸæˆåµŒå…¥ 

```python hl:15
from sentence_transformers import SentenceTransformer  

# åŠ è½½æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰  
model = SentenceTransformer("all-MiniLM-L6-v2")  # è½»é‡çº§æ¨¡å‹  

# å‡†å¤‡æ–‡æœ¬  
sentences = [  
    "Python is a programming language",  
    "I love coding in Python",   
    "Pizza is delicious",  
    "Programming is fun"  
]  

# ç”ŸæˆåµŒå…¥
# ï¼ˆæ‰¹é‡å¤„ç†ï¼‰  
embeddings = model.encode(sentences)  

print(embeddings.shape)  
# è¾“å‡ºï¼š(4, 384)   
# å«ä¹‰ï¼š4ä¸ªå¥å­ï¼Œæ¯ä¸ªå¥å­384ç»´å‘é‡  
```

### ğŸ”§ Step 3ï¼šè®¡ç®—ç›¸ä¼¼åº¦ â†’ å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦

```python
from sklearn.metrics.pairwise import cosine_similarity  

# è®¡ç®—æ‰€æœ‰å¥å­`ä¸¤ä¸¤ä¹‹é—´`çš„ç›¸ä¼¼åº¦  
similarities = cosine_similarity(embeddings)  

print(similarities)  
# è¾“å‡ºç¤ºä¾‹ï¼ˆç›¸ä¼¼åº¦çŸ©é˜µï¼‰ï¼š  
# [
#  [1.00, 0.75, 0.12, 0.34],   # å¥å­1 vs æ‰€æœ‰å¥å­  
#  [0.75, 1.00, 0.08, 0.41],   # å¥å­2 vs æ‰€æœ‰å¥å­  
#  [0.12, 0.08, 1.00, 0.15],   # å¥å­3 vs æ‰€æœ‰å¥å­  
#  [0.34, 0.41, 0.15, 1.00]]   # å¥å­4 vs æ‰€æœ‰å¥å­  

# è§‚å¯Ÿï¼š  
# å¥å­1å’Œå¥å­2ç›¸ä¼¼åº¦0.75ï¼ˆéƒ½å…³äºPythonç¼–ç¨‹ï¼‰âœ…  
# å¥å­3å’Œå…¶ä»–å¥å­ç›¸ä¼¼åº¦ä½ï¼ˆè®²çš„æ˜¯æŠ«è¨ï¼‰âœ…  
```

---

## å®é™…åº”ç”¨åœºæ™¯

### ğŸ¯ åº”ç”¨1ï¼šè¯­ä¹‰æœç´¢å¼•æ“

```python
# æ–‡æ¡£åº“  
documents = [  
    "Python is great for data science",  
    "JavaScript runs in the browser",  
    "Machine learning with Python",  
    "React is a JavaScript library"  
]  

# ç”Ÿæˆæ–‡æ¡£åµŒå…¥ï¼ˆæå‰è®¡ç®—ï¼Œå­˜å…¥æ•°æ®åº“ï¼‰  
doc_embeddings = model.encode(documents)  

# ç”¨æˆ·æœç´¢  
query = "How to do data analysis?"  
query_embedding = model.encode(query)  

# è®¡ç®—ç›¸ä¼¼åº¦  
from sklearn.metrics.pairwise import cosine_similarity  
scores = cosine_similarity([query_embedding], doc_embeddings)[0]  

# æ’åºè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£  
import numpy as np  
top_index = np.argmax(scores)  
print(f"æœ€ç›¸å…³æ–‡æ¡£ï¼š{documents[top_index]}")  
# è¾“å‡ºï¼šPython is great for data science  
```

### ğŸ¯ åº”ç”¨2ï¼šæ™ºèƒ½å»é‡

```python hl:12
# ç”¨æˆ·è¯„è®º  
reviews = [  
    "This product is amazing!",  
    "This item is fantastic!",      # å’Œç¬¬1æ¡è¯­ä¹‰é‡å¤  
    "How do I return this?",  
    "Where is my order?"  
]  

# ç”ŸæˆåµŒå…¥  
embeddings = model.encode(reviews)  

# è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä¾‹å¦‚0.8ï¼‰  
threshold = 0.8  
duplicates = []  

for i in range(len(reviews)):  
    for j in range(i + 1, len(reviews)):  
        sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]  
        if sim > threshold:  
            duplicates.append((i, j, sim))  
            print(f"é‡å¤ï¼š\n  [{i}] {reviews[i]}\n  [{j}] {reviews[j]}\n  ç›¸ä¼¼åº¦ï¼š{sim:f}\n")  

# è¾“å‡ºï¼š  
# é‡å¤ï¼š  
#   [0] This product is amazing!  
#   [1] This item is fantastic!  
#   ç›¸ä¼¼åº¦ï¼š0.87  
```

### ğŸ¯ åº”ç”¨3ï¼šæ–‡æ¡£èšç±»

```python hl:21
from sklearn.cluster import KMeans  

# å¤§é‡æ–‡æ¡£  
documents = [  
    "Python programming tutorial",  
    "Learn JavaScript basics",  
    "Python for beginners",  
    "JavaScript advanced course",  
    "Delicious pizza recipe",  
    "How to make pasta"  
    # ... æ›´å¤šæ–‡æ¡£  
]  

# ç”ŸæˆåµŒå…¥  
embeddings = model.encode(documents)  

# èšç±»ï¼ˆåˆ†æˆ3ç±»ï¼‰  
kmeans = KMeans(n_clusters=3, random_state=42)  
labels = kmeans.fit_predict(embeddings)  

# æŸ¥çœ‹ç»“æœ  
for i, label in enumerate(labels):  
    print(f"æ–‡æ¡£ {i} å±äºç°‡ {label}: {documents[i]}")  

# è¾“å‡ºç¤ºä¾‹ï¼š  
# æ–‡æ¡£ 0 å±äºç°‡ 0: Python programming tutorial  
# æ–‡æ¡£ 1 å±äºç°‡ 1: Learn JavaScript basics  
# æ–‡æ¡£ 2 å±äºç°‡ 0: Python for beginners  
# æ–‡æ¡£ 3 å±äºç°‡ 1: JavaScript advanced course  
# æ–‡æ¡£ 4 å±äºç°‡ 2: Delicious pizza recipe  
# æ–‡æ¡£ 5 å±äºç°‡ 2: How to make pasta  
```

## æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„å†…éƒ¨åŸç†

### ğŸ§  å®ƒæ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼Ÿ

```python hl:5
# è®­ç»ƒæ•°æ®ï¼šå¥å­å¯¹ + ç›¸ä¼¼åº¦æ ‡ç­¾  
è®­ç»ƒæ ·æœ¬ = [  
    ("Python is great", "I love Python", ç›¸ä¼¼åº¦=0.9),  
    ("Python is great", "Pizza is yummy", ç›¸ä¼¼åº¦=0.1),  
    # ... æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬  
]  

# è®­ç»ƒç›®æ ‡ï¼šè®©ç›¸ä¼¼å¥å­çš„å‘é‡é è¿‘ï¼Œä¸ç›¸ä¼¼çš„è¿œç¦»  
```

è®­ç»ƒæµç¨‹ï¼š

1. ç”¨BERT/RoBERTa ç­‰æ¨¡å‹ä½œä¸ºåŸºç¡€
2. æ·»åŠ ç‰¹æ®Šçš„`æ± åŒ–å±‚`ï¼ˆæŠŠ`å¤šä¸ªè¯å…ƒå‘é‡`å˜æˆä¸€ä¸ªï¼‰
3. ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼ˆContrastive Learningï¼‰
4. ä¼˜åŒ–ç›®æ ‡ï¼š
	- ç›¸ä¼¼å¥å­`ä½™å¼¦ç›¸ä¼¼åº¦é«˜`ï¼Œ`ä¸ç›¸ä¼¼`çš„ä½

## å…³é”®æŠ€æœ¯ç»†èŠ‚

### ğŸ“ å¸¸è§æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹                | ç»´åº¦   | é€Ÿåº¦     | ç²¾åº¦    | é€‚ç”¨åœºæ™¯      |
| ----------------- | ---- | ------ | ----- | --------- |
| all-MiniLM-L6-v2  | 384  | ğŸš€ğŸš€ğŸš€ | â­â­â­   | å¿«é€ŸåŸå‹ã€å®æ—¶æœç´¢ |
| all-mpnet-base-v2 | 768  | ğŸš€ğŸš€   | â­â­â­â­  | é€šç”¨ä»»åŠ¡ï¼Œå¹³è¡¡æ€§èƒ½ |
| gte-large         | 1024 | ğŸš€     | â­â­â­â­â­ | é«˜ç²¾åº¦éœ€æ±‚     |

### âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# æŠ€å·§1ï¼šæ‰¹é‡å¤„ç†  
# âŒ æ…¢ï¼šé€ä¸ªå¤„ç†  
for text in texts:  
    embedding = model.encode(text)  

# âœ… å¿«ï¼šæ‰¹é‡å¤„ç†ï¼ˆå¿«10å€+ï¼‰  
embeddings = model.encode(texts, batch_size=32)  

# æŠ€å·§2ï¼šå½’ä¸€åŒ–å‘é‡ï¼ˆåŠ é€Ÿç›¸ä¼¼åº¦è®¡ç®—ï¼‰  
from sklearn.preprocessing import normalize  
embeddings_normalized = normalize(embeddings)  
# ç„¶åå¯ä»¥ç”¨ç‚¹ç§¯ä»£æ›¿ä½™å¼¦ç›¸ä¼¼åº¦  

# æŠ€å·§3ï¼šé™ç»´ï¼ˆç”¨äºå¯è§†åŒ–æˆ–å‡å°‘å­˜å‚¨ï¼‰  
from sklearn.decomposition import PCA  
pca = PCA(n_components=128)  
embeddings_compressed = pca.fit_transform(embeddings)  
```

---

## æ–‡æœ¬åµŒå…¥ vs è¯å…ƒåµŒå…¥

### ğŸ†š æ ¸å¿ƒåŒºåˆ«

```python
# è¯å…ƒåµŒå…¥ï¼ˆToken Embeddingï¼‰  
# - LLMå†…éƒ¨ä½¿ç”¨  
# - ä¸€å¥è¯ â†’ å¤šä¸ªå‘é‡  
text = "I love Python"  
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")  
model = AutoModel.from_pretrained("bert-base-uncased")  
outputs = model(tokenizer(text, return_tensors="pt"))[0]  
print(outputs.shape)  # torch.Size([1, 5, 768])  
#                              â†‘  â†‘  â†‘  
#                           batch 5ä¸ªè¯å…ƒ æ¯ä¸ª768ç»´  

# æ–‡æœ¬åµŒå…¥ï¼ˆSentence Embeddingï¼‰  
# - åº”ç”¨å±‚ä½¿ç”¨  
# - ä¸€å¥è¯ â†’ ä¸€ä¸ªå‘é‡  
from sentence_transformers import SentenceTransformer  
model = SentenceTransformer("all-MiniLM-L6-v2")  
embedding = model.encode("I love Python")  
print(embedding.shape)  # (384,) â€” ä¸€ä¸ª384ç»´å‘é‡  
```

### ğŸ“Š ä½¿ç”¨åœºæ™¯å¯¹æ¯”

| åœºæ™¯      | ä½¿ç”¨è¯å…ƒåµŒå…¥ | ä½¿ç”¨æ–‡æœ¬åµŒå…¥ |
| ------- | ------ | ------ |
| LLMæ–‡æœ¬ç”Ÿæˆ | âœ…      | âŒ      |
| è¯­ä¹‰æœç´¢    | âŒ      | âœ…      |
| æ–‡æ¡£åˆ†ç±»    | âŒ      | âœ…      |
| èšç±»/å»é‡   | âŒ      | âœ…      |
| é—®ç­”åŒ¹é…    | âŒ      | âœ…      |
| å‘½åå®ä½“è¯†åˆ«  | âœ…      | âŒ      |

---

## å¸¸è§è¯¯åŒº

### âŒ è¯¯åŒº1ï¼šä»¥ä¸º`æ–‡æœ¬åµŒå…¥`å¯ä»¥ç”Ÿæˆæ–‡æœ¬

```python
# âŒ é”™è¯¯ç†è§£  
embedding = model.encode("Hello")  
new_text = model.decode(embedding)  # æ²¡æœ‰è¿™ä¸ªæ–¹æ³•ï¼  

# âœ… æ­£ç¡®ç†è§£  
# æ–‡æœ¬åµŒå…¥æ˜¯å•å‘çš„ï¼šæ–‡æœ¬ â†’ å‘é‡  
# ä¸èƒ½åå‘ï¼šå‘é‡ â†’ æ–‡æœ¬  
```

### âŒ è¯¯åŒº2ï¼šä»¥ä¸ºè¶Šé•¿çš„æ–‡æœ¬åµŒå…¥ç»´åº¦è¶Šé«˜

```python
# âŒ é”™è¯¯è®¤çŸ¥  
short_text = "Hi"  
long_text = "This is a very long document with many words..."  

embedding1 = model.encode(short_text)  
embedding2 = model.encode(long_text)  

print(embedding1.shape)  # (384,)  
print(embedding2.shape)  # (384,) â€” ç»´åº¦ç›¸åŒï¼  

# âœ… æ­£ç¡®ç†è§£  
# æ— è®ºæ–‡æœ¬å¤šé•¿ï¼Œè¾“å‡ºå‘é‡ç»´åº¦å›ºå®š  
```

### âŒ è¯¯åŒº3ï¼šä»¥ä¸ºä¸åŒæ¨¡å‹çš„åµŒå…¥å¯ä»¥æ··ç”¨

```python
# âŒ é”™è¯¯  
model1 = SentenceTransformer("all-MiniLM-L6-v2")  # 384ç»´  
model2 = SentenceTransformer("all-mpnet-base-v2")  # 768ç»´  

emb1 = model1.encode("Python")  
emb2 = model2.encode("Java")  

similarity = cosine_similarity([emb1], [emb2])  # ğŸ’¥ ç»´åº¦ä¸åŒ¹é…ï¼  

# âœ… æ­£ç¡®  
# å¿…é¡»ç”¨åŒä¸€ä¸ªæ¨¡å‹ç”Ÿæˆæ‰€æœ‰åµŒå…¥  
```

---

## ä¸€å¥è¯æ€»ç»“

æ–‡æœ¬åµŒå…¥ = `æŠŠæ•´å¥è¯/æ–‡æ¡£å‹ç¼©æˆä¸€ä¸ªå‘é‡ï¼Œè®©ç”µè„‘èƒ½"æ¯”è¾ƒ"æ–‡æœ¬çš„æ„æ€`

æ ¸å¿ƒä»·å€¼ï¼š
- ğŸ¯ è¯å…ƒåµŒå…¥ï¼š
	- LLMçš„ "å†…éƒ¨è¯­è¨€"ï¼ˆå¤šä¸ªå‘é‡ï¼‰
- ğŸš€ æ–‡æœ¬åµŒå…¥ï¼š
	- åº”ç”¨çš„ "æ¥å£æ ‡å‡†"ï¼ˆä¸€ä¸ªå‘é‡ï¼‰

å¼€å‘è€…è®°ä½ä¸‰ç‚¹ï¼š

1. ğŸ“¦ ä¸€ä¸ªæ–‡æœ¬ `â†’` ä¸€ä¸ªå‘é‡ï¼ˆæ— è®ºæ–‡æœ¬å¤šé•¿ï¼Œç»´åº¦å›ºå®šï¼‰
2. ğŸ” ä¸»è¦ç”¨äºæœç´¢ã€åˆ†ç±»ã€èšç±»ç­‰ä»»åŠ¡ï¼ˆä¸æ˜¯ç”¨äºç”Ÿæˆæ–‡æœ¬ï¼‰
3. âš¡ ç”¨ä¸“ç”¨æ¨¡å‹ï¼ˆsentence-transformersï¼‰æ•ˆæœè¿œè¶…ç®€å•å¹³å‡

## ğŸ è¯¾åå®æˆ˜

è¯•è¯•è¿™ä¸ªå®Œæ•´æ¡ˆä¾‹ï¼Œæ­å»ºä¸€ä¸ªè¿·ä½ æœç´¢å¼•æ“ï¼š

```python
from sentence_transformers import SentenceTransformer  
from sklearn.metrics.pairwise import cosine_similarity  

# 1. åŠ è½½æ¨¡å‹  
model = SentenceTransformer("all-MiniLM-L6-v2")  

# 2. æ„å»ºçŸ¥è¯†åº“  
knowledge_base = [  
    "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ",  
    "JavaScriptä¸»è¦ç”¨äºç½‘é¡µå¼€å‘ï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ",  
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ",  
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è§£å†³å¤æ‚é—®é¢˜"  
]  

# 3. ç”Ÿæˆæ–‡æ¡£åµŒå…¥  
doc_embeddings = model.encode(knowledge_base)  

# 4. ç”¨æˆ·æé—®  
query = "å¦‚ä½•ç”¨Pythonåšæ•°æ®åˆ†æï¼Ÿ"  
query_embedding = model.encode(query)  

# 5. æœç´¢æœ€ç›¸å…³ç­”æ¡ˆ  
scores = cosine_similarity([query_embedding], doc_embeddings)[0]  
best_match_idx = scores.argmax()  

print(f"é—®é¢˜ï¼š{query}")  
print(f"ç­”æ¡ˆï¼š{knowledge_base[best_match_idx]}")  
print(f"ç›¸å…³åº¦ï¼š{scores[best_match_idx]:f}")  
```

è¿è¡Œåä½ ä¼šå‘ç°ï¼Œå³ä½¿æŸ¥è¯¢å’Œæ–‡æ¡£ç”¨è¯ä¸å®Œå…¨ä¸€æ ·ï¼Œä¾ç„¶èƒ½æ‰¾åˆ°è¯­ä¹‰æœ€ç›¸å…³çš„ç­”æ¡ˆâ€”â€”è¿™å°±æ˜¯æ–‡æœ¬åµŒå…¥çš„é­”åŠ›ï¼âœ¨
