
# 语义搜索与RAG技术全景

`#2025/12/31` `#ai` 

> 前面 我们学会了如何用链、记忆和智能体来增强LLM的能力。  
> 现在 我们要解决一个更实际的问题：**如何让LLM获取并使用外部知识？**

---


## 目录
<!-- toc -->
 ## 一、为什么需要语义搜索和RAG？ 

### 问题1：传统搜索的局限

想象一下传统的关键词搜索：

```
用户搜索："如何做好吃的意大利面"  
传统搜索：找包含"意大利面"这个词的文章  
问题：可能错过"pasta料理秘诀"这类相关内容  
```

**局限性**：

- ❌ 只能匹配完全相同的词
- ❌ 无法理解语义相似性
- ❌ 搜到的结果可能不相关

---

### 问题2：LLM的"幻觉"问题

```python
用户问："2023年世界杯冠军是谁？"  
LLM回答："我认为是巴西队。" # ❌ 错误！但说得很自信  
```

**LLM的问题**：

- 训练数据有`时效性`（可能是2021年的数据）
- 会"`编造`"听起来合理但错误的答案
- 无法访问实时信息或专有数据

### 解决方案预览

| 技术       | 解决什么问题   | 核心思路          |
| -------- | -------- | ------------- |
| **语义搜索** | 理解搜索意图   | 基于`语义`而非关键词   |
| **RAG**  | LLM的幻觉问题 | 先搜索真实文档，再生成答案 |

## 二、语义搜索：让搜索"懂你"

### 什么是语义搜索？

**核心定义**：通过理解**语义含义**而非简单的**关键词匹配**来实现精准检索。

### 形象对比

```
传统搜索：  
查询："银行在哪里？"  
匹配：文档中必须有"银行"这个词  

语义搜索：  
查询："银行在哪里？"  
理解：用户想找金融机构的位置  
匹配：可以找到"储蓄所地址""信用社位置"等相关内容  
```

---

### 为什么语义搜索这么重要？

**历史里程碑**：

2018年，谷歌发布BERT论文后几个月：

- 谷歌：将BERT整合到搜索引擎，称为"**搜索史上最具突破性的进步之一**"
- 微软：在必应搜索中使用大型Transformer模型，带来"**过去一年中最显著的体验提升**"

**这证明了什么？**

- ✅ 语言模型有强大的实用价值
- ✅ 可以显著提升成熟系统的性能
- ✅ 全球数十亿用户都能受益

---

## 三、语义搜索的三大核心技术

文档将主流技术分为三大类，我们逐一理解：

---

### 技术1：稠密检索（Dense Retrieval）

#### 核心思路

**把文本转换成`数字向量`，通过`向量相似度`找到最相关的文档**。

#### 工作流程图解

```
用户查询："科学有多精确？"  
    ↓  
转换成向量：[0.2, 0.8, 0.3, ...]  
    ↓  
与文档库中所有向量比较距离  
    ↓  
找到最近的3个向量  
    ↓  
返回对应的文档  
```

![{%}|592](https://www.ituring.com.cn/figures/2025/HandsonLLM/170.jpg)

**图：`稠密检索`是语义搜索的第一大核心类型，通过文本嵌入的`相似度`实现精准的结果筛选**

```
搜索查询 → [嵌入向量]   

文档库：  
- 文档1 → [向量1]  
- 文档2 → [向量2]  ← 最相似！  
- 文档3 → [向量3]  

结果：  
1. 文档40  
2. 文档68  
3. 文档2  
```

---

#### 关键优势

**语义相似的文本在向量空间中会彼此靠近**：

```
向量空间示意图：  

    文本2 ●  
         
查询 ★      文本1 ●  

              文本3 ●（距离远）  
```

- 查询和文本2最近 → 最相关
- 查询和文本1次近 → 次相关
- 查询和文本3远 → 不相关

---

### 技术2：重排序（Reranking）

#### 为什么需要重排序？

**问题场景**：
- 第一次搜索可能返回`100`个结果
- 排序可能不够精准
- 需要进一步优化前3-5个结果

#### 工作流程

如图8-2所示：

```
查询："MacBook价格"  
    ↓  
初步搜索（如关键词搜索）  
    ↓  
候选结果：  
1. 文档40  
2. 文档68  
3. 文档2  
    ↓  
重排器深度分析  
    ↓  
优化后排序：  
4. 文档2  （之前排第3）← 更相关！  
5. 文档40 （之前排第1）  
6. 文档68 （之前排第2）  
```

![{%}|664](https://www.ituring.com.cn/figures/2025/HandsonLLM/171.jpg)

> **图：`重排器`是语义搜索的第二大核心类型，`重排器`能够接收搜索查询与初始结果集，并根据相关性进行重新排序，从而显著提升结果质量**

#### 关键特点

|维度|第一阶段检索|重排序|
|---|---|---|
|**速度**|快（处理海量文档）|慢（精细分析）|
|**准确性**|中等|高|
|**输入**|用户查询|查询+候选结果|
|**处理量**|成千上万个文档|10-100个候选结果|

---

### 技术3：RAG（检索增强生成）

#### 什么是RAG？

**RAG = Retrieval-Augmented Generation**

**核心公式**：

```
RAG = 搜索系统 + 生成式LLM  
```

#### 工作原理

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/172.jpg)

**图：RAG 系统能够针对用户的问题生成精准回答，并（在理想情况下）标注其参考的信息来源**

```hl:12
用户问题："MacBook Pro多少钱？"  
    ↓  
[步骤1：检索]  
搜索相关文档  
    ↓  
找到：  
- 搜索结果1："MacBook Pro价格$2,249"  
- 搜索结果2："M3芯片版本更贵"  
- 搜索结果3："官网有优惠"  
    ↓  
[步骤2：生成]  
把问题+搜索结果一起给LLM  
    ↓  
LLM输出：  
"根据搜索结果[1]，MacBook Pro目前价格是$2,249。  
官方渠道[3]可能有优惠活动。"  
```

#### RAG的强大之处

|功能|无RAG的LLM|有RAG的LLM|
|---|---|---|
|**信息时效性**|❌ 训练数据可能过时|✅ 实时检索最新信息|
|**事实准确性**|❌ 容易产生幻觉|✅ 基于真实文档回答|
|**引用来源**|❌ 无法提供|✅ 标注信息来源|
|**专有数据**|❌ 无法访问|✅ 可接入企业数据|



#### RAG的典型应用

**1. 生成式搜索引擎**

```
Perplexity、Microsoft Copilot、Google Gemini  
→ 不只是返回链接，而是生成完整答案  
```

**2. "与我的数据对话"**

```
企业场景：  
"根据我们的销售报告，上季度哪个产品卖得最好？"  
→ RAG检索内部报告 → LLM生成分析  
```

**3. 增强LLM的知识库**

```
书籍内容、技术文档、法律条款等  
→ 让LLM能准确引用和解释特定内容  
```

---

## 四、三大技术的对比总结

### 完整对比表

|技术|输入|输出|优势|典型应用|
|---|---|---|---|---|
|**稠密检索**|查询文本|相关文档列表|理解语义|语义搜索|
|**重排序**|查询+候选文档|优化后的排序|提升精准度|搜索优化|
|**RAG**|问题|带引用的答案|减少幻觉|问答系统|

---

### 技术之间的关系

**它们不是互斥的，而是可以组合使用！**

```
最佳实践流程：  

用户问题  
    ↓  
[稠密检索]找到Top 100候选文档  
    ↓  
[重排序]优化到Top 5最相关文档  
    ↓  
[RAG]把Top 5文档+问题输入LLM  
    ↓  
生成带引用的精准答案  
```

---

## 五、实际应用场景举例

### 场景1：企业知识库问答

```python
# 传统方案  
用户："公司的请假政策是什么？"  
员工手册：300页PDF  
员工：需要自己翻找 😓  

# RAG方案  
用户："公司的请假政策是什么？"  
系统：  
1. 搜索员工手册相关章节  
2. LLM总结并回答  
3. 附上原文页码  
员工：秒懂！😊  
```

---

### 场景2：客服机器人

```python
# 问题  
客户："我的订单怎么还没到？"  

# RAG流程  
1. 检索订单数据库 → 找到该客户订单号XX123  
2. 检索物流系统 → 当前在运输中  
3. LLM生成回答：  
   "您的订单XX123已发货，预计3天内送达。  
    当前位置：XX市配送中心。[来源：物流系统]"  
```

---

### 场景3：学术研究助手

```python
# 需求  
研究员："总结最近关于Transformer的论文"  

# RAG流程  
1. 搜索学术数据库 → 找到2023-2024年相关论文  
2. 提取关键信息  
3. LLM生成综述：  
   "近期Transformer研究主要聚焦三个方向：  
    1) 效率优化[论文1,2]  
    2) 多模态扩展[论文3,4]  
    3) 长文本处理[论文5,6]"  
```

---

## 六、技术演进时间线

### 历史发展脉络

```
2018年：BERT发布  
    ↓  
谷歌、微软将其整合到搜索引擎  
    ↓  
2020年：RAG技术提出  
    ↓  
2023年：生成式搜索大爆发  
（Perplexity、Copilot等）  
    ↓  
现在：RAG成为LLM最受瞩目的应用场景  
```

---

## 七、下一步

下面将深入解析这些技术的实现：

```
8.2节 → 语义搜索实践  
├─ 8.2.1 稠密检索详解  
├─ 8.2.2 重排序详解  
└─ 8.2.3 评估指标  

8.3节 → RAG系统构建  
├─ 8.3.1 从搜索到RAG  
├─ 8.3.2 使用API实现  
├─ 8.3.3 本地模型实现  
├─ 8.3.4 高级技巧  
└─ 8.3.5 效果评估  
```

---

## 八、核心要点总结

### 1. 三大技术记忆法

```
稠密检索：把文本变成向量，找最近的  
重排序：对初步结果进行精细优化  
RAG：搜索+生成，既准确又智能  
```

---

### 2. 为什么重要？

| 价值         | 说明           |
| ---------- | ------------ |
| **解决实际问题** | LLM的幻觉、时效性问题 |
| **商业价值**   | 谷歌、微软等巨头验证   |
| **应用广泛**   | 搜索、问答、知识管理   |

---

### 3. 关键认知

**这些技术不是孤立的**：
- 稠密检索是基础
- 重排序是优化
- RAG是应用

**它们共同构成现代智能搜索系统的完整解决方案！**

---

## 核心启示

**语义搜索和RAG的本质**：
- 不是让搜索更快
- 而是**让搜索更懂你**
- 不是让LLM更聪明
- 而是**让LLM有据可依**

就像给LLM装上了"外接大脑"——既能理解语义，又能检索事实，还能生成答案！

现在你已经了解了语义搜索和RAG的全景图，接下来让我们深入每个技术的实现细节！
