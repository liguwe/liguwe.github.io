
# 视频导读

> https://www.youtube.com/watch?v=341Rb8fJxY0&list=PLTKMiZHVd_2IIEsoJrWACkIxLRdfMlw11&index=2


- 00:00 是的，大家好，欢迎大家回到《从零开始构建大型语言模型》一书的补充编程视频系列。
- 00:10 现在我们要谈第二章，使用文本数据，也就是说我们要准备用来训练LLM的数据。
- 00:20 是的，直接说到章节本身，第二章，你可以看到从开头开始有很多章节。
- 00:29 真正的目标是为LLM准备数据集。
- 00:33 这里其实有一张很不错的图，展示了整个LLM构建过程中这一步。
- 00:41 这里构建大型语言模型意味着设置数据集、编写注意力机制、编写LLM架构等等。
- 00:49 这实际上是训练LLM的第一阶段，第二阶段是预训练，第三阶段我们会对LLM进行微调。
- 00:59 不过，为了让内容更集中，我们接下来要谈到如何准备数据集，是的，给你另一种视角。
- 01:11 基本上，我们将把文本分词化，然后将这些分词化的子部分转换为令牌ID，再编码成向量。
- 01:26 我知道，这个视频可能快一点。
- 01:29 我之前提到过，视频没有书本身那么详细，更多是作为编码示例的辅助资源。
- 01:37 不过，是的，给你一个整体情况。
- 01:39 所以目标实际上是准备原始文本数据，以便LLM能够处理这些数据。
- 01:47 主要是LLM处理文本，但你知道，我们首先必须把它转换成一种格式，可以被阅读，比如说，LLM能理解的形式，从这个意义上说，这里的向量就像数学向量一样。
- 02:04 如果我往上滚动一点，它其实也和向量是文本的数字表示的概念很相似。
- 02:13 这就是我们的目标，从字母和单词转化为更数字化的表示，然后在预训练时用来优化LLM的权重参数。
- 02:27 不过，我不想说得太远，关于预培训。
- 02:31 这里的重点其实是准备LLM的输入数据。
- 02:38 特别是在第二部分，分词化文本，重点主要集中在底部的这个子部分，我们会把输入文本拆解成单独的小块。
- 02:53 是的，这被称为代币化。
- 02:57 给你举个例子。
- 03:00 所以我们稍后会使用一个叫 tiktoken 的库。
- 03:05 我认为这里有一个很棒的虚拟可视化工具，叫做tiktokenizer。
- 03:16 而且它们确实有不同的大型语言模型。
- 03:19 比如，有一个GPT-2大型语言模型。
- 03:23 如果我们输入一些文本，比如说，世界好，类似这样的内容，这就是一个测试。
- 03:32 这里你可以看到它如何被拆解成单个代币。
- 03:37 你可以看到hello是token，逗号是token，world是token，dot是token，这些是tokenID。
- 03:44 我们稍后会讲到代币ID是如何创建的。
- 03:48 目前，重点是把它拆解成单独的部分。
- 03:52 我们不会实现这里用的完全相同的算法。
- 03:57 我有一些额外的资料，稍后会分享。
- 04:02 首先，我们想从整体角度讲述如何将文本拆解成单个块。
- 04:08 所以一种方式基本上就是用正则表达式。
- 04:13 但像我在书中做的那样，先按顺序下载数据集，然后在本章后面用数据集实现PyTorch的数据加载器。
- 04:29 所以在我深入讨论其他内容之前，先从一个数据集开始。
- 04:37 所以我们将使用一个库“os”，它其实是一个标准的Python库，还有“urllib.request”，它也是一个标准的Python库。
- 04:47 这通常是用来处理文件，这类是从互联网下载东西。
- 04:53 我们要用的数据集叫做“裁决”。
- 04:59 那我就去这里新建一个标签页。
- 05:04 而《裁决》则是伊迪丝·沃顿的一篇短篇小说。
- 05:13 所以这些数据作为公共领域数据存在。
- 05:18 我记得这里有个维基百科版本，其实挺不错的。
- 05:24 这是一个非常简短的故事，我选择这个作为数据集，是因为，是的，我们想保持简单，在预训练时不会花太长时间
- 05:33 LLM更多是出于教育目的，而且它也相当简单、易读，最重要的是，它是一个公共领域的数据集或书籍。
- 05:45 这意味着我们可以不用担心版权限制，至少在美国是这样。
- 05:52 所以在这种情况下，这是一个公开领域的开放数据集。
- 05:55 比如用这个方法训练LLM时是不用担心的。
- 06:01 好的，那我们下载这个数据集。
- 06:05 我们可以在这里复制粘贴这个，创建一个新文件，一个文本文件，插入这个，保存，依此类推。
- 06:16 我这里有个更快的方法，所以我就直接复制粘贴一下。
- 06:21 你也可以把它放在书的GitHub仓库里，所以这个链接是我提供该数据集的GitHub链接，以防将来这个数据集可能无法从维基百科或其他地方获取。
- 06:38 你可以在书的 GitHub 仓库“rasbt/LLMs-from-scratch”中找到一个版本，第一章，抱歉，第二章，第一个文件夹的主章代码，我包含了
- 06:55 “the-verdict”在这里，如果你想从GitHub复制原始文本，你必须点击这里的原始文本，然后你会得到原始文本，在这里，你可以看到，这和你在这个页面上看到的很相似。
- 07:15 所以如果我现在复制这个URL，插入这里，实际上就是你之前看到的那个URL，只是没有所有换行符。
- 07:28 当我执行这段代码时，它会下载这个数据集并保存到文件中，the-verdict.txt。
- 07:36 让我先刷新一下，然后这里刷新一下，是的，你可以看到它现在在这个文件夹里，也就是说它现在在我们电脑的本地。
- 07:45 下一步是用 Python 打开这个文件，所以现在它就放在我们的电脑上，下一步就是打开这个，我先复制一下，我们就这样
- 07:59 我只是读着，这里的“r”，然后在Windows上，我觉得必须明确说明我们用的是哪种编码，也就是“UTF-8”，标准编码。
- 08:11 是的，然后我们只需阅读这里的原始文本，再仔细检查一下，这样我们就能再次确认文本看起来没问题，这样我们就能看到Python解释器已经成功了
- 08:27 这里加载了文本，我们也可以再确认一下长度，所以如果你在运行代码，我建议你运行这个测试，看看大致相同的结果
- 08:39 这里有数字，这意味着你没有缺失任何字符，所以如果你正确加载数据集，应该有大约20,479个字符，现在让我们简单介绍一下这个概念
- 08:53 用正则表达式进行分词化，这就像是我们使用真实的，比如说，tiktoken库来做令写化之前的热身。所以我们将使用 Python
- 09:05 正则表达式，我们这里用一些更简单的文本来做热身。我们用我刚输入的文本，提供这里，然后输入结果，然后
- 09:21 正则表达式。我得说正则表达式是我的弱项，所以我其实不太擅长，但说实话，现在像ChatGPT这样的大型语言模型，正则表达式其实很强
- 09:33 其实很容易搞明白。所以这本质上就是一个在空白字符上拆分的正则表达式，顺便说一句，你其实不需要真正知道什么是正则表达式
- 09:44 是针对训练大型语言模型的更大背景，因为我们后来使用基于字节对编码的分词器。我还有一个从零开始的实现，书里没有，
- 09:55 不过这些内容也在补充材料里，我也许以后可以给你看。不过，为了简单起见，如果你还记得我之前提到的，我们的目标是
- 10:06 首先将文本拆分成单个标记，我们用这个正则表达式来做，然后打印结果，看看效果如何。所以，是的，我们
- 10:18 现在我们有单个单词，还有空白字符，等等。我们可能还想把标点符号分成独立的字符。
- 10:31 为此，我们需要做一个更复杂的正则表达式，就像我说的，我不太擅长正则表达式，所以让我复制粘贴
- 10:41 这里，所以这现在是一个稍微复杂的正则表达式。因此，这个版本还会将标点符号作为独立的标记，而之前它们是
- 10:56 词本身。好，这就是我们最简单的代币化方式。现在，正如你在这里看到的，如果我去这里，输出中实际上没有空白字符，所以有一点
- 11:14 比如，我们也可以去掉空白字符，这只是一个可选的作，我们可以直接去掉空白
- 11:23 角色。这里我有一个简单的列表理解，可以剔除空白字符，所以你现在可以看到这是没有空白字符的结果，
- 11:36 它和你看到的有些不同，因为这是一个复杂的字节对编码算法，我稍后会详细讲述，我会分享一些专门的部分
- 11:48 其实就是这样。所以目前，我们只是构建一个非常简单的版本，来大致理解我们在这里做的事情，比如我们是如何对文本进行分词化的。多做一点
- 12:01 高级，我的意思是，这是一种处理简单情况的好方法，但现在想象我们有一个更复杂的情况，那么这实际上会失败，实际上并不能解决
- 12:16 比如双划号之类的特殊字符，这样我们就能让它更复杂，所以我现在有了更复杂的正则表达式，还有，
- 12:27 空白区剥离。这很好地区分了单词和特殊代币。所以我们现在做了很多工作，把文本拆分成单独的部分来准备
- 12:41 标记和标点字符 我们用这个简单的例子测试，但我们实际用它来验证真实文本。为此，我们现在必须真正使用我们的原始文本，
- 12:57 我们这里最上面就是原始文本。这就是我们的原始文本，我们来看看这样做后会发生什么，是的，我们已经对原始文本进行了代币化，现在让我们看看有多少代币
- 13:15 有，所以我们有长度，让我把它保存为变量，因为这是大量代码，我们就称之为预处理，
- 13:37 然后，是的，我们这里有4690个代币，所以到目前为止我们在2.2节中做的是原始文本
- 13:51 我们对它进行了代币化，接下来的部分我们将讨论将这些原始文本转换为所谓的代币ID。
- 14:05 是的，现在让我们谈谈将代币转换为代币ID。之前我们讨论过如何将文本拆解成单个代币，现在在本视频中，我们将讨论如何将代币转换为代币ID，也就是所谓的代币ID。
- 14:23 它们本质上是唯一的整数，那我们该如何做到呢？第一步，我们或许可以看第2.3节，第一步是建立所谓的词汇。所以词汇表是
- 14:37 每个词之间唯一的映射，与唯一的整数。举个例子，为了简化，
- 14:44 如果我们这里有一个非常小的数据集，比如“快速的棕色狐狸跳过懒狗”，第一步就是将文本拆分成代币，这也是我们之前做过的。现在我们要整理
- 14:56 按字母顺序排列，确保里面没有重复的。然后我们将为整数分配一个唯一的映射。那么，我们如何在 Python 中实现这一点呢？所以我们已经有了
- 15:09 从上一节预处理，所以“预处理”指数据集中所有的唯一标记，现在我们可以先去除重复的标记。所以我们可以做
- 15:23 “set（preprocessed）”，可以去除重复的部分。然后我们也用“排序”来排序它们
- 15:29 Python函数，我们可以为所有单词分配一个变量来保持它们。所以我们先确认它真的有效。所以我们可以看到这些都是唯一的单词，词很多，我们实际上也可以
- 15:42 看看我们有多少，这份名单有多长。所以，大约有1130字。所以它的体积是预处理的四倍。所以我们有很多重复的，我们先把它留给
- 15:58 后来，我们称之为词汇量。那么，我们有多少独特的单词？是的，词汇量显示在1130以下。接下来，我们来真正建立词汇量，这是
- 16:14 其实在这个图示中相当简单，为了教育目的，我们可以为每个代币分配一个整数，一个唯一的整数，为每个整数代币枚举
- 16:28 全是文字。它会遍历所有唯一的单词，然后按升序分配这些整数标签，所以我们实际上可以看看，你可以看到它本质上是把每个单词映射到整数，这基本上就是我们现在做的。
- 16:52 现在，我们有了词汇表，接下来的步骤是，既然有了词汇表，我们就用这个词汇来实际对训练数据进行标记化
- 17:07 变成了令牌ID。你可以看到词汇是按顺序排列的，所以我们用这个映射按字母顺序排列。现在，对于任何新文本，比如培训文本，以及后续的文本
- 17:18 当我们想使用LLM时，我们会用这个词汇将任何文本转换成令牌ID。所以，基本上用词汇映射，如果我们有单词“the”，我们会找到单词“the”
- 17:32 在词汇表中，比如找到对应的整数，所以这里没有显示因为太长，对应整数7，然后第二个词“brown”对应整数0，接着“dog”对应整数1，依此类推。
- 17:48 因此，我们将给定文本编码到这些令牌ID。我们用 Python 来做这件事。我其实会在这里复制粘贴，否则也会花时间
- 18:03 如果我只是打字，我会犯错，因为在镜头前打字和安静写代码是完全不同的。所以我要给你展示一个简单的分词器
- 18:16 class，那是Python类，执行以下任务，通常执行“**init**”
- 18:24 当你实例化一个新对象时，我们稍后会做，让我先解释一下，然后我们这里创建一个新对象，但“**init**”是构造函数，它会取一个词汇来定义，或者直接保存词汇作为字符串到整数映射。
- 18:43 字符串映射到整数意味着我们从字符串映射到整数表示，然后我们也有同样的反转。我们现在不是用 is ，而是用“s：i”来反转字符串和整数，所以它会从整数映射到字符串。
- 19:04 所以，我们有整数和字符串，基本上是反演这个过程。是的，这里我们有两个词汇表，常规词汇和倒置词汇，然后我们有一个“编码”
- 19:17 这是一种方法，可以取任意文本，使用2.1节（抱歉，2.2）中的正则表达式，并将文本拆分成标记。这就是我们之前看到的情况。如果，这个词汇量相当长
- 19:32 我往上滑，所以让我或许可以评论一下。在之前的视频中，我们已经将文本拆解成代币。所以我们用了这个正则表达式。我们就这样走了
- 19:48 重用之前的正则表达式，然后去除空白，这里有趣的是，这里是创建令牌ID的地方。我们将使用
- 20:00 词汇表从这里开始，对于预处理中的每个令牌或字符串，我们会对它们进行转换。为了让你明白这意味着什么，如果我把这里的“Jack”交给我的词汇，
- 20:18 如果我有类似的东西，这应该能给我一个身份识别，对吧？这本质上是令牌ID，我也可以展示这里创建反向映射，如果我们想回到字符串和类型57，这应该能给出原始字。
- 20:47 基本上就是这里发生的，我们有一个词汇表，一个逆词汇表，这里我们只是从字符串到整数，字符串到整数指的是这个，
- 21:00 我们从字符串到这个令牌ID。我们对数据集中的每个令牌ID都这样做。数据集是文本的标记化版本，没错，这就是全部。
- 21:15 然后我们有一种“解码”方法。而“解码”方法则是倒着的。所以这里我们要通过从整数映射回字符串来重建文本，这正是我刚才给你展示的。
- 21:28 所以整数到字符串意味着我们从整数回到字符串，我觉得是的，这可能相对复杂，也可能不复杂，但信息量很大，所以在这种情况下我们实际使用它，我觉得当我们真正使用它时会更清晰。
- 21:46 那我们简单点，幸运的是我这里没有自动补全，我就从这里复制一下，实际上我们用词汇实例化一个新对象，像这样，哎呀，
- 22:00 没有定义，我需要先执行这个单元格，每次我说话和编码同时出现这种情况，我会忘记事情。那我们就用文字吧，别让我自己打字，
- 22:12 我只是复制粘贴，第一步是把文本转换成这些ID，先做“tokenizer.encode”，然后是“text”。这本质上就是把这个“编码”方法称为，
- 22:28 然后我们希望能看到这些代币ID被转换成。所以我们现在可以看到，这应该是该文本的令牌ID表示，是的，我们也可以
- 22:41 反过来，如果我们有这些令牌ID，就可以做“tokenizer.decode”和ID，这对应于这段代码。这基本上就是把它转换回文本。是的，什么叫
- 22:55 好处是你也可以做往返，这意味着我们可以这样做，“解码”，然后把这个编码放进去。这应该能给我们原文，或者
- 23:10 要接近原文。你可以看到有一些细微差别，比如这里有额外的空白，但总体来说效果相当不错。QE有一种方法可以将文本从令牌ID转换回文本。
- 23:30 所以这将是一个非常简单的分词器，它并不是GPT模型使用的真正分词器，这也是我称它为“SimpleTokenizer”的原因，但我认为这说明了
- 23:42 分词化 非常不错：我们如何从文本转变为整数表示。接下来的视频，我们将谈谈特殊编码、特殊代币以及我们如何处理它们。
- 23:59 好了，现在让我们谈谈添加特殊上下文标记。之前我们讨论过如何从训练集中的所有不同代币创建所谓的词汇，
- 24:11 然后我们用这些词汇创建了令牌ID。现在我们将讨论如何用特殊代币扩展这个词汇。例如，我们可能想根据
- 24:24 分词器，用于训练集外未知词的令牌。或者，例如，我们可以通过所谓的“<|endoftext|>”来表示文本结束。我们稍后在实际预训练和微调LLM时，会<text_endoftext|>详细讨论这个话题。
- 24:43 所以，这里的目标本质上是稍微扩展我们的简单分词器来处理这些特殊令牌。我还想给大家展示一下我们代币管理器的一个不足。所以，如果我上去
- 24:54 这里我们根据词汇定义了简单的分词器，如果我现在有稍微更复杂的文本，我会说是“复杂”。所以，在我们用这段文字之前，如果我有文本，
- 25:08 比如说，我们做点不那么复杂，但做点不同的东西，比如“你好，你喜欢喝茶吗？比如说，这是——一个测试吗。让我们试着看看我们的代币生成器是什么
- 25:27 当我们实际尝试编码时，会这样做。所以，正如你所见，现在出现了一个错误，它显示“KeyError： Hello”。“你好”是个非常普通的词。那么，为什么会出现错误呢？原因是
- 25:43 因为我们处理的短信显然是这样。所以，这里的“裁决”显然没有包含“你好”这个词。如果你正在创建一个大型语言模型，并用数万亿个令牌训练它，
- 25:58 这个词很可能在训练集中，比如这里我们只有这个非常小的数据集，甚至可能不包含像hello这样的简单单词。顺便说一句
- 26:10 稍后我还会告诉你更多关于一个能够自然处理未知单词的算法，比如如果单词是未知词，它会将其拆解成单个字符。
- 26:21 所以，也许是为了让你明白我的意思。所以，如果我直接输入类似这样的内容，你可以看到如果它不知道这个词，它会把它拆分成单个字符。但这更为
- 26:32 高级算法，目前我们如何通过处理未知词来让简单的分词器稍微复杂一些？所以，我们可以扩展我们的代币，
- 26:47 如果我们有所有标记，这些标记是预处理的文本标记或单词的列表。所以，这本质上是从数据集中重建我们唯一的代币。现在我们可以做的是
- 27:04 实际上是延长它。所以，如果我们在 Python 列表上使用 extend 方法，实际上可以添加额外的标记，比如我之前提到的文本结尾标记，
- 27:19 或者我们也可以添加一个用于未知单词的标记。所以，我们其实可以在这里添加任何我们想要的内容，只是扩展它。所以与之前的不同之处在于，我们正在创造它
- 27:31 和之前类似，但现在我们增加了数据集中还没有的代币。然后，是的，我们会像之前一样生成词汇。所以，这就是我们之前用的，
- 27:45 比如这个词汇的整数映射符号。然后是的，我们来看看词汇的长度，我想在我们之前，让我往上滚动一点，
- 28:00 1130。现在我们应该有1132个代币，我们再确认一下，是的，我们新增了这两个代币，
- 28:12 我们实际上打印第一个或最后一个，在这里是最后五个，我们可以打印前五个，它们应该看起来和之前看到的差不多。所以，就是特殊字符，
- 28:26 但更有趣的是，我们实际打印词汇表中的最后五个词。我们可以看到词汇表中现在有了这两个新的特殊标记，类似地我们可以修改
- 28:42 简单的分子器，真正使用这些特殊令牌。所以，我们现在必须做的是，基本上要添加一条特殊规则来处理这些数据。所以我就直接复制粘贴在这里，所以
- 28:57 如果字符串不是空的，我们返回该字符串。如果这个词汇表里有，否则如果不在词汇表里。所以，否则我们将以未知身份返回，寻找这个未知的标记，
- 29:11 这样我们就能防止这个修改版的简单分词器在诸如“hello here”、“yes”和“this”等词汇上崩溃，
- 29:23 这只是一个简单的修改。这实际上有助于我们运行简单的文本示例，为了简化，我们先复制这个。所以，我们正在重新初始化分词器。现在我们有了第二版代币生成器。我们会创建一个新的代码单元。然后是“tokenizer.encode（text）”。
- 29:47 希望现在这个方法有效。是的，你可以看到我们现在有了这个未知的标记，所以实际上，当我们回头看看会发生什么。所以，“tokenizer.decode”，
- 29:59 你现在可以看到它处理了未知标记，当然如果我们想重新生成原始文本，这并不理想。而且在接下来的章节中，我还会谈到一个
- 30:15 一个算法会让这个过程更复杂一些，叫做字节对编码，我会解释字节对编码如何处理未知的标记。
- 30:29 现在我们来谈谈字节对编码，这是一种帮助我们将分词器提升到新高度的算法。字节对编码是一种大约存在了一段时间的算法
- 30:41 大约30年左右。但确实，现在它在实现分词器方面非常流行，比如GPT-1、GPT-2、GPT-3和4，它们都在用这个字节对编码算法来实现
- 30:53 他们的代币化器。甚至其他公司，比如Meta AI最近的Llama 3模型，也采用了字节对编码。因此，字节对编码确实帮助我们解决了一个主要问题
- 31:04 我们代币管理器的不足。所以，如果我再往上看几个单元格，我们“SimpleTokenizer”的一个缺点是无法处理未知词。比如说，在这个例子中我们有
- 31:18 这个词“你好”。由于训练数据中没有表示hello，这会引发一个关键错误，
- 31:25 所以，我们的处理方式是用这里的未知占位符替换了未知的标记。所以，我们这里有一条特殊规则，用来替换任何不包含在
- 31:40 词汇量。所以，任何在这些所谓未知特殊标记训练中没见过的东西。比如说，如果我们有这样的文本
- 31:50 给你。因此，这些未知词会被“<|unk|>”占位符标记和这个标记替换，但对LLM来说，主要的缺点是它们看起来都一样。所以，LLM并没有
- 32:03 真正知道这里和那里有什么，因为这两种情况都用同一个令牌ID。在许多现实任务中，你有具体的名字或其他没有的事物
- 32:15 如果你在训练数据中包含多个未知标记，LLM根本不知道你在说什么，或者你指的是什么。
- 32:24 是的，这并不理想。因此，字节对编码算法是一种可以始终将任何类型的单词拆分成子标记的方法。所以，为了给你举个例子，我们简要讲一下
- 32:38 又是Tiktokenizer应用。我们马上也会在视频中使用tiktoken应用，或者说Python库。但如果我打字，比如说一个陌生词，然后就说什么
- 32:49 就像这里这样，你可以看到它实际上被拆分成了子词。那么，这里可能发生了什么
- 32:58 我们来解决这个问题，这更美好了。所以，我敢打赌这部分内容不包含在GPT-2的训练数据中，但它仍然能够将其拆解
- 33:10 在不失败的情况下，变成单个标记。所以，没有引发错误，也没有插入占位标记之类的。所以，发生的事情是它分解得更久了，未知数
- 33:22 单词被分成已知的子单词。比如说，它在训练集中看到过类似未知的存在。所以，这些都是未知的词。所以，你可以看到这里它成功地将它们拆分为
- 33:33 单独的子标记。如果我们有一个字符串，它实际上与训练数据中的任何东西都不对应，那么它总是会退回到单个字符。
- 33:44 所以，在这种情况下它只会用一个角色作为标记。所以，从这个角度看，它永远不会失败，因为如果出现未知词，它总会回到这些单独的字符，
- 33:54 这可能有点低效，因为一个词变成了许多代币。所以，如果你有很多未知词，这种编码方式并不是最高效的输入文本，但它永远不会
- 34:06 这次失败了。所以，这比我们之前的方法改进了，之前的方法只是用“<|unk|>”占位符替换未知标记。所以，如果你对字节对感兴趣
- 34:22 编码算法，它是如何工作的，OpenAI 实际上开源了这种字节对编码算法的推理方法。所以，他们有个GPT-2的GitHub仓库......应该是公正的
- 34:37 OpenAI GPT-2。然后在这个文件夹“src”和“encoder.py”中，他们实现了字节对编码，用来将单词或输入文本拆分到这些子标记中。现在他们不分享了
- 34:51 训练方法。所以，他们只会分享你已经训练过的代码
- 34:58 这个分词器。不过，如果你感兴趣，我是从零开始实现的。所以，如果你往下滚动，看看书的额外内容。这就是我从零开始开发的rasbt/LLMs仓库。
- 35:13 然后你会点击这个从零开始编码字节对标记器的链接，这会带你进入一个笔记本，Jupyter 笔记本，它一步步实现字节对编码
- 35:25 从零开始，包括培训。最后我还有一种方法可以把OpenAI权重加载进来。所以，这应该类似于OpenAI为GPT-2模型所用的方法。所以，如果
- 35:43 如果你想了解更多细节，我推荐你去看看这个。但请记住，这只是可选的，这已经很高级了，而且这本笔记本也很长，所以我才没写进去
- 35:54 书里写的，因为实际上这本书是关于大型语言模型的，这是完全不同的话题。这其实可以写成另一本书的话题，因为它其实相当复杂。
- 36:05 所以，这里的主要信息其实是，我们将在未知令牌等方面获取文本，并且现在有了方法通过这个BPE方法将其拆分成各个子令牌。
- 36:19 实际上，我们将使用名为 tiktoken 的库，这是 OpenAI 的一个开源库。所以，这本书，后面的章节，会从零开始实现所有内容，因为没错
- 36:30 这就是那本书的书名，《从零开始构建大型语言模型》，但我认为分词器其实并不是大语言模型本身的一部分。所以，在这里我们要说分词器
- 36:42 使用TikTok的这个库，因为它其实也非常实用，大多数人实际上都用它。所以，为什么它高效，比如比
- 36:52 这里的这个实现，以及OpenAI的另一个实现，是说最昂贵的函数调用或代码都是用Rust实现的，而Rust是一种不同的编程语言
- 37:05 这通常用于实现高性能代码。为了方便使用，人们通常会围绕这个设置一个 Python API。所以，我们将在 Python 中使用这个库，但大多数
- 37:17 核心部分用 Rust 实现，使其推理速度非常快。所以，我们要做的是实现，抱歉导入了 tiktoken 库，开始吧，
- 37:32 所以，tiktoken库确实帮助我们使用这个GPT-2代币管理器。顺便说一句，为什么用GPT-2而不是GPT-3或GPT-4，是因为在第五章后面我们会训练一个小型的GPT-2风格
- 37:46 模特。之后我们还会看到如何加载OpenAI的预训练权重，OpenAI共享了GPT-2模型权重。遗憾的是，他们没有共享GPT-3或GPT-4的模型权重
- 37:57 但GPT-3的架构与GPT-2相同，依此类推。无论如何，我们将使用 GPT-2 分词器使其兼容后续实现的模型
- 38:10 第五章。所以，是的，我们可以像这样导入 tiktoken 库，如果你按照第一个视频中描述的这个 Python 环境，用这个 requirements.txt 文件搭建了，应该可以
- 38:23 自动生效。如果出现导入错误，说明你可能还没安装 Tiktoken Python 库。所以，你可以选择用“uv pip install”，或者直接用“pip install tiktoken”
- 38:37 安装这个库。所以，如果你在这里执行这个代码单元，它会安装 tiktoken 库，但我注解了，因为我已经安装了它。接下来我也想说
- 38:50 简单确认一下我用的是哪个版本。所以，我一直有反复确认的习惯，因为是的，你永远不知道几年后是否运行笔记本，如果结果不同，可能是因为库的不同版本。
- 39:04 我用的是0.9版本，但如果你用的是旧版本，比如0.7或0.6之类的版本，应该也没问题
- 39:11 诸如此类。但如果你得到一些奇怪的结果，和预期不太一样，确保有正确版本永远不会是坏主意。所以，你也可以修正这个版本
- 39:21 比如这样的数字。那么，我们现在怎么用tiktokenizer？所以，我们要去了
- 39:27 实例化一个新的分词器对象。我们会使用我之前提到的GPT-2分词器。所以，我们确实得到了编码，然后是像这样的GPT-2。这将实例化GPT-2分词器
- 39:44 词汇量齐全，训练方式也很丰富。所以，这个已经准备好使用了，我们不需要训练任何东西。然后用法和我们自己的简单方法完全一样
- 39:58 分词器，这就是为什么我们在这里做了这么多实现，因为我认为这有助于说明编码和解码是如何像这些方法一样工作的。而这个tiktokenizer正在使用
- 40:11 完全相同的API。你可以看到我们现在正在编码这个。还有类似这样的“tokenizer.decode”。这让我们恢复了原文，正是我们之前拥有的。
- 40:31 现在让我给你展示一个更高级的案例，我在这里复制粘贴了一些文字。
- 40:39 我们来尝试编码这段文本。所以，在我之前提到它能处理任意词汇之前，所以，它实际上可以把任何单词拆解成子标记，那么，我们来看看会发生什么。
- 40:58 所以，你现在可能会注意到，实际上它会触发一个错误。为什么呢？所以有一个有趣的地方
- 41:05 关于GPD分词器的事。其中一种是他们使用所谓的特殊文本结束标记，我想我在之前的视频中略过，但通常使用的是文本结束标记，
- 41:19 如果我能在这里找到这个图，它通常用来串接文本，让我再确认一下，是的，这里有。所以，当你准备数据集用于训练大型语言模型时，
- 41:33 通常会有多份文件。为了向LLM提示一个文档的结束和下一个文档的起点，通常会添加“<|文本结尾|>”令牌，你可以在下一个文档的开头或上一个文档的结尾添加，这无所谓。
- 41:49 但通常你会用“<|end-of-text|>”分隔符来显示新文档的起点。所以，那个
- 41:55 LLM知道这是新文档。现在它不再是之前文件的一部分。这也被GPT-2等应用。而且这是他们用的特殊代币，但不是
- 42:07 这个代币管理器默认启用了，这也是它抱怨的原因。所以，你必须不幸地把它作为特殊标记添加。所以，你必须非常明确。然后
- 42:20 像这样加入这个特别的代币。是的，你现在可以看到这个功能正常了。因此，文本结尾对应于50,256，这本质上就是这个分词器的词汇量大小。那么，词汇表
- 42:39 本例中有50,256条条目。还有一点我想提，它可能看起来不适合标注为“allowed_special”的东西，但其实是可行的
- 42:53 用任何其他标记。所以，我可以随意添加一些文字。而且会是这样运作的。所以，正如我们之前说的，它可以拆解任意文本，只是这个例外，带有文本结尾的标记，
- 43:05 是的。这就是字节对编码算法的工作原理。所以，它本质上是一种将任意文本拆解成子词标记的方法。如果你感兴趣
- 43:15 关于具体实现方式，我强烈推荐。或者说，不算特别推荐，但我建议你看看这个源代码，我现在不太推荐，因为它
- 43:27 相当高级。这确实有点让人困惑。如果你愿意，也可以看看我在这里的实现，我在那里添加了一些评论。希望能让这件事更丰富一些
- 43:38 虽然可读，但确实是高级材料，我觉得这几乎可以成为另一本书的主题，实施时不一定需要理解
- 43:48 一个法学硕士。核心信息是我们现在有一个算法可以将文本拆解成标记，在下一个视频中我们将讨论实现数据样本。
- 44:03 现在我们来谈谈带有滑动窗口的数据采样。所以，这意味着我们之前谈过
- 44:09 关于从给定文本中创建这些子词标记。然后我们把潜台词标记转换成了标记ID。之后我们将学习如何将向量转换为向量，嵌入向量，
- 44:22 然后把它们交给LLM，但目前重点还是在这个公园，我们如何高效地完成？例如，LLM无法一次性接收所有代币作为输入，
- 44:34 所以，本节的主题是如何高效地向大型语言模型提供更小的令牌ID块。这样我们就能以后高效地训练LLM。那么，我们来看第2.6节，
- 44:50 在我们进入代码示例之前，还有一件事想提。大型语言模型（LLM）的显著特点之一是它们一次预测一个代币。
- 45:00 比如我们有一个输入文本，比如大型语言模型学会一次预测一个词，如果那是输入文本，我们考虑第一个词，如果只传递这个词，理想情况下LLM应该能预测下一个词。在下一行，LLM们学习下一个令牌是2，依此类推。
- 45:19 所以，目标是教LLM一次预测一个词。这也是大型语言模型的一个有趣之处，它在大规模训练时非常高效
- 45:33 非常庞大的数据集。在传统机器学习中，我们人类通常需要找到方法来创建带有标签的数据。这里其实非常简单，因为我们可以使用原始文本和标签
- 45:47 仅仅是成为下一个代币，就能自我创造。所以，我们只是接收文本，隐藏部分文本，比如这里我们隐藏文本，输入是一个令牌。然后下一个标记是
- 46:00 LLM学习预测的目标标签。例如，我们将进一步了解这个过程是如何运作的
- 46:06 在后续章节中优化损失函数、预测下一个标记等内容。这里的目标是以类似格式准备数据，这样当我们把输入传递给LLM时，
- 46:19 LLM随后有了它可以预测的下一个目标。所以，如果我回到我的代码，我们要做的是高效地对我们的“裁决”数据集进行这个作。
- 46:31 所以，再说一遍，“裁决”是我们下载的伊迪丝·沃顿的短篇小说
- 46:40 更早。接下来我们要做的是用 TikTokToken 分词器把这些编码成 token ID。如果我们这样做，我们可以看到有5145个令牌ID，
- 46:53 只是想想象他们，能看到他们。这些就是我们文本中的所有令牌ID，现在的目标是如何以子块的形式高效地将它们提供给LLM。
- 47:05 所以你可以看到，代币很多，LLM无法全部通过。
- 47:10 比如说，我们想提供四块，四个，因为它体积小，能放进JupyterLab里，当我们想象它时，我可以做出漂亮的图形。但实际上，这些块
- 47:21 通常要大得多，通常每次有1000、4000、8000个token，具体取决于LLM，而在这里，GPT-2的情况是同时使用1024个token
- 47:34 在预训练期间，我们只用四张笔记本来做可视化，这样我能更有效地用图表展示一些内容。所以，是的，这就是原因
- 47:46 我想在这里说。我们还会使用一个特定的子集，并且截断前50个代币，以便有更好的示例和文本，但这就像
- 47:58 实现细节，让视觉效果更美观一些。现在我想展示的是我们如何准备x（输入）和y（目标）。我已经在这里给你看过了
- 48:16 我们始终关注的是下一个目标代币。所以，我们在这里创建一个数据集，目标是输入移动一个位置，这有助于我们将目标作为LLM的下一个标记。举个例子，我这里有一段示例文本。
- 48:39 所以，示例文本应该是五千多，哎呀，tokens。所以，我用的是5095代币，因为我刚刚截断了一些代币。所以，我们有5095个令牌，上下文大小是四个。所以，我们只考虑四个代币。
- 49:01 这里我们先取前四个标记，然后移动一个位置，目标位置移动一个位置。你可以在输出中看到
- 49:12 我们有输入，然后目标移动一个位置。所以，如果我这样展示，它会是这样的样子，但有点难以辨认。于是，我插入了这个空白。
- 49:22 所以，你可以看到这里的重叠。所以，你可以看到这三个代币重叠，这将是下一个代币，而这是之前未在
- 49:34 目标。所以，如果LLM收到这个token，它应该预测这个token，如果LLM收到这些token，它应该预测这个token，如果LLM收到这些token，它应该预测
- 49:47 这个，如果它收到这些，它应该预测另一个，依此类推。这就是这里的全部想法，也许为了让我刚才给你视觉上展示的内容更清楚一些。
- 49:58 这里还有另一个代码示例，完全符合我刚才展示的效果，显示下一个令牌，因为这些数字可能有点抽象，我们也可以直接用分词器，比如让我复制一下，这样比直接重打更方便。
- 50:18 所以，我们可以用标记器把标记ID解码回文本，同时对目标和输入上下文都做这个，所以这也是一样的，抱歉，我可能忘了关闭
- 50:36 括号内。所以，你可以看看它会是什么样子。所以，我们总是预测句子中的下一个词，为了让这个过程更高效，我们将使用 PyTorch，
- 50:46 PyTorch 是一个非常受欢迎的深度学习框架，但它可能是目前使用最广泛的深度学习框架，因此我们将导入它并使用数据加载器
- 51:00 然后数据集类，因为他们实现得非常高效，我们会重复使用这些部分，因为它们其实不属于大型语言模型，我们还在讨论
- 51:08 设置数据集时，重新发明轮子、自己写数据加载器会有点繁琐。所以，是的，我们会用PyTorch
- 51:18 因此，尽管库名为“pytorch”，PyTorch 仍被导入为“torch”，这只是我们必须记住的惯例。如果你跟着第一章的视频看
- 51:32 我给你看过一些安装教程，如果你按照这个步骤安装requirements.txt，应该已经能用了。如果不行，说明你还没安装PyTorch
- 51:43 安装方式之一是“UV PIP 安装”或“PIP 安装”，具体取决于你的系统设置。只要输入这个命令，它应该会自动获得好的版本
- 51:54 给你的电脑用的。不过如果你想更具体一点，比如说，可以去 pytorch.org，这个网站上有类似安装程序菜单，里面有不同的命令
- 52:06 电脑。举个例子，我在Mac上运行pip，如果你默认用Linux，它会给我这个命令，这里是CPU代码，需要一点点
- 52:21 如果你有显卡，默认输入“pip install torch”，它会自带CUDA 12.4
- 52:31 图书馆。所以，它支持CUDA 12.4的GPU，但如果你想要特定版本，也可以更改，而且有不同的URL，等你观看时可能已经有了
- 52:41 全新的CUDA版本。所以，这其实是针对你支持哪些GPU驱动的具体内容，但你知道默认情况下，你也可以直接用“pip install torch”，理论上应该能用。
- 52:55 现在回到这个版本，我也只是想确认一下我用的是哪个版本。
- 53:02 所以，我现在用的是2.6版本，我应该说我开始写这本书时是2.0，因为那是两年前我开始写书时的最新版本
- 53:14 但我在后续所有版本的PyTorch上测试了这本书，没发现任何代码有什么区别。在GitHub仓库里我也有自动化测试，所以每次都会有新的PyTorch
- 53:24 版本出来了，我只是在反复确认它是否还能用。到目前为止，我没看到PyTorch有任何会让代码有变化的区别。所以，在这种情况下你可以放心
- 53:36 如果还要使用旧版的 PyTorch，例如 2.0、2.1、2.2、2.3、2.4 和 2.5，它们应该能正常工作
- 53:44 好吧。不过如果你发现有什么不对劲的地方，比如结果看起来不同，你可以尝试通过“pip install torch”来安装这个特定版本的PyTorch。
- 53:58 然后是2.6.0版本，这样就能安装我现在用的这个特定版本的PyTorch，好吗？所以，这算是个小插曲，也许还有一点我想提的是
- 54:13 如果你是PyTorch的新手，它本质上是一个深度学习框架，内容相对相当全面
- 54:19 它包含了很多东西，但我们只会用到这本书中的一个子集。本质上它更像是一个线性代数库，带有一些深度学习的便利函数，整体内容
- 54:31 如果你想了解PyTorch，可以看课程和书籍;如果你想高效学习，这里还有附录A。所以，我从2018年开始用PyTorch
- 54:44 七年了，我教过很多PyTorch工作坊，还写了另一本关于PyTorch的书，等等。我想说的是，这些年我用了不少PyTorch，而且我确实很频繁
- 54:57 认真思考这本书到底需要知道哪些部分。基于此，我编纂了这个附录A，基本上是一本大约40页的书章节
- 55:10 讲讲你基础LLM培训和开发所需的基础知识，这样你应该能相对快速上手。当然，如果你感兴趣，我推荐可以
- 55:23 考虑过以后开一整门课程或写一本关于PyTorch的书，但如果你只是想快速掌握这本书的PyTorch，可以先读附录A，也许已经足够了。
- 55:37 这可能会有更陡峭的学习曲线，因为我想保持高效和紧凑，但也许这正是你当时真正掌握节奏所需的全部。所以，你不说走
- 55:46 绕道几个月去学PyTorch，然后你可能会感到沮丧或者无聊。所以我建议你可以试试附录A，也许已经是了
- 55:59 足够让你了解情况了。现在回到数据加载。我们刚才谈到了PyTorch和导入PyTorch，现在我们实际上回到第2.6节，使用滑动窗口的数据采样。
- 56:13 我们之前谈到了目标词预测下一个词，现在我们将相应地准备数据集。所以，我们将接收输入数据，创建这些区块，
- 56:25 我之前提到过，我们用的是4块大小或上下文大小4来可视化，这样它就能很好地融入这个图形，等等。但实际上这些通常更大
- 56:36 比如GPT-2模型中的1024，或更新型号每行使用8,000个16,000个代币，但在这里很难直观地表达。所以，正如你所见，这里有一个左边的框
- 56:53 这就是输入 x 我们创建的目标，然后目标移动一个位置，正如你在这里看到的，原理和我之前展示的一样，我们只是创建输入和目标
- 57:04 同时从文本中获取，以便我们以后训练LLM时可以同时将两者都输入给LLM。那么，让我用代码来做这件事，我们会用 PyTorch 数据集类。
- 57:19 所以我们使用 PyTorch 数据集类的原因是，它能让我们以后使用 PyTorch 数据加载器，它们本质上是一种非常高效、创建批量、洗牌数据以及利用多个后台进程加快数据处理的方法。
- 57:37 它增加了很多便利性。数据集的样子是这样的：我们有一个构造函数，把它们设置为空列表，然后我们要对文本进行分词。所以，“txt”是输入
- 57:50 我们要传递文本，所以我们得到了令牌ID，这里是我们创建所谓区块的部分，这些区块基本上就是这两个区块。而不是，我应该去
- 58:03 也许再回来一次，我们不再只是为输入和目标创建一个区块，而是对整个数据集这样做。所以，我们在这里存储这些区块，我们在这里添加它们
- 58:15 输入和目标。所以，我们把它们附加到之前创建的这些列表上，原理和之前一样，我们是滑动输入数据集，然后
- 58:29 这里目标移动一个位置。所以，我们要做的是把左边的这个框滑到输入上。所以，你可以看到第一行是“在
- 58:40 然后当我们把它滑到上面时，下一个是“The City Stoded the”，然后是“Old Library”，依此类推，所以是的，是往前滑一个位置的，如果你的数据集非常大，这可能不是正确的做法，因为你无法在内存中存储15万亿个令牌。
- 59:02 所以，这会耗尽内存，但如果你只有几本书，几GB或几兆字节的数据，比如你有，比如一千本书，这会很方便
- 59:13 健康。所以，你不必担心过度优化，如果你真的要用数万亿个代币训练LLMs，还有一些更高级的方法和工具，但在这里，比如
- 59:25 教育目的：我们保持内容相对简单易读，因为这是基本方法。是的，也要理解底层是如何运作的
- 59:38 现在我们有了这个数据集，我提到了我们想使用的数据加载器。所以，让我把所有代码都输入出来，直接插入下面。所以，数据加载器在这里创建，我有一个
- 59:52 我正在定义的函数返回这个数据加载器。所以我们在这里定义文本，可以设置批处理大小，每个context_length的max_length。在设置4是长度之前，我们也可以先设置一个默认参数，之后可以覆盖它。
- 1:00:11 我们决定了移动速度，取决于我们移动了多少位（稍后会回到这个话题）、是否要洗牌数据集，或者是否要丢弃最后一批
- 1:00:22 或者不。这其实是个有趣的问题，主要是为了避免亏损激增。比如说，如果我们有一个数据集，比如一个例子，第二个例子，依此类推，如果我们有一个这样的数据集
- 1:00:37 我就这样做，假设我们有一个批次大小为2，数据加载器会对数据进行除以
- 1:00:52 批量设置为2，但如果数据集大小不能被批量大小整除，最后一批通常会变成非常小的批次，因为剩下的就是剩余部分
- 1:01:05 这常常导致训练不稳定等减重峰值。所以，如果你做多个纪元训练，最好总是移除最后一批。你也不一定非得去
- 1:01:16 担心删除某些数据点会遗漏，但通常数据集非常庞大，你知道如果不使用最后一批数据其实没什么影响，所以
- 1:01:28 通常保持相同规模的批次是个好主意，这样训练时就不会出现这些不稳定的问题。这就是为什么我建议总是放弃最后一批。然后
- 1:01:40 num_workers是用来使用多个后台进程的。我这里把它设为零，因为我正在执行
- 1:01:46 这在笔记本中，在某些情况下确实可能行不通，因为笔记本在生成多个子进程方面比Python脚本更受限。或者如果你是
- 1:01:59 使用 Windows 我觉得 Windows 在 Python 中通常会遇到多个后台进程的问题。所以，这其实是一个安全的选择。不过你也可以尝试设置更大的数字。不过对于这些用途，笔记本里设为0应该没问题。
- 1:02:17 是的，这确实是很概念化的，这里还有几点，我们正在初始化分词器，这就是我们之前提到的 TikTok 分词器。然后我们创建数据集，
- 1:02:29 这就是我们这里的数据集，我们正在创建它，嗯，是的，'txt'，'txt'将是我们的训练数据集。这里是我们用设置I创建数据加载器的地方
- 1:02:42 之前定义。我们正在回归。这一切都非常，我会说是“概念性的”。那我们就实际看看它在实际作中。为此我们再次使用“裁决”，即我们的
- 1:02:55 我们之前下载的简短故事。伊迪丝·沃顿的短篇小说，raw_text。所以，事情是这样的
- 1:03:04 看起来我们之前见过，现在我们要把它传给我们的数据集和数据加载器。所以，为了不然视频会很长，我就直接复制粘贴
- 1:03:15 这里有代码。我们将从批次大小1、上下文长度1、步幅1开始。所以批量尺寸基本上就是每批样品的数量。
- 1:03:29 最大长度是我们的上下文长度。关键在于上下文的长度，比如盒子的大小，以及
- 1:03:36 里面有很多代币。然后，进步取决于我们移动了多少地方。所以，为了简化起见
- 1:03:44 我会给你展示1的步数，然后我们会改变它。所以，如果这点还不清楚，请稍等......我们会慢慢处理的。无论我们是否洗牌数据。然后我们开始迭代
- 1:03:57 这里只是为了手动演示数据集。之后我会给你展示另一种做法。所以，这里我们创建了一个迭代对象，它是我们的对象，然后
- 1:04:09 我们正在从数据集中取下一个样本。所以，当我们执行这个过程时，可以看到下一批。所以，这是第一个输入，它的大小只有1，然后是
- 1:04:24 目标。所以你可以看到目标被移动了1个位置。然后我们再画一批。你可以看到，这里的目标变成了这些输入。所以，第一个总是
- 1:04:37 输入。第二个总是目标，因为我们就是这样设置数据集的。所以，第一个是输入，第二个是目标。现在你可以看到，这可能不是
- 1:04:53 理想状态是因为当我们有这个输入，而第二次迭代是这个输入时，存在重叠。所以，这些代币是一样的，如果你
- 1:05:06 这样做的话，LLM会多次看到某些标记，这就导致了过度拟合。所以，实际上更好的做法是把步幅增加四步。如果我们这样做，你可以看到每个代币都是独一无二的。所以你可以看到它们不再重叠了。
- 1:05:26 它的作用是让这个盒子移动四步——四个位置。所以，一、二、三、四，
- 1:05:33 移动四位。所以，我们加上“城市站立了”，你可以看到这里显示的内容
- 1:05:39 第二排。我只展示一个步幅，是为了让你看到这个方法确实有效，没有缺失任何数据，因为你可以看到这个数据就在这里接着。
- 1:05:53 如果步幅是1，我觉得这会更直观，但如果让人困惑，也别担心。只需走到4的步幅，你就会得到你在这里看到的
- 1:06:03 算。现在，这是针对1批次的。当我们训练深度网络时，通常效率很高
- 1:06:12 用更大的批次。所以，我们可以使用批量8。我只是在这里给你展示同样的东西。批次尺寸不是1，而是8批次，最大长度4，步幅4。然后，是的，也有一些标签，这样会更清晰一些。
- 1:06:29 是的，你可以看到这里有输入和目标，这和我们看到的类似，这里有多行，每一行是一个训练示例。所以，我们有一个批量大小：一、二、三、四、五、六、七、八，然后是对应的目标。
- 1:06:51 是的，这实际上就是我们实现这个数据加载器的方式。之后，我们也可以更高效地迭代。所以，这个话题以后再说。我只是想设置这个数据加载器，这样以后我们就有一种方式可以高效地把数据加载到LLM里。
- 1:07:14 我们之前讨论了很多关于创建令牌ID的内容，但现在让我们进入下一步，谈谈创建令牌嵌入。所以，如果我回到2.2节的早期图表，
- 1:07:25 我们看这张图，我展示了输入文本会被转换为标记化文本，而标记化文本又被转换为标记ID。下一步是创建这些代币
- 1:07:37 从令牌ID进行嵌入——本质上是将一个ID和整数值转换成嵌入向量。因此，这个嵌入矢量包含实数，比如浮点数
- 1:07:49 这些数字，然后可以进行优化。所以，是的，为了展示它的工作原理，我们将用一些令牌ID作为示例，就像PyTorch张量一样，这样更容易看到什么是
- 1:08:06 继续。那么，我们先做 torch.tensor 然后插入它们。我得说，这些数字相对较大。所以，我们实际上用一些小的，这样可以更好地呈现出一个清晰的可视化效果。
- 1:08:20 这些只是我们输入中的一些词语或符号。所以，我们现在有了这个输入张量，下一步就是创建一个所谓的嵌入层。通常是这样
- 1:08:33 嵌入层是LLM本身的一部分。我们本书后面会多次讨论，但这将是一个独立的嵌入层，通常称为词汇量。这就是分词器、词汇分词器以及输出维度的大小。
- 1:08:56 这就是向量嵌入的尺寸。在我们的案例中，tiktoken tokenizer的词汇量是50,257。抱歉，可能不是这样
- 1:09:10 正确的号码。让我再确认一下。实际上，代币化器——一种方式，如果你像我一样忘记了，那
- 1:09:20 词汇的名称是，你可以用Python的“dir”，它应该会显示出来。所以，是n_vocab。如果我上去，输入n_vocab，然后计算长度，你可以看到——哎呀——
- 1:09:38 这不是一份清单;它只是一个整数。你可以看到它有50,257个独特代币。所以，这也是我们嵌入层的一个论据——它支持所有
- 1:09:50 这些不同的代币。另一个是输出维数，即这些向量的期望尺寸。
- 1:10:05 为了简化，我们实际上使用更简单、更小的数值，这样更容易可视化。
- 1:10:11 那么，我用词汇大小六和输出维度。让我复制一下。
- 1:10:22 我们做三个，因为我们这里看到的也是三个——就像每个向量有三个小方框。
- 1:10:28 所以，我们有一个非常小的例子，希望能容易看出来。还有一点，我还要补充
- 1:10:36 这里有个随机的种子。我为什么要这么做？这是因为这是一个较新的网络层，权重随机。所以，每次实例化新图层时，随机权重都不同，所以我只是想确保你得到和我一样的结果。所以，我正在初始化这个随机种子。
- 1:10:56 顺便说一句，如果你不熟悉随机种子，也请告诉我。我还有一个关于随机种子的不错视频可以和大家分享。但没错，这就是这个嵌入层，而嵌入层有一个权重矩阵。
- 1:11:15 重量。所以，这会在神经网络训练中进行优化，比如大型语言模型训练。是的，它们被称为权重参数或参数，这些参数是
- 1:11:28 这些都会被调整。顺便说一句，如果你已经熟悉线性层或矩阵乘法等，嵌入层只是实现
- 1:11:40 为了让某些查找更简单或更高效。这更高级——如果你不熟悉，不必读。但对于好奇的人来说，我这里确实有一些额外资料，帮助你理解......
- 1:12:02 就一会。。。嵌入层与线性层之间的区别。所以，如果你好奇这和矩阵乘法和线性层有什么关系，这就像是给那些
- 1:12:14 好奇。但如果你不熟悉，也不必担心。我会尝试用更直观的解释来解释。那我给你看点东西。如果我做嵌入层......
- 1:12:27 我会调用，比如说，这里的这个输入。我们就从这些数字中选一个吧。
- 1:12:34 Torch.tensor，我们做一个更小的张量，假设这里的三个张量。
- 1:12:42 向量。让我们看看会发生什么。你可以看到它在这里拉出一个向量。有一个由三个不同数字组成的张量。如果你眼光敏锐，可能会发现其实是这样
- 1:12:57 从这个嵌入矩阵中行到这里。所以，如果我们在这里输入三——Python 是零索引的——所以是 0、1、2、3，它给出了这个矩阵的向量。同样，如果我去这里并且真的去了......我们用第一个......我做这个，你可以看到这是第一个，嗯......
- 1:13:23 抱歉，没到第三排。抱歉——0、1、2，因为那是数字2，而这正是对应这行的索引2。同样地，如果我现在使用整个输入ID......
- 1:13:40 我正在从这个矩阵中提取数值。所以，我可能应该在这里展示它们作为参考。
- 1:13:50 所以，第一个，也就是二，对应于0、1、2——就是这里这个。第二个对应于这三个。所以，这就是这排。五——这个——0、1、2、3、4。
- 1:14:07 五，依此类推。所以，本质上，当我们在输入ID上调用这个嵌入层时，我们就是从这个矩阵中提取向量。这个矩阵的大小从技术上讲是我们的
- 1:14:22 代币化器。所以，我可以给你举个例子：tokenizer.n_vocab，对吧？所以，在这种情况下，这可能是个非常大的事件。我应该打印吗？也许我们试着看看会发生什么。其实是
- 1:14:35 点点，但这样基本上就是50,257行。是的，这本质上就是用来将令牌ID转换为这些向量的嵌入层。这是
- 1:14:48 通常是LLM的一部分。是的，我之前提到的这些数字——它们是随机的。所以，如果我真的
- 1:14:54 评论一下，你可能会看到这些数字变化，因为权重矩阵——是的，权重矩阵会变。通常，我们想做的是从小随机数开始。
- 1:15:08 这些数字其实并不重要，但在训练过程中，这些数字会被优化，因为LLM会从端到端优化，以产生所需的效果
- 1:15:21 文字——训练文本中的下一个词。所以，不用担心这些数字为什么是随机的，因为它们还没有经过训练。它们只是起始价值。我们稍后会训练它们
- 1:15:32 在第五章。但现在你应该已经对嵌入有了很好的理解。嵌入是从该嵌入矩阵中获得的向量，之后会被训练。
- 1:15:46 那么，既然我们已经理解了词嵌入，让我们更进一步，加入位置信息。我稍后会告诉你位置信息的含义。但没错，就是为了
- 1:15:57 回顾——我们有输入文本，对输入文本进行了分牌化，并创建了令牌ID。然后，上一节
- 1:16:05 专注于创建所谓的代币嵌入，而我们只有一个非常小的玩具示例。然后，为了使用分词器的实际大小，我们提到分词器有50,257个单词。所以，我们将这个词汇大小传递给嵌入层，使其生成该大小。
- 1:16:26 现在，我们也稍微扩大了这个规模，因为之前，我们的输出维度只有一、二、三个代币，这只是一个玩具例子。现在，我们把它增加到256，也就是
- 1:16:39 是我们之后用来训练LLM的。它更真实的尺寸。GPT-2最初的训练输出维度大概是1024——我不太确定;我得再确认一下。
- 1:16:53 但确实，这在小型电脑上，比如笔记本电脑上也能很好地运行。
- 1:16:58 如果你的电脑支持，你总可以尝试更大数值。但我认为大多数硬件都应该支持这功能以供演示，同时也比仅仅三维嵌入尺寸更真实一些。所以，我们现在有256个维度。
- 1:17:15 所以，如果我们这样做，我们就有一个包含5万行和大约200列的嵌入矩阵。这就是我们的权重矩阵，和之前做的类似。现在，迈出一步
- 1:17:30 此外，我们还有数据加载器。现在这些嵌入是如何结合起来的？
- 1:17:35 之前，我们创建了这些令牌ID。如果我直接给你看，和我们之前讲的内容一样。所以，我们有一个数据加载器，可以生成大小为八、最大长度为四的批次——
- 1:17:50 所以，每个训练示例有四个令牌ID，然后我们有八个示例。那么，现在它与我们在上一节看到的令牌嵌入有什么关系呢？那么，我们该怎么做
- 1:18:04 现在实际上是将这些令牌ID转换为这些嵌入向量。如果我这么做，我之前给你展示的是，如果我输入输入ID，它会生成 的样本向量
- 1:18:18 就是我们之前提到的那些简单的输入ID。但现在，我们将使用实际的数据批处理。这就是我们转换的数据批次，我们称之为token_embeddings。
- 1:18:32 让我们拭目以待。我们来看看输出的大小。
- 1:18:41 所以，你可以看到现在我们还有八个批次。现在它是一个三维张量，数字对应批次大小。所以，我们有一、二、三、四、五、六、七、八
- 1:18:52 行——八个例子。每个例子有四个标记，比如一、二——哎呀——一、二、三、四。但现在，
- 1:19:01 不仅仅是令牌ID。这些实际上都是256维的向量。这里，每个令牌ID都被转换成三维矢量。现在，它是一个256维的矢量。很难想象256维的向量，但我们实际上可以打印出其中一个。
- 1:19:24 所以，如果我们看第一个——第一个批次，第一个标记——这应该给出向量——对应第一个标记的256维向量
- 1:19:39 批次。基本上，就是这个。好了，现在我们已经将令牌ID转换为令牌嵌入。下一步是添加位置信息。让我看看身材好不好看
- 1:19:54 为了这个。之前，我们所做的是从权重矩阵中提取每个输入实例的嵌入向量。所以，我之前选择了一个输入，是的，我会
- 1:20:07 比如说，在解释上相对不错，每行输出看起来都不一样。所以，我们有“狐狸跳过狗”。这些是对应于类似示例输入的令牌ID。
- 1:20:19 然后，我们从权重矩阵中提取了对应代币ID的嵌入向量。那么，如果我们有相同的令牌ID，会发生什么？例如，“狐狸跳过狐狸”。这里，这个标记索引2是权重矩阵中的第三行。因此，我们得到这行。
- 1:20:42 但因为在最后一个令牌ID，我们有相同的词，同样的令牌ID，它有相同的向量。我是说，这说得通，对吧？我们正在查这里的局面，我们要撤退了
- 1:20:54 向量，然后这样生成输出。这没问题，而且确实可行，但我们实际上可以
- 1:21:01 补充一些信息。因此，为了根据该词在输入中的位置，向LLM提供更多信息，我们可以创建所谓的位置嵌入信息。在那里
- 1:21:15 有不同的方式可以做到这一点。GPT-2 采用了非常简单的方式——它使用了第二层嵌入。如今还有其他方法，例如使用旋转位置嵌入。如果你是
- 1:21:27 出于好奇，在我 GitHub 仓库的额外内容中，我从零实现了 Llama 3.2。它更高级、更复杂，而且描述得不像书里那么美好，
- 1:21:39 但他们用的是叫做RoPE嵌入的东西，我在这里也实现了。但GPT-2在这方面，确实，幸运的是，嵌入要简单得多。它只是使用了第二个嵌入
- 1:21:50 层叠。那么，让我实际演示一下这个方法是怎么实现的。我们用的是之前用的嵌入类，但现在有了令牌嵌入层。此外，我们还有一个位置嵌入
- 1:22:03 图层中添加了位置信息。所以，目标是——也许我有个身材——是的，我确实有
- 1:22:08 关于这个数据。如果我们有这样的输入嵌入，目标是在这里添加第二组值——也就是我们添加的位置嵌入。所以，我们有——抱歉——token嵌入。踏步
- 1:22:23 通过这个图，我更慢地为每个示例选择相同的符号嵌入，以保持简单。所以，如果输入中出现了同一个词，比如“狐狸狐”，
- 1:22:37 例如，所有情况下，令牌嵌入都相同。但随后，我们会在此基础上添加位置信息，因此实际输入嵌入的效果会根据其位置不同而有所不同。现在，我用的是这些非常好的数字，这些数字很容易读懂。
- 1:22:55 实际上，这些都是以随机数字开头的，所以看起来没那么漂亮。
- 1:22:59 但这些数字——位置嵌入数——在LLM训练后期也会被自动优化。所以，它们一开始是随机数，之后会被优化
- 1:23:10 类似于令牌嵌入。所以这里，是的，我基本上是在创建一个第二位置嵌入层。然后，我在这里做的是设置最大长度。因此，最大长度为
- 1:23:24 支持多少输入。在这种情况下，我们只有四个，对吧？所以，我们最大长度是四人——一、二、三、四。它只能支持四个输入标记。实际上，这一层要大得多。所以，为了简化，我们就保持它很小，方便可视化和方便。
- 1:23:46 以后训练LLM时，我们实际上会使用更大的数值。然后，我们可以用torch.arange。所以，arange 在这里创建顺序的顺序值——零、一、二和三。
- 1:24:04 所以，零、一、二、三——它只是张量。它是位置的占位张量：位置零，
- 1:24:10 位置一、二、三。我们有一个位置嵌入矩阵，它和之前的矩阵很相似——哎呀。然后，当我们打电话时......
- 1:24:28 当我们打电话时......
- 1:24:33 用我们的Arange，它会按顺序提取这些值。所以，你可以看到
- 1:24:45 给你。它只是给我获取这些值，这就是这个位置嵌入层中实际发生的事情。然后，我们会把它们叠加在一起。
- 1:24:59 也许在执行之前——这个有形状 [8， 4， 256]，这个应该有相同的形状，这样我们就可以把它们相加在一起。我还没在这里执行，抱歉。
- 1:25:19 这实际上只有[4,256]，因为PyTorch的工作原理。我的意思是，为了提高效率，我们不需要在批次维度上重复这个，因为这个会被添加到每个批次中
- 1:25:32 分开。所以，实际上发生的事情是，当我们有这样的东西时，输入嵌入的大小和——抱歉——令牌嵌入的维数和位置嵌入是一样的。
- 1:25:44 然后，输入嵌入的维度也会相同。在这里，这个概念在PyTorch中被称为广播。所以，让我这样给你展示一下吧。当我们这样做时......
- 1:26:02 我们可以把这些数值相加。这是训练集中的第一个示例，然后是我们的位置嵌入矢量。这本质上类似于这样做，只不过现在是这样
- 1:26:14 我们对批次中的所有示例都这样做。但第一个例子——也许你能在这里看到——应该是类似的。所以，你可以看到这个和这个很相似。我是说，这个
- 1:26:25 就像是四舍五入的东西——不是四舍五入的东西，而是不同的印刷表现。但这行本质上和这行是一样的，依此类推。所以，这只是对整体的概括
- 1:26:39 批量尺寸。哎呀，让我稍微整理一下。然后，当我像以前一样把它们加在一起时，
- 1:26:48 形状应该和之前一样。所以，我们只是补充了这些额外的位置信息。因此，我们现在涵盖了整个输入流程。我们有意见
- 1:27:03 我们将文本拆解成令牌化文本，然后转换成令牌ID。然后，我们创造这些
- 1:27:10 通过我们的令牌嵌入层进行令牌嵌入。我们创建位置嵌入，将它们相加
- 1:27:17 为了获得输入嵌入。所以，目前我们需要知道的数据准备步骤就这些。接下来，让我们真正探讨一下类似GPT的解码器变换器的内部结构。那么，什么是
- 1:27:33 真的在模型里？还有一点我应该说——嵌入层技术上是GPT模型的一部分，正如我们稍后会看到的。我只是在这里画他们，因为这样更容易画
- 1:27:45 既然我们已经谈过数据加载器，就来谈谈它们吧。但没错，在下一章中，我们将更多地讨论GPT模型内部的注意力机制。所以，希望这些对你有帮助，也希望下一个视频能见到你。
