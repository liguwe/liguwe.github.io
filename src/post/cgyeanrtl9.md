
# 多模态 LLM

`#2026/01/01` `#ai` 

- 当语言模型可“看到”图像并回答相关问题时，其效用将显著增强。
- 这种能够处理文本与图像（每种数据类型称为一种 **模态** ）的模型，即被称为 **多模态** （multimodal）模型，如图所示。

![{%}|656](https://www.ituring.com.cn/figures/2025/HandsonLLM/196.jpg)

> **图 ：能处理多种数据模态（如图像、音频、视频或传感器数据）的模型称为多模态模型。模型接收某种模态作为输入，但不一定能生成对应模态的输出**

我们已见证 LLM 展现出包括`泛化推理、数学运算及语言理解`在内的涌现能力。

事实上，语言并非孤立存在的，肢体动作、面部表情、语调变化等非语言要素，均能增强口语表达。

这一原理同样适用于 LLM。若赋予其理解多模态信息的能力，其功能边界将得以拓展，从而能够解决更多新型问题。

→ 又比如最近经常提的 `世界模型`


## 目录
<!-- toc -->
 ## 视觉 Transformer 

> 详情参考 [0. 多模态 LLM](/post/cgyeanrtl9.html)

## 多模态嵌入模型

> 更多参考 [3. 多模态嵌入模型 - 让AI同时理解图片和文字](/post/74boh33l6f.html)

## 让文本生成模型具备多模态能力

> 详见 [4. 让文本生成模型具备多模态能力](/post/w7ikzpy681.html)

## 小结

本章系统探讨了**如何让文本模型具备视觉理解能力**，主要涵盖以下三大技术路径：

### 1. **视觉Transformer (ViT)**

- 将图像切分成小块（patches），像处理文字一样处理图片
- 为多模态模型提供了图像编码的基础能力
- 实现了从像素到数值表示的转换

### 2. **多模态嵌入模型 (CLIP)**

- **核心创新**：把图片和文字转换到**同一个向量空间**
- **训练方式**：对比学习——让配对的图文相似，不配对的图文远离
- **典型应用**：
    - 零样本分类（无需训练直接分类）
    - 跨模态检索（文字搜图片、图片搜文字）
    - 驱动AI绘画工具（如Stable Diffusion）

### 3. **多模态文本生成 (BLIP-2)**

- **设计思路**：
	- 在图像编码器和文本生成模型之间加入"翻译器"（Q-Former）
- **训练策略**：
    - 冻结两端（图像编码器和LLM）
    - 只训练中间的Q-Former
- **实际能力**：
    - 图像描述生成
    - 视觉问答（看图回答问题）
    - 多轮对话（带图像的智能助手）

### 关键技术突破

通过这些技术，LLM实现了从"只会读文字"到"能看懂图片"的跨越，使得：

- 用户可以上传图片并提问
- 模型能结合视觉和文字信息进行推理
- 构建真正的多模态智能对话系统成为可能

#### 技术意义

这些多模态技术**连接了文本与视觉表示**，揭示了LLM实现多模态能力的核心机制，为构建更智能、更通用的AI系统奠定了基础。

**简单总结**：本章讲述了如何给文本AI"装上眼睛"——通过`ViT`处理图片、CLIP对齐图文、BLIP-2生成描述，最终实现"看图说话"的多模态智能系统！
