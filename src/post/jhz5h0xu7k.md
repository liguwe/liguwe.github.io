
# 引入特殊上下文词元


`#2026/01/01` `#ai` 


## 目录
<!-- toc -->
 ## 1. 为什么需要特殊词元？ 

🤔 想象你在给语言模型构建一个"`词汇世界`"，有些特殊的"`通行证`"可以帮助模型更好地理解和处理文本。

## 2. 常见的特殊词元

### 2.1 未知词元 `<|unk|>`

- 用于处理训练词汇表中不存在的单词
- 相当于告诉模型："这是一个我不认识的词"

### 2.2 文本结束词元 `<|endoftext|>`

- 标记不同文本片段的分隔
- 帮助模型区分独立的文本源

### 2.3 填充词元 `[PAD]`

- 用于处理批量训练时的长度不一致问题
- 可以将短文本补齐到统一长度

## 3. 代码实现示例

```python
# 导入所需库  
import re  

# 初始化词汇表  
all_tokens = sorted(list(set(preprocessed)))  # 获取所有唯一词元  

# 添加特殊词元  
all_tokens.extend([  
    "<|endoftext|>",  # 文本结束标记  
    "<|unk|>"         # 未知词元标记  
])  

# 创建词汇表映射  
vocab = {token: index for index, token in enumerate(all_tokens)}  

# 打印词汇表大小  
print(len(vocab.items()))  # 输出: 1132（比原来的1130多了2个特殊词元）  
```

## 4. 实际应用案例

```python hl:13
class SimpleTokenizerV2:  
    def __init__(self, vocab):  
        self.str_to_int = vocab  # 词元到ID的映射  
        self.int_to_str = {i: s for s, i in vocab.items()}  # 反向映射  

    def encode(self, text):  
        # 分词处理  
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)  
        preprocessed = [  
            item.strip() for item in preprocessed if item.strip()  
        ]  
        
        # 处理未知词元  
        preprocessed = [  
            item if item in self.str_to_int   
            else "<|unk|>" for item in preprocessed  
        ]  
        
        # 转换为词元ID  
        ids = [self.str_to_int[item] for item in preprocessed]  
        return ids  
```

## 5. 特殊词元的应用场景

• 处理未登录词汇  
• 区分不同文本片段  
• 批量训练文本填充  
• 提高模型的鲁棒性

## 6. 代码验证

```python
# 测试未知词处理  
tokenizer = SimpleTokenizerV2(vocab)  
text = "Hello, do you like tea?"  
print(tokenizer.encode(text))  
# 输出可能包含 <|unk|> 对应的词元ID  
```

## 7. 深入理解

🌟 特殊词元就像是语言模型的"万能通行证"，帮助模型在复杂的文本世界中自由穿梭！

## 小贴士

• 不同模型可能有不同的特殊词元  
• 选择和设计特殊词元需要根据具体任务  
• 特殊词元可以极大地提升模型的理解能力

## 结语

`特殊词元`是大语言模型处理文本的"瑞士军刀"，能够灵活应对各种文本处理场景！
