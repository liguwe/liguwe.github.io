
# 语义搜索与 RAG

`#2025/12/31` `#ai` 

 **语义搜索** （semantic search）
 - 其核心在于通过`语义理解`而`非简单的关键词匹配来`实现精准检索。  
幻觉
- 尽管模型能够流畅自信地输出答案，但其内容在`准确性和时效性`方面仍存在不足。
- 这种现象被称为“`幻觉`”，  
 RAG（检索增强生成）
- 解决 `幻觉` 的主要方法之一，便是构建能够`实时检索相关信息`并输入 LLM 的系统，从而生成有事实依据的答案。


## 目录
<!-- toc -->
 ## 语义搜索与 RAG 技术全景 

围绕如何优化语言模型在搜索领域的应用，学界已形成丰富的研究成果。当前主流技术可分为三大类：稠密检索（dense retrieval）、重排序（reranking）与 RAG。 更详细的参考 [1.  语义搜索与RAG技术全景](/post/2xpeq1ktxr.html)

## 语言模型驱动的语义搜索实践

> 更多详情参考 [2. 语言模型驱动的语义搜索实践](/post/0tx5jxa6my.html)

## 8.3　RAG

随着 LLM 的大规模应用，用户开始频繁向其提问并期待事实性回应。模型虽然能正确回答部分问题，但也会出现大量看似自信实则错误的答案。业界主流解决方案是采用 RAG 技术，该技术最早在 2020 年的论文“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks” 4 中提出，其架构如图 8-24 所示

4 Patrick Lewis et al.“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” *Advances in Neural Information Processing Systems* 33 (2020): 9459-9474.
**

RAG 系统兼具检索与生成的双重能力，可视为传统生成系统的升级版：既有效减少了幻觉现象，又显著提升了回答的事实准确性。该技术还支持“与数据对话”（chat with my data）的应用场景，使企业和个人能够将 LLM 与内部数据或特定数据源（如书籍内容）对接。

这种模式同样适用于搜索引擎领域。当前越来越多的搜索引擎（例如 Perplexity、Microsoft Bing AI 5 和 Google Gemini）正在集成 LLM，用于生成搜索结果摘要或直接回答用户提问。

5 现为 Microsoft Copilot。——编者注

### 8.3.1　从搜索到 RAG

现在我们尝试将普通搜索系统升级为 RAG 系统，核心方法是在搜索流程末端接入 LLM。具体实现方式是将用户的问题与检索获得的前若干个相关文档共同输入 LLM，使其基于检索提供的上下文生成答案。图 8-25 展示了该过程的典型示例。



这种生成过程被称为 **基于知识的生成** ，因为检索系统提供的相关信息为模型构建了特定上下文，使其能够在目标领域内进行定向生成。延续前文嵌入式搜索的案例，图 8-26 直观展示了如何在搜索流程后衔接基于知识的生成环节。


### 8.3.2　示例：使用 LLM API 进行基于知识的生成

接下来我们了解如何在搜索结果后添加基于知识的生成步骤，构建首个 RAG 系统。本示例将使用 Cohere 的托管 LLM（基于本章前文所述的搜索系统），通过嵌入式搜索获取相关性最高的文档后，将这些文档与问题共同输入 `co.chat` 端点，从而生成基于知识的答案：

```nix
query = "income generated"

# 1.检索
# 我们将使用嵌入式搜索，但理想情况下应该使用混合搜索
results = search(query)

# 2.基于知识的生成
docs_dict = [{'text': text} for text in results['texts']]
response = co.chat(
    message = query,
    documents=docs_dict
)

print(response.text)
```

结果：

```haskell
The film generated a worldwide gross of over $677 million, or $773 million with subsequent re-releases.
```

我们对部分文本进行了高亮标记，因为模型识别出这些文本片段来源于我们输入的第一个文档：

```haskell
citations=[ChatCitation(start=21, end=36, text='worldwide gross', document_ids=['doc_0']), ChatCitation(start=40, end=57, text='over $677 million', document_ids=['doc_0']), ChatCitation(start=62, end=103, text='$773 million with subsequent re-releases.', document_ids=['doc_0'])]

documents=[{'id': 'doc_0', text': 'The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014'}]
```

### 8.3.3　示例：使用本地模型的 RAG

现在，让我们尝试使用本地模型复现这一基础功能。尽管较小的本地模型在性能上可能不及大型托管模型，且无法实现文本片段引用功能，但演示这一流程仍具有重要参考价值。首先，我们需要下载一个量化模型。

1. **加载生成模型**  
	通过以下步骤加载模型：
	```haskell
	!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
	```
	借助 llama.cpp、llama-cpp-python 与 LangChain 实现文本生成模型的加载流程：
	```haskell
	from langchain import LlamaCpp
	# 注意确保模型路径在你的系统上是正确的！
	llm = LlamaCpp(
	    model_path="Phi-3-mini-4k-instruct-q4.gguf",
	    n_gpu_layers=-1,
	    max_tokens=500,
	    n_ctx=2048,
	    seed=42,
	    verbose=False
	)
	```
2. **加载嵌入模型**  
	现在，我们加载一个用于生成嵌入向量的语言模型。在本示例中，我们将选用 BAAI/bge-small-en-v1.5 模型。截至撰写本书时，该模型在 MTEB 排行榜的嵌入模型类别中名列前茅，同时模型体积较小。
	```haskell
	from langchain.embeddings.huggingface import HuggingFaceEmbeddings
	# 用于将文本转换为数值表示的嵌入模型
	embedding_model = HuggingFaceEmbeddings(
	    model_name='BAAI/bge-small-en-v1.5'
	)
	```
	现在我们可以通过嵌入模型完成向量数据库的初始化流程：
	```haskell
	from langchain.vectorstores import FAISS
	# 创建本地向量数据库
	db = FAISS.from_texts(texts, embedding_model)
	```
3. **RAG 提示词**  
	提示词模板在 RAG 流程中具有关键性作用，这是我们将相关文档信息传递给 LLM 的核心。为此，我们将创建名为 `context` 的附加输入变量，该变量专门用于向 LLM 提供检索所得的文档内容：
	```python
	from langchain import PromptTemplate
	# 创建提示词模板
	template = """<|user|>
	Relevant information:
	{context}
	Provide a concise answer the following question using the relevant information
	provided above:
	{question}<|end|>
	<|assistant|>"""
	prompt = PromptTemplate(
	    template=template,
	    input_variables=["context", "question"]
	)
	　
	　
	from langchain.chains import RetrievalQA
	# RAG流程
	rag = RetrievalQA.from_chain_type(
	    llm=llm,
	    chain_type='stuff',
	    retriever=db.as_retriever(),
	    chain_type_kwargs={
	        "prompt": prompt
	    },
	    verbose=True
	)
	```
	现在，我们可以调用模型并提出问题：
	```haskell
	rag.invoke('Income generated')
	```
	结果：
	```haskell
	The Income generated by the film in 2014 was over $677 million worldwide.
	This made it the tenth-highest grossing film of that year. It should be noted, however, this figure includes both initial ticket sales as well as any subsequent re-releases. With these additional releases, total earnings surged to approximately $773 million. The release format transitioned from traditional film stock projection in theaters to digital projectors once it was expanded to various venues in the United States. This shift might have contributed to wider audience reach and potentially higher grossing figures over time.
	However, specific data on how this affected total earnings isn't provided in the information above.
	```
	与之前一样，我们可以通过调整提示词来控制模型的生成效果（例如回答长度和语气等）。

### 8.3.4　高级 RAG 技术

本节列举几种提升 RAG 系统性能的进阶技术。

1. **查询改写**  
	当 RAG 系统作为聊天机器人时，若用户提问冗长或需要关联对话上下文，基础 RAG 在信息检索环节可能表现欠佳。此时，使用 LLM 将原始查询转化为更利于检索的简洁形式是一种有效的策略。例如：

	> 用户提问：“我们明天有一篇关于动物的作文要交。我喜欢企鹅，可以写关于企鹅的。但我也可以写海豚。它们是动物吗？也许是吧。我们写海豚吧。比如，它们生活在哪里？”  
	这个原始查询应被改写为：  
	> 查询：“海豚生活在哪里”  
	此类改写可通过特定提示词或 API 实现。例如，Cohere 的 `co.chat` 便内置了专用的查询改写模式。

2. **多查询 RAG**  
	此方法扩展查询改写能力，支持针对复杂问题生成多个关联查询。例如：

	> 用户提问：“比较 NVIDIA 2020 年与 2023 年的财报。”  
	理想情况是找到同时包含两年数据的文档，但更有效的做法是生成两个独立查询：  
	> 查询 1：“NVIDIA 2020 年财报”
	> 
	> 查询 2：“NVIDIA 2023 年财报”  
	随后将两次检索的最佳结果输入模型进行事实性回答。进一步改进，可赋予改写器自主判断能力：需要执行检索，或直接生成可靠的答案。

3. **多跳 RAG**  
	针对需要分步推理的复杂问题，系统需执行连续检索。例如：

	> 用户提问：“2023 年排名最靠前的汽车制造商有哪几个？它们是否都生产电动汽车？”  
	处理流程如下：  
	> 第 1 步，查询 1：“2023 年排名最靠前的汽车制造商”  
	基于检索结果（如丰田、大众和现代），生成后续查询：  
	> 第 2 步，查询 1：“丰田汽车公司电动汽车”
	> 
	> 第 2 步，查询 2：“大众汽车集团电动汽车”
	> 
	> 第 2 步，查询 3：“现代汽车公司电动汽车”

4. **查询路由**  
	该技术使模型具备多数据源定向检索能力。例如：
	- 用户提出人力资源相关问题 → 检索公司知识库（如 Notion）
	- 用户提出客户数据相关问题 → 检索 CRM 系统（如 Salesforce）
5. **智能体 RAG**  
	至此，你可能已经意识到，前述增强功能正逐步将愈加复杂的任务交给 LLM。这种演进依赖于 LLM 对信息价值的评估能力，以及其整合多源数据的处理能力。这种新特性使得 LLM 愈发接近于能在现实世界执行任务的智能体。值得注意的是，数据源本身亦可抽象为工具。正如我们已经见到基于 Notion 的搜索功能，同理应也能实现向 Notion 发布内容的技术路径。  
	需特别说明的是，并非所有 LLM 都具备本节讨论的 RAG 功能。截至本书撰写时，仅有少数头部托管模型尝试支持此类特性。值得关注的是，Cohere 推出的 Command R+ 在此类任务中表现卓越，且其开放权重版本也可供使用。

### 8.3.5　RAG 效果评估

RAG 模型的评估体系仍处于快速发展阶段。推荐阅读论文“Evaluating Verifiability in Generative Search Engines”（2023），该研究通过人工评估对比了多种生成式搜索系统 6 ，其评估框架包含四个核心维度。

6 Nelson F. Liu, Tianyi Zhang, and Percy Liang.“Evaluating Verifiability in Generative Search Engines.” *arXiv preprint arXiv:2304.09848* (2023).

> 流畅性（fluency）

　　生成文本的语言流畅度与逻辑连贯性。

> 感知效用（perceived utility）

　　回答内容的信息价值与实用价值。

> 引用召回率（citation recall）

　　外部事实陈述中获得完整引证支持的比例。

> 引用精确率（citation precision）

　　引用内容对相关论断的支持的有效性。

尽管人工评估仍是黄金标准，但学界正探索通过 LLM-as-a-judge 范式实现自动化评估，即使用高性能 LLM 对生成结果进行多维度评分。Ragas 便是实现此类评估的开源工具库，它还包含以下两个评估指标。

> 忠实度（faithfulness）

　　答案与所提供上下文的一致性程度。

> 答案相关性（answer relevance）

　　答案与提问主题的契合度。

Ragas 官方文档详细阐述了各项指标的计算公式。

## 小结

本章系统探讨了`语言模型`在`搜索系统`中的创新应用。
- **稠密检索** ：
	- 基于文本嵌入相似性的检索机制，通过将搜索查询向量化，匹配最相近的文档嵌入。
- **重排器** ：
	- 以 `monoBERT` 为代表的系统，通过评估查询与候选文档的相关性分数实现结果排序优化。
- **RAG** ：
	- 在搜索流程末端部署生成式 LLM，基于检索所得文档生成附带引证的回答。
