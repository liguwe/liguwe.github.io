
# GPT-1 介绍


`#2025/07/31`


## 目录
<!-- toc -->
 ## 1. GPT-1 出现之前的问题：监督学习的局限性 

在 GPT-1 出现之前，构建高性能的自然语言处理（NLP）模型主要依赖一种叫 `监督学习` 的方法。  
- 监督学习需要大量的手动标记数据，比如：
- 如果要做情感分析（判断一段话是正面还是负面），我们需要收集很多人工标记的句子，比如：
	- “天气太糟糕了” → 负面情感 
	- “我很开心” → 正面情感  
- 问题是：
	- 标注数据很难收集，成本高，而且需要标注的人具备专业知识。
- 所以，监督学习的效果受限于标注数据的规模和质量。

## 2. GPT-1 的创新：引入无监督预训练

GPT-1 提出了一个新的学习方式，分成两个步骤：
1. 无监督预训练：
	- 不需要人工标记数据，只需要大量的普通文本（比如书籍、文章）。
	- GPT-1 的训练目标很简单：预测下一句话或下一个词。
		- 比如给定一句话：“我喜欢吃苹果”，模型会尝试预测下一个词可能是“因为它很甜”。
	- 不需要人工标注，只需要海量的文本数据。
	- GPT-1 使用了一个叫 BookCorpus 的数据集（包含约 11,000 本未出版的书籍文本）来进行预训练。
2. 监督微调：
	- 在完成预训练后，模型会在少量手动标注的数据上进行微调。
	- 比如，如果目标是情感分析任务，模型会在少量标注的情感数据上重新训练，以适应这个具体任务。

这种方法结合了`无监督预训练和监督微调`的优点：
- 无监督预训练解决了数据不足的问题，因为它可以利用大量未标注的文本。
- 监督微调让模型能够更好地适应具体任务。


> 为了训练 GPT-1，作者使用了一个叫 `BookCorpus` 的数据集：
> - 这个数据集包含 11,000 本未出版的书籍文本，内容非常丰富。
> - 通过这些书籍，GPT-1 学会了如何理解和生成自然语言。

## 3. GPT-1 的工作流程

### 3.1. 无监督预训练

- 先用大量的普通文本（比如书籍）训练模型，让它学习语言的基本规律，比如：
	- 单词之间的关系
	- 句子结构
	- 上下文的含义
- 这个阶段不需要人工标注数据，只是让模型“读书”并预测下一个词

### 3.2. 监督微调

- 模型预训练后，虽然已经懂得很多语言规律，但还不能直接完成复杂任务（比如情感分析）
- 为了让模型适应具体任务，需要用一小部分人工标注数据进行微调。比如：
	- 给模型一些标注好的句子（正面/负面），让它学会情感分析任务
- 微调的过程相当于“针对性训练”，让模型变得更聪明

## 4. GPT-1 的特点

- GPT-1 的架构只有一个 `解码器`（类似 Transformer 的解码器部分），没有编码器。
- 它的参数量是 1.17 亿个，相比后来的 GPT-2 和 GPT-3，规模很小。
- 尽管模型规模小，但它在多个 NLP 任务上表现不错，尤其是在微调后。

## 5. 总结：GPT-1 是怎么工作的？

1. 先读书：
	1. 用大量普通文本（比如书籍）训练模型，让它学会语言规律。
2. 再微调：
	1. 用少量人工标注数据让模型适应具体任务，比如情感分析。
3. 结果：
	1. 虽然 GPT-1 是个“小模型”，但它已经能完成多个 NLP 任务，为后来的更强大模型铺平了道路。

## 6. 为什么 GPT-1 是重要的？

GPT-1 的出现标志着一个重要的转变：
- 从依赖人工标注数据的监督学习，转向利用海量未标注数据的无监督学习。
- 它证明了：
	- 可以先用无监督学习进行预训练，再用少量标注数据进行微调，从而完成复杂任务。
后来的 GPT-2、GPT-3 等模型都在 GPT-1 的基础上进一步改进，使用更大的数据集和更多的参数，展现了 Transformer 架构的巨大潜力。

简单来说，GPT-1 的创新在于：
- 不再依赖大量人工标注的数据，通过无监督学习解决了数据不足的问题。
- 结合监督微调，让模型既能泛化（适应多种任务），又能专注于具体任务。
- 它是 NLP 领域的一个重要里程碑，开启了生成式预训练模型的时代。