
# è¯å…ƒåµŒå…¥ï¼ˆä»£ç ç‰ˆæœ¬è¯´æ˜ï¼‰

`#2025/12/28` `#ai` 

> å¦‚æœä¸å¥½ç†è§£ï¼Œå…ˆè¯» [213. è¯äº‘åµŒå…¥ï¼ˆä¼šç¼–ç¨‹çš„é«˜ä¸­ç”Ÿç‰ˆæœ¬ï¼‰](/post/nkewsnpdy7.html)


## ç›®å½•
<!-- toc -->
 ## æ‰¿æ¥å‰æ–‡ï¼šä»å­—ç¬¦ä¸²åˆ°æ•°å­—çš„æ¡¥æ¢ 

- å‰æ–‡ï¼š
	- æˆ‘ä»¬è§£å†³äº†"`å¦‚ä½•åˆ‡åˆ†æ–‡æœ¬`"ï¼ˆåˆ†è¯ï¼‰  
- ç°åœ¨è¦è§£å†³"`å¦‚ä½•ç”¨æ•°å­—è¡¨ç¤ºè¿™äº›è¯å…ƒ`"ï¼ˆåµŒå…¥ï¼‰

```python
# é—®é¢˜æœ¬è´¨  
æ–‡æœ¬ = "Hello world"  
è¯å…ƒåˆ—è¡¨ = ["Hello", "world"]  # 2.1èŠ‚ï¼šåˆ†è¯  
æ•°å­—çŸ©é˜µ = [0.1, 0.5, ...], [0.3, 0.8, ...](/post/wb78c8h11l.html#01,-05,-],-[03,-08,-)   # 2.2èŠ‚ï¼šåµŒå…¥ 
```

## æ ¸å¿ƒæ¦‚å¿µï¼šè¯å…ƒåµŒå…¥ = è¯å…ƒ ID åˆ° å‘é‡çš„æ˜ å°„è¡¨

### ğŸ’¡ ç±»æ¯”ï¼šHashMap in Action

```python
# ä¼ ç»ŸHashMap
user_db = {  
    "user_123": {
	    "name": "Alice", 
	    "age": 25
	},  
    "user_456": {
	    "name": "Bob", 
	    "age": 30
	}  
}  

# è¯å…ƒåµŒå…¥ï¼ˆToken Embeddingï¼‰  
embedding_matrix = {  
    0: [0.12, 0.45, 0.89, ...],    # è¯å…ƒID=0 â†’ å‘é‡  
    1: [0.67, 0.23, 0.11, ...],    # è¯å…ƒID=1 â†’ å‘é‡  
    50257: [-0.34, 0.78, ...]      # è¯å…ƒID=50257 â†’ å‘é‡  
}  
```

å…³é”®å·®å¼‚ï¼š

- `HashMap` å€¼æ˜¯ç»“æ„åŒ–æ•°æ®ï¼ˆå­—ç¬¦ä¸²ã€æ•°å­—ï¼‰
- åµŒå…¥çŸ©é˜µçš„å€¼æ˜¯`é«˜ç»´å‘é‡`ï¼ˆé€šå¸¸ 384ç»´ã€768ç»´ã€1024ç»´ï¼‰
- å‘é‡çš„`æ¯ä¸ªç»´åº¦`æ•æ‰è¯å…ƒçš„æŸç§è¯­ä¹‰ç‰¹å¾

>  [214. ä¸ºä»€ä¹ˆè¯å…ƒåµŒå…¥é€šå¸¸æ˜¯ 384ç»´ã€768ç»´ã€1024ç»´ï¼Ÿ](/post/67brnkc0bg.html)

## è¯­è¨€æ¨¡å‹çš„é™æ€åµŒå…¥å±‚

ä¸€å¥è¯æ€»ç»“ï¼šåµŒå…¥çŸ©é˜µ = ä¸€ä¸ªå¤§è¡¨æ ¼ï¼ˆè¯è¡¨å¤§å°Ã—å‘é‡ç»´åº¦ï¼‰ï¼Œç”¨è¯å…ƒIDæŸ¥è¡¨å¾—åˆ°å‘é‡ï¼ŒO(1)è¶…å¿«ï¼Œå°±åƒç”¨å­¦å·æŸ¥å­¦ç”Ÿä¿¡æ¯

### ğŸ“¦ å­˜å‚¨ç»“æ„ï¼šåµŒå…¥çŸ©é˜µ

```python
class LanguageModel:  
    def __init__(self, vocab_size=50000, embedding_dim=768):  
        # æ ¸å¿ƒï¼šåµŒå…¥çŸ©é˜µæ˜¯æ¨¡å‹çš„ç¬¬ä¸€å±‚  
        self.embedding_matrix = torch.randn(vocab_size, embedding_dim)  
        #                              â†‘           â†‘  
        #                         è¯è¡¨å¤§å°    æ¯ä¸ªè¯å…ƒçš„å‘é‡ç»´åº¦  
        
    def get_embedding(self, token_id):  
        # æœ¬è´¨ä¸Šå°±æ˜¯æ•°ç»„æŸ¥è¯¢ O(1)  
        return self.embedding_matrix[token_id]  
```

| å‚æ•°              | å«ä¹‰   | æ¸¸æˆç±»æ¯”       | å®é™…ä¾‹å­          |
| --------------- | ---- | ---------- | ------------- |
| `vocab_size`    | è¯è¡¨å¤§å° | æ¸¸æˆä¸­æœ‰å¤šå°‘ç§è£…å¤‡  | 50,000 ä¸ªä¸åŒçš„è¯å…ƒ |
| `embedding_dim` | å‘é‡ç»´åº¦ | æ¯ä»¶è£…å¤‡æœ‰å¤šå°‘ä¸ªå±æ€§ | 768 ä¸ªå±æ€§å€¼      |

ç”Ÿæˆç¤ºä¾‹ï¼š

```
                  ç»´åº¦0   ç»´åº¦1   ç»´åº¦2   ...  ç»´åº¦767  
è¯å…ƒID 0 (å¦‚"the")  [0.12,  0.45,  0.89,  ...,  0.34]  
è¯å…ƒID 1 (å¦‚"a")    [0.67,  0.23,  0.11,  ...,  0.78]  
è¯å…ƒID 2 (å¦‚"is")   [0.34,  0.78,  0.56,  ...,  0.91]  
...  
è¯å…ƒID 49999        [0.21,  0.65,  0.32,  ...,  0.44]  
```

`torch.randn` çš„ä½œç”¨

```python
torch.randn(50000, 768)  # ç”Ÿæˆ 50000Ã—768 çš„éšæœºçŸ©é˜µ  
```

- âœ… **éšæœºåˆå§‹åŒ–**ï¼šåˆšå¼€å§‹å‘é‡å€¼æ˜¯éšæœºçš„ï¼ˆä¸çŸ¥é“è¯çš„å«ä¹‰ï¼‰
- âœ… **æ­£æ€åˆ†å¸ƒ**ï¼šå€¼å¤§å¤šåœ¨ -1 åˆ° 1 ä¹‹é—´ï¼ˆé˜²æ­¢æ•°å€¼è¿‡å¤§æˆ–è¿‡å°ï¼‰
- âš ï¸ **è®­ç»ƒå‰æ— æ„ä¹‰**ï¼šè¿™äº›éšæœºæ•°æ²¡æœ‰è¯­ä¹‰ï¼Œéœ€è¦é€šè¿‡è®­ç»ƒè°ƒæ•´

å®é™…ä¾‹å­ï¼š

```python
# GPT-2 åˆ†è¯å™¨è¯è¡¨å¤§å°ï¼š50,257  
# æ¯ä¸ªè¯å…ƒå‘é‡ç»´åº¦ï¼š768  

# ä¸‹è½½æ¨¡å‹æ—¶ä½ ä¼šå¾—åˆ°ï¼š  
embedding_matrix.shape = (50257, 768)  # çº¦38MBçš„å‚æ•°é‡  


# GPT-2 çš„é…ç½®  
model = LanguageModel(vocab_size=50257, embedding_dim=768)  

# å°å‹æ¨¡å‹é…ç½®  
model_small = LanguageModel(vocab_size=30000, embedding_dim=384)  

# å¤§å‹æ¨¡å‹é…ç½®  
model_large = LanguageModel(vocab_size=100000, embedding_dim=3072) 
```

è®­ç»ƒå¦‚ä½•æ”¹å˜åµŒå…¥çŸ©é˜µï¼Ÿ

```python
# è®­ç»ƒå‰ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰  
"king"  â†’ [0.12, -0.45, 0.89, ...]  # éšæœºå€¼  
"queen" â†’ [-0.67, 0.23, -0.11, ...]  # éšæœºå€¼  
# ä¸¤ä¸ªå‘é‡å®Œå…¨ä¸ç›¸å…³  

# è®­ç»ƒåï¼ˆå­¦ä¹ åˆ°è¯­ä¹‰ï¼‰  
"king"  â†’ [0.52, 0.83, 0.21, ...]  
"queen" â†’ [0.48, 0.79, 0.25, ...]  
# ä¸¤ä¸ªå‘é‡éå¸¸æ¥è¿‘ï¼âœ…  
```

**è®­ç»ƒè¿‡ç¨‹** = åå‘ä¼ æ’­ä¸æ–­è°ƒæ•´åµŒå…¥çŸ©é˜µçš„å€¼ï¼Œè®©ç›¸ä¼¼è¯çš„å‘é‡é è¿‘ã€‚

### ğŸ”— åˆ†è¯å™¨ä¸æ¨¡å‹çš„å¼ºç»‘å®šå…³ç³»

```python
# âŒ é”™è¯¯ï¼šæ··ç”¨ä¸åŒçš„åˆ†è¯å™¨å’Œæ¨¡å‹  
gpt2_model = load_model("gpt2")  
bert_tokenizer = load_tokenizer("bert")  
token_ids = bert_tokenizer("Hello")  # [101, 7592, 102]  
embeddings = gpt2_model.embedding(token_ids)  # ğŸ’¥ ç»´åº¦ä¸åŒ¹é…ï¼  

# âœ… æ­£ç¡®ï¼šé…å¥—ä½¿ç”¨  
gpt2_tokenizer = load_tokenizer("gpt2")  
token_ids = gpt2_tokenizer("Hello")  # [15496]  
embeddings = gpt2_model.embedding(token_ids)  # âœ… æ­£å¸¸å·¥ä½œ  
```

åŸå› ï¼š

- BERTè¯è¡¨ï¼š30,522 ä¸ª è¯å…ƒ
- GPT-2è¯è¡¨ï¼š50,257 ä¸ª è¯å…ƒ
- æ¨¡å‹çš„åµŒå…¥çŸ©é˜µç»´åº¦`å¿…é¡»åŒ¹é…`åˆ†è¯å™¨è¯è¡¨å¤§å°

## ä¸Šä¸‹æ–‡ç›¸å…³åµŒå…¥ï¼šä»staticåˆ°dynamic

### ğŸ†š é™æ€ vs åŠ¨æ€åµŒå…¥

```python
# é™æ€åµŒå…¥ï¼ˆword2vecã€GloVeï¼‰  
word2vec["bank"]  # æ°¸è¿œè¿”å›ç›¸åŒçš„å‘é‡  
# â†’ [0.12, 0.45, 0.89, ...]  æ— è®ºä¸Šä¸‹æ–‡  

# é—®é¢˜ï¼š"bank"åœ¨ä¸åŒå¥å­ä¸­å«ä¹‰ä¸åŒ  
sentence1 = "I went to the bank to deposit money"  # é“¶è¡Œ  
sentence2 = "I sat on the river bank"              # æ²³å²¸  
# ä½†å‘é‡å®Œå…¨ç›¸åŒ âŒ  
```

```python
# åŠ¨æ€åµŒå…¥ï¼ˆTransformer LLMï¼‰  
model = AutoModel.from_pretrained("bert-base")  

sentence1 = "I went to the bank to deposit money"  
sentence2 = "I sat on the river bank"  

embedding1 = model(sentence1)["bank"]  # â†’ [0.12, 0.45, ...]  
embedding2 = model(sentence2)["bank"]  # â†’ [0.34, 0.78, ...]  ä¸åŒï¼âœ…  
```

---

### ğŸ§  å¦‚ä½•ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³åµŒå…¥

ä»£ç å®æˆ˜ï¼š

```python
from transformers import AutoModel, AutoTokenizer  

# åŠ è½½æ¨¡å‹  
tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-xsmall")  
model = AutoModel.from_pretrained("microsoft/deberta-v3-xsmall")  

# è¾“å…¥æ–‡æœ¬  
text = "Hello world"  

# Step 1: åˆ†è¯  
tokens = tokenizer(text, return_tensors='pt')  
# è¾“å‡ºï¼š{'input_ids': [101, 7592, 2088, 102](/post/wb78c8h11l.html#101,-7592,-2088,-102), ...}  

# Step 2: å‰å‘ä¼ æ’­ 
output = model(tokens)[0]  
# è¾“å‡ºå½¢çŠ¶ï¼š[1, 4, 384]  
#          â†‘  â†‘  â†‘  
#       batchè¯å…ƒæ•°å‘é‡ç»´åº¦  
```

è§£é‡Šè¾“å‡ºï¼š

```python
output.shape  # torch.Size([1, 4, 384])  

# 4ä¸ªè¯å…ƒï¼š  
# [0] [CLS]  â†’ [0.12, 0.45, ...] (384ç»´)  
# [1] Hello  â†’ [0.67, 0.23, ...] (384ç»´)  
# [2] world  â†’ [0.89, 0.11, ...] (384ç»´)  
# [3] [SEP]  â†’ [0.34, 0.78, ...] (384ç»´)  
```

---

### ğŸ¯ å¤„ç†æµç¨‹å¯¹æ¯”

#### é™æ€åµŒå…¥æµç¨‹  

```python
# é™æ€åµŒå…¥æµç¨‹  
text = "bank"  
    â†“  
token_id = 1234  
    â†“  
embedding = embedding_matrix[1234]  # ç›´æ¥æŸ¥è¡¨  
    â†“  
[0.12, 0.45, 0.89, ...]  # å®Œæˆ  
```

#### åŠ¨æ€åµŒå…¥æµç¨‹ï¼ˆTransformerï¼‰  

```python
# åŠ¨æ€åµŒå…¥æµç¨‹ï¼ˆTransformerï¼‰  
text = "I went to the bank"  
    â†“  
token_ids = [101, 1045, 2253, 2000, 1996, 2924, 102]  # åˆ†è¯  
    â†“  
static_embeddings = embedding_matrix[token_ids]  # åˆå§‹åµŒå…¥  
    â†“  
contextual_embeddings = transformer_blocks(static_embeddings)  # ç»è¿‡Nå±‚å¤„ç†  
    â†“  
[                                         # æ¯ä¸ªè¯å…ƒéƒ½æºå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯  
    [0.12, ...],  # [CLS]  
    [0.34, ...],  # I  
    ...  
    [0.78, ...],  # bank â† è¿™ä¸ªå‘é‡ç°åœ¨åŒ…å«"é“¶è¡Œ"çš„è¯­ä¹‰  
    [0.91, ...]   # [SEP]  
]  
```

## å®é™…åº”ç”¨åœºæ™¯

### 1ï¸âƒ£ å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

```python
sentence = "Apple is looking at buying UK startup for $1 billion"  
embeddings = model(sentence)  

# æ¯ä¸ªè¯å…ƒçš„åµŒå…¥ç°åœ¨åŒ…å«ä¸Šä¸‹æ–‡  
# "Apple" â†’ å‘é‡ä¼šæ›´æ¥è¿‘"å…¬å¸"è€Œé"æ°´æœ"  
# "UK" â†’ å‘é‡ä¼šè¢«è¯†åˆ«ä¸ºåœ°ç†ä½ç½®  
```

### 2ï¸âƒ£ æŠ½å–å¼æ–‡æœ¬æ‘˜è¦

```python
document = """  
GPT-4 is a large language model. It can generate text.  
The model was trained on internet data.  
"""  

# è·å–æ¯ä¸ªå¥å­çš„åµŒå…¥  
sentence_embeddings = model(document)  

# è®¡ç®—æ¯ä¸ªå¥å­çš„é‡è¦æ€§åˆ†æ•°  
scores = calculate_importance(sentence_embeddings)  

# é€‰æ‹©Top-Kå¥å­ä½œä¸ºæ‘˜è¦  
summary = extract_top_sentences(document, scores, k=2)  
```

### 3ï¸âƒ£ è¯­ä¹‰æœç´¢

```python
# æ–‡æ¡£åº“  
docs = [  
    "Python is a programming language",  
    "Machine learning uses algorithms",  
    "Deep learning is a subset of ML"  
]  

# ç”Ÿæˆæ–‡æ¡£åµŒå…¥  
doc_embeddings = [model(doc) for doc in docs]  

# æŸ¥è¯¢  
query = "What is Python?"  
query_embedding = model(query)  

# æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æ¡£  
similarities = cosine_similarity(query_embedding, doc_embeddings)  
best_match = docs[argmax(similarities)]  
```

---

## å…³é”®æŠ€æœ¯ç»†èŠ‚

### ğŸ“ ç»´åº¦è®¾è®¡

```python
# å¸¸è§ç»´åº¦é…ç½®  
æ¨¡å‹             | è¯è¡¨å¤§å° | åµŒå…¥ç»´åº¦  
----------------|---------|--------  
BERT-base       | 30,522  | 768  
GPT-2           | 50,257  | 768  
GPT-3           | 50,257  | 12,288  
Phi-3-mini      | 32,000  | 3,072  
```

æƒè¡¡ï¼š

- ç»´åº¦è¶Šé«˜ = è¡¨è¾¾èƒ½åŠ›è¶Šå¼º = æ¨¡å‹è¶Šå¤§ = æ¨ç†è¶Šæ…¢
- ç»´åº¦è¶Šä½ = æ¨¡å‹è¶Šå° = è¡¨è¾¾èƒ½åŠ›å—é™

---

### âš¡ æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹å¤„ç†åµŒå…¥  
texts = ["Hello", "World", "Python"] * 1000  # 3000ä¸ªå¥å­  

# âŒ ä½æ•ˆï¼šé€ä¸ªå¤„ç†  
embeddings = [model(text) for text in texts]  # 3000æ¬¡å‰å‘ä¼ æ’­  

# âœ… é«˜æ•ˆï¼šæ‰¹å¤„ç†  
batch_size = 32  
embeddings = model(texts, batch_size=batch_size)  # 94æ¬¡å‰å‘ä¼ æ’­  
```

---

## å¸¸è§è¯¯åŒº

### âŒ è¯¯åŒº1ï¼šè¯å…ƒåµŒå…¥ = è¯å‘é‡

```python
# word2vecæ˜¯è¯å‘é‡ï¼ˆé™æ€ï¼‰  
word2vec["king"] - word2vec["man"] + word2vec["woman"] â‰ˆ word2vec["queen"]  

# Transformerè¯å…ƒåµŒå…¥æ˜¯åŠ¨æ€çš„ï¼Œä¸èƒ½è¿™æ ·ç®€å•è®¡ç®—  
bert("king") åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å‘é‡ä¸åŒ  
```

### âŒ è¯¯åŒº2ï¼šå¯ä»¥ç›´æ¥ä¿®æ”¹åµŒå…¥çŸ©é˜µ

```python hl:4
# âŒ é”™è¯¯  
model.embedding_matrix[100] = my_custom_vector  # ç ´åè®­ç»ƒå¥½çš„æƒé‡  

# âœ… æ­£ç¡®ï¼šå¾®è°ƒæ•´ä¸ªæ¨¡å‹  
model.train()  
optimizer.step()  # é€šè¿‡åå‘ä¼ æ’­æ›´æ–°  
```

### âŒ è¯¯åŒº3ï¼šåµŒå…¥å‘é‡å¯ä»¥è·¨æ¨¡å‹å¤ç”¨

```python
# âŒ é”™è¯¯  
gpt2_embedding = gpt2_model.get_embedding("hello")  
bert_model.use_embedding(gpt2_embedding)  # è¯­ä¹‰ç©ºé—´ä¸åŒï¼  

# âœ… æ­£ç¡®ï¼šæ¯ä¸ªæ¨¡å‹æœ‰è‡ªå·±çš„åµŒå…¥ç©ºé—´  
```

---

## ä¸€å¥è¯æ€»ç»“

- è¯å…ƒåµŒå…¥ = `è¯å…ƒIDåˆ°é«˜ç»´å‘é‡çš„æ˜ å°„è¡¨`ï¼Œæ˜¯LLMçš„"`ç¬¬ä¸€å±‚ç¥ç»ç½‘ç»œ`"
- é™æ€åµŒå…¥ï¼ˆword2vecï¼‰ï¼š
	- æŸ¥è¡¨ç›´æ¥è¿”å›  
- åŠ¨æ€åµŒå…¥ï¼ˆTransformerï¼‰ï¼š
	- æŸ¥è¡¨ + Nå±‚å¤„ç† â†’ åŒ…å«ä¸Šä¸‹æ–‡çš„å‘é‡

å¼€å‘è€…è®°ä½ä¸‰ç‚¹ï¼š
1. åµŒ`å…¥çŸ©é˜µ`ä¸`åˆ†è¯å™¨å¼º`ç»‘å®šï¼Œä¸å¯æ··ç”¨
2. `Transformerç”Ÿæˆ`çš„æ˜¯`ä¸Šä¸‹æ–‡ç›¸å…³åµŒå…¥`ï¼ŒåŒä¸€è¯åœ¨ä¸åŒå¥å­ä¸­å‘é‡ä¸åŒ
3. `åµŒå…¥ç»´åº¦`å†³å®šäº†`è¡¨è¾¾èƒ½åŠ›`å’Œ`è®¡ç®—æˆæœ¬`çš„å¹³è¡¡
