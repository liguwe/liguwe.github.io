
# 多模态嵌入模型 - 让AI同时理解图片和文字

`#2026/01/01` `#ai` 


## 目录
<!-- toc -->
 ## 一、什么是嵌入模型？先回顾一下 

在讲多模态之前，我们先复习一下**嵌入模型**是什么：

想象你要让计算机理解一篇文章或一个句子，但计算机只认识数字。**嵌入模型**就是把文字转换成一串数字（向量）的工具。比如：

```
"我喜欢猫" → [0.2, 0.8, 0.3, 0.1, ...]  （一串数字）  
"我爱小猫" → [0.3, 0.7, 0.4, 0.2, ...]  （另一串数字）  
```

神奇的是，**意思相近的句子，转换出来的数字也会很接近**！这就是嵌入模型的核心价值。

## 二、什么是多模态嵌入模型？

### 1. 问题来了：能不能让图片和文字用同一套数字表示？

以前我们只能处理纯文字。现在问题来了：
- 能不能把**图片**也转成数字？
- 能不能让**图片的数字**和**文字的数字**放在同一个空间里比较？

比如：

```
图片：🐶（一只小狗的照片）  
文字："一只可爱的小狗"  
```

如果它们都转成数字后很接近，那我们就能：

- **用文字搜图片**：输入"小狗照片"，找到相关图片
- **用图片搜文字**：上传一张猫的照片，找到描述这只猫的文字

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/201.jpg)

> **图：多模态嵌入模型可在同一向量空间中为不同模态生成嵌入向量**

## 三、多模态嵌入模型的工作原理

### 核心思想：两个编码器 + 同一个向量空间

多模态嵌入模型就像两个翻译器，把不同语言（图片和文字）翻译成同一种"数字语言"：

```
┌─────────────┐          ┌──────────────┐  
│  图片编码器  │ ────→   │  图片的数字   │  
└─────────────┘          │  [0.5, 0.2...] │  
                         │                │  
┌─────────────┐          │  文字的数字    │  
│  文字编码器  │ ────→   │  [0.4, 0.3...] │  
└─────────────┘          └──────────────┘  
                              ↓  
                         计算相似度！  
```

**关键点**：两种模态（图片和文字）生成的嵌入向量在**同一个向量空间**中，可以直接比较距离。

![{%}|464](https://www.ituring.com.cn/figures/2025/HandsonLLM/202.jpg)

> **图 ：不同模态的语义相近的嵌入向量，在向量空间中仍呈现邻近分布*

---

## 四、最流行的模型：CLIP

### CLIP是什么？

CLIP（`对比语言-图像预训练`）是目前最主流的多模态嵌入模型。它的厉害之处在于：

1. **零样本分类**：
	1. 不需要训练，直接告诉它"这是猫还是狗"，它就能判断
2. **跨模态检索**：
	1. 用文字找图片，或用图片找文字
3. **驱动AI绘画**：
	1. 像Stable Diffusion这样的AI绘画工具，就是用CLIP来理解你的文字描述

### CLIP的训练过程（用人话讲）

假设有一个数据集：

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/203.jpg)

> **图：多模态嵌入模型训练所需数据形式**

```
图片1：一只猫的照片  
文字1："一只像素化的可爱猫咪"  

图片2：雪地里的小狗  
文字2："一只小狗在雪地里玩耍"  
```

**训练步骤**：
- **第1步**：分别编码
	- 图片编码器把图片转成数字
	- 文字编码器把文字转成数字

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/204.jpg)

> **图：在 CLIP 训练的第一步中，分别使用图像编码器和文本编码器对图像和文本进行嵌入处理**

- **第2步**：计算相似度
	- 配对的（图片1+文字1）应该相似度**很高**
	- 不配对的（图片1+文字2）应该相似度**很低**

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/205.jpg)

> **图：在 CLIP 训练的第二步中，使用余弦相似度计算句子嵌入与图像嵌入之间的匹配程度**

- **第3步**：不断调整
	- 如果配对的不够相似，就调整参数让它们靠近
	- 如果不配对的太相似，就调整参数让它们远离

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/206.jpg)

> **图：在 CLIP 训练的第三步中，根据预期相似度更新文本编码器和图像编码器参数。这种参数更新使相似输入的嵌入向量在向量空间中的距离逐渐缩小**

这种训练方法叫`对比`学习，在第10章会详细讲解。

## 五、实战：OpenCLIP的使用

OpenCLIP是CLIP的开源实现，下面是简单的使用示例：

![{%}](https://www.ituring.com.cn/figures/2025/HandsonLLM/207.jpg)

> **图 ：AI 生成的一只在雪地中玩耍的小狗的图像**

### 代码示例（简化版）

```python
from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel  

# 1. 加载模型的三个组件  
model_id = "openai/clip-vit-base-patch32"  
clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)  # 处理文字  
clip_processor = CLIPProcessor.from_pretrained(model_id)      # 处理图片  
model = CLIPModel.from_pretrained(model_id)                   # 主模型  

# 2. 准备数据  
image = Image.open("puppy.png")  # 一张小狗图片  
caption = "a puppy playing in the snow"  # 文字描述  

# 3. 处理文字  
text_inputs = clip_tokenizer(caption, return_tensors="pt")  
text_embedding = model.get_text_features(**text_inputs)  # 得到文字的数字表示  

# 4. 处理图片  
image_inputs = clip_processor(images=image, return_tensors="pt")  
image_embedding = model.get_image_features(**image_inputs)  # 得到图片的数字表示  

# 5. 计算相似度  
similarity = text_embedding @ image_embedding.T  # 点积计算相似度  
print(f"相似度分数：{similarity}")  # 分数越高越相似  
```

**关键点**：

- 文字嵌入和图片嵌入的维度完全一样（比如都是512维）
- 这样才能直接计算相似度

---

## 六、多模态嵌入的应用场景

### 1. 以图搜图 / 以文搜图

```
用户输入："海边日落"  
系统：返回所有相关的日落照片  
```

### 2. 图片分类（零样本）

```
给模型一张图片 + 几个候选标签  
"这是：[猫, 狗, 鸟]"  
模型直接选出最相似的标签，不需要专门训练  
```

### 3. AI绘画引导

```
用户："画一只戴墨镜的猫"  
CLIP：把这段文字转成数字  
Stable Diffusion：根据这些数字生成图片  
```

---

## 七、核心要点总结

1. **多模态嵌入模型**：把图片和文字都转成数字，放在同一个空间里
2. **CLIP**：目前最流行的方案，通过对比学习训练
3. **训练原理**：让配对的图文相似度高，不配对的相似度低
4. **实际应用**：搜索、分类、AI绘画等

**类比理解**：  
想象一个翻译系统，把中文和英文都翻译成"国际通用语"。翻译后，意思相同的中文和英文句子在"国际通用语"中会很接近。多模态嵌入就是把"图片语言"和"文字语言"翻译成同一种"数字语言"！

---

这样讲，是不是清晰多了？核心就是：**两个编码器 + 同一个数字空间 + 对比学习训练** = 多模态嵌入模型！
